[{"content":" Universality and Scale Invriance at the Criticality of Site-Bond Percolation Network (drafting)\nSite and bond percolation models exhibit a phase transition at a shared critical point, where both demonstrate self-similarity and scale invariance—hallmarks of continuous phase transitions. Using the Renormalization Group (RG) method from a top-down approach, which derive a scaling procedure from the assumption of self-similar, we compute the scaling exponents for the percolation universality class. While this top-down approach offers pedagogical simplicity, it lacks the physical intuition provided by the Ginzburg-Landau framework. To bridge this gap, we explore a cross-referenced analysis of these two approaches, aiming to deepen our understanding of network percolation phenomena.\nGroup Theory Application in Quantum Mechanics (drafting)\nThis blog mainly focus on the symmetries in the quantum theory, starting from the simplest solution of hydrogen atom, a case that respects the rotational symmetry. Then, to more complicated situation such as in the molecular case and spatially repeating lattice, a case that respects translational symmetry, emphazising the connection between the symmetries of the problem and the form of the solutions.\nFermi suface: A Geometry Interpretation of Conductors (drafting)\nProvide a comprehensive review of Fermiology.\nPhases Transition and Renormalization Group method applied in Random Graphs Model\nFundamental physics often deals with elegant, simplified models. However, complex problems, such as those in social sciences, require a different approach. For example, the \u0026ldquo;six degrees of separation\u0026rdquo; observation, made in the early 20th century, wasn\u0026rsquo;t fully understood until more recently. By adjusting the length and time scales, we can study the relevant interactions leading to emergent connectivity patterns. In this blog, I will use both computational methods and RG group method to demonstrate qualitative effects and quantiative measure of random graphs.\nSolving the Ising Model\nMy first blog, motivated by the discovery of a continuous phase transition in the two-dimensional Ising model, which is absent in the one-dimensional case. This led me to investigate the role of dimensionality in critical phenomena.\nRenormalization Group method with Examples\nDuring a group project on logistic mapping as a mathematical model for double period bifurcations, I encountered the renormalization group (RG) method for studying dynamic stability. This blog demonstrates RG calculations on the one-dimensional Ising model, though the RG method in logistic mapping remains purely qualitative.\nTop-down derivation of Electromagnetism\nWhile reading Zee\u0026rsquo;s book on group theory, I became fascinated with his other works on quantum field theory and gravitation. One particularly interesting concept is how Maxwell\u0026rsquo;s equations can be derived, assuming no prior knowledge of gravity or electromagnetism, simply by understanding how spacetime works. This blog reproduces that derivation.\nFourier Transforms: A Group Theoretic Perspective\nGroup theory and representation theory are elegant frameworks. From the perspective of group theory, Fourier transforms are simply representations of the \\(Z_{n}\\) group or U(1) group in the continuous limit. The orthogonality relations in Fourier analysis are reflected in the character orthogonality of irreducible representations. Despite the symplicity, it opens up a possiblity to explore the orthogonality in other symmetries group.\nQuantum Mechanical Historian\nIn my undergraduate study of quantum mechanics, the motivations behind the Schrödinger wavefunction and operator approaches were often overlooked. Von Neumann\u0026rsquo;s mathematical analysis unified both approaches using the concept of a Hilbert space and introduced the density matrix, a key tool in studying quantum information. This blog covers these foundational ideas.\n","permalink":"https://htsod.github.io/posts/read_me/","summary":"Universality and Scale Invriance at the Criticality of Site-Bond Percolation Network (drafting)\nSite and bond percolation models exhibit a phase transition at a shared critical point, where both demonstrate self-similarity and scale invariance—hallmarks of continuous phase transitions. Using the Renormalization Group (RG) method from a top-down approach, which derive a scaling procedure from the assumption of self-similar, we compute the scaling exponents for the percolation universality class. While this top-down approach offers pedagogical simplicity, it lacks the physical intuition provided by the Ginzburg-Landau framework.","title":"README"},{"content":"Title A concise and descriptive title that captures the essence of your research project.\nAbstract A brief summary of the research project, highlighting the key research question, methodology, expected outcomes, and significance.\nIntroduction Background: Provide a comprehensive overview of the relevant physics concepts and current understanding of the research area.\nMotivation: Explain the rationale behind your research, including the gap in knowledge or unanswered questions you aim to address.\nLiterature Review Relevant Studies: Critically analyze existing research related to your topic, highlighting key findings, limitations, and potential areas for further exploration.\nTheoretical Framework: Discuss the theoretical concepts underpinning your research and how they relate to your study.\nResearch Questions/Hypotheses: Specific Questions: Clearly state the central research questions that your study will attempt to answer.\nHypotheses (if applicable): Formulate testable hypotheses based on your research questions.\nMethodology: Experimental Design: Detailed description of the experimental setup, including apparatus, materials, and procedures.\nData Collection: Explain how data will be collected, including measurement techniques, data recording methods, and potential error analysis.\nData Analysis: Outline the statistical or computational methods you will use to analyze your data and interpret results.\nExpected Outcomes: Potential Findings: Describe the anticipated results of your research and their implications for the field.\nScientific Contribution: Explain how your research will contribute new knowledge or advance current understanding in physics.\nTimeline: Project Milestones: Break down your research project into distinct phases with estimated timelines for each step (literature review, experimental setup, data collection, analysis, writing).\nBudget: Cost Breakdown: Itemize the estimated costs associated with your research, including equipment, materials, personnel, and other relevant expenses.\nSignificance/Impact: Broader Applications: Discuss the potential real-world applications or societal impact of your research findings.\nReferences: Citation List: Provide a complete list of all cited sources in a consistent format.\n","permalink":"https://htsod.github.io/life_trivia/gl_eqn_criticality/","summary":"Title A concise and descriptive title that captures the essence of your research project.\nAbstract A brief summary of the research project, highlighting the key research question, methodology, expected outcomes, and significance.\nIntroduction Background: Provide a comprehensive overview of the relevant physics concepts and current understanding of the research area.\nMotivation: Explain the rationale behind your research, including the gap in knowledge or unanswered questions you aim to address.\nLiterature Review Relevant Studies: Critically analyze existing research related to your topic, highlighting key findings, limitations, and potential areas for further exploration.","title":"Review of Continuous Phase Transition, Ginzburg-Landau Equation and Renormalization Group"},{"content":"Abstract Site and bond percolation models exhibit a phase transition at a shared critical point, where both demonstrate self-similarity and scale invariance—hallmarks of continuous phase transitions. Using the Renormalization Group (RG) method in ( Citation: Sethna,\u0026#32;2020 Sethna,\u0026#32; J.\u0026#32; (2020). \u0026#32; Entropy, Order Parameters, and Complexity (2). \u0026#32; Clarendon Press. ) , which proposed a scaling procedure from the assumption of self-similar, we derive the scaling exponents for the percolation universality class. While this top-down approach offers pedagogical simplicity, it lacks the physical intuition provided by the Ginzburg-Landau framework. To bridge this gap, we explore a cross-referenced analysis of these two approaches, aiming to deepen our understanding of network percolation phenomena.\nIntroduction Percolation theory studies the connected compoents of a damaged graph \\(G=(V, E)\\), where \\(V\\) denotes the number of nodes and \\(E\\) denotes the number of edges. The damaging process is done by removing the edges with probability \\(p\\) - this is called bond percolation - Or removing the nodes with probability \\(p\\) - this is called site percolation.\nThe percolation process is closely related to the continuous phase transition discussed in statistical mechanics. One physical example of percolation is the sharpe drop of electrical conductivity as one punches holes on a metal sheet to its critical value. ( Citation: Last\u0026#32;\u0026amp;\u0026#32;Thouless,\u0026#32;1971 Last,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;Thouless,\u0026#32; D. \u0026#32; (1971). \u0026#32;Percolation theory and electrical conductivity. Phys. Rev. Lett.,\u0026#32;27.\u0026#32;1719–1721. https://doi.org/10.1103/PhysRevLett.27.1719 ) The conductivity of the metal sheet can be written as a function of the area of the holes and it embodies continuous change of physical behavior.\nConsider having a few holes to a intact sheet. Current could always find ways to go around the holes featuring the conducting regime. As the number of holes increases to the critical value, the allowed conduction path narrowed down, and eventually the metal sheet falls apart and become fully disconnected. The conductivity varies continuously as area of the holes are being punched through, meanwhile also exhibit two regimes of behavior as the parameter is varied. This experimental set-up is analogous to a percolation problem.\nPercolation does not limit to a particular physical phenomena. In fact, it is a common trait of many network system. For example, when modelling the spread of forest fire and infectious diseases ( Citation: Stauffer\u0026#32;\u0026amp;\u0026#32;Aharony,\u0026#32;1994 Stauffer,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Aharony,\u0026#32; A.\u0026#32; (1994). \u0026#32; Introduction to percolation theory. \u0026#32; Taylor \u0026amp; Francis. ) , percolation transitions are frequently observed. So, it will be convinient to have a simple random graph model to solely study the percolation effect. In this brief review, we will be considering two dimensional bond percolation and site percolation from a top-down approach. We simulate the model and quantify the observables to derive the scaling law at and near the critical point. Then, we will be justifying some of the observation under the framework of Ginzburg-Landau Equation.\nExperiment Set-up Network Simulation In a bond percolation network, we initialize a fully connected \\(L \\times L\\) grid lattice and enforcing the periodic boundary condition. Each bond would have probability \\(p\\) to be removed or equivalently having \\(1- p\\) probability to be added. For a site percolation, we initialze the graph on a fully connected triangular \\(L \\times L\\) lattice. Though this time we are removing the site instead of the bond.\nimport main # Plot a grid network with dimension m x n. Edges have 0.5 probability to be removed m, n, p = 20, 20, 0.5 main.plot_grid_clusters(m, n, p) # Plot a grid network with dimension m x m. Edges have 0.5 probability to be removed # By construction the triangular graph must have L x 2L to be have equivalent sites as the grid network m, n, p = 20, 40, 0.5 G = main.triangular_percolation(m, n, p) main.plot_triangular_cluster(G, m, n) On the left of the graph shows the bond percolation, where each bond between the nodes have probability \\(p=1/2\\) to be removed. On the right shows the site percolation with the same probability \\(p=1/2\\). Components of the graph is color coded by their relative size. The larger cluster tend to have a darker color.\nAs observe from the figure above, the system parameters to control the percolation are the same (the same lattice size \\(L\\) and probability of removal \\(p\\)), the microscopic detail, such as the number of nearest neighbor, is different. For the bond percolation, each node has four nearest neighbor whereas for site percolation, each node has six nearest neighbor.\nPhases near the critical point How do we tell that these model exhibit phase transition? To answer this question, we investigate the observables of the graph, such as the distribution of the size of the network components \\(n(S)\\), where \\(S\\) refers to the size of the component. As well as the normalized largest components for ensembles with \\(P(p) = S_{largest}(p)/L^{2}\\), where \\(S_{largest}\\) representing the largest cluster as a function of the probability \\(p\\).\nWe distinguish different phases by the change of symmetry of the system in interest, which often manifest itself through the scaling law of the observables near the criticla point. For example, the melting of ice exhibit a broken translation symmetries changing from liquid to solid. Symmetry, as we know it, cannot change smoothly ( Citation: Anderson,\u0026#32;2019 Anderson,\u0026#32; P.\u0026#32; (2019). \u0026#32; Basic notions of condensed matter physics. \u0026#32; Taylor \u0026amp; Francis Limited. ) . Hence, the observables will have a sharp transition, known as scaling, that separate two regimes of behavior.\nTwo regimes of behavior To see that the two regimes of behavior exhibit, we take the limit of these observables as \\(p\\) ranging from \\(0\\) to \\(1\\). At \\(p\\rightarrow 0\\), it is a fully connected graph with \\(P(p) = 1\\) and \\(n(S)= \\delta_{S=L\\times L}L \\times L\\). As \\(p \\rightarrow 1\\), it will become a sparsely connected graph and the corresponding \\(P(p) = 0\\) and \\(n(S) = 1\\) in the large \\(L\\) limit.\nHowever, checking the limits will only tell us that the system exhibit two regimes of behavior but necessarily tell us that there must be a critical point separating the two phases. How do we know that there indeed exists a critical point at the phase transition but not some smooth varying function that changes polynomial with \\(p\\), such as \\(P(p) \\propto (1-p) \\). In general, it is fairly difficult to come up with the analytic form of the observables, and in the case of percolation, it is impossible to have analytic form of the function that covers the entire domain. However, it is possible to derive the behavior near the critical point using the method of Renormalization Group (RG) to approximate the critical behavior. As promosed, from a top-down approach, we would rely on numerical simulation to decide the existence of phases, and then justify the use of RG approach.\nNumerical simulation Running simulation with \\(L = 20\\) for different probability \\(p\\) and comparing the distribution. One will find that the grid percolation has a sharp transition about the critical point \\(p_{c} \\sim 0.5\\). Plotting the grid percolation near the critical point:\nFor \\(p \u0026lt; 0.5\\), as shown for the bond percolation on the left figure, the largest cluster nearly covers the entire graph showing a phase with large \\(P(p=0.4)\\) value. In constrast, at \\(p \u0026gt; 0.5\\) as shown on the right figure, clusters broken down in smaller proportion, showing a distinct phase with much smaller \\(P(p=0.6)\\) value. Even as we close up the difference to the critica value of \\(0.5\\), these two phases persist, which validates our claim about the system having a critical point at \\(p_{c}=0.5\\) through a numerical method.\nOn contrast, we could derive the value of transition if we assume the system must exhibit a critical point where the system drastically changes its behavior. Suppose grid network \\(A\\) percolates at \\(p_{c}\\) for \\(p_{c} \\in (0, 1) \\). We could always construct an dual grid network \\(B\\) by shifting all the nodes to the diagonal and re-connect the nodes where there isn\u0026rsquo;t any connection in \\(A\\). The two graph is complementary by construction. So, \\(p_{A} = 1- p_{B}\\). In addition, both model is statistically equivalent, that implies both network percolate at the same critical value \\(p_{c}\\). Hence, \\(p_{c} = 1 - p_{c}\\), yielding the critical value at \\(p_{c} = \\frac{1}{2}\\).\nScaling-law Behavior about the critical point Right at the critical points, it becomes increasingly difficult to tell the phases of the system. However, there are some interesting observation that could help us get started in the analysis of criticallity.\nFor the largest cluster with the simulation running at \\(L = 1024\\) and \\(p = 0.5\\), no matter if we running with grid or triangular lattice, it displays roughly the same pattern. This is known as universality. In the thermodynamics limit, the macroscopic observables are incensitive to the microscopic detail; the microscopic details behave as a fluctuation that goes away when summed over a large system size.\nOne the left column, we have bond percolation(top) and edge percolation(bottom) at a relatively small system size of \\(L = 20\\). On the right column, we have the largest cluster from the corresponding bond percolation(top) and edge percolation(bottom) at a much larger system size with \\(L = 1024\\). Though differed in their density in the large system limit, both models are statistically similar, demonstrating universality.\nTo quantify the pattern that we are seeing, we propose a coarse grain procedure and it goes as follow: we average over the pixel values of four lattices in squre and replacing the four lattices with a larger square with the average of the four, essentially shrinking the lattice from a size of \\(1024 \\times 1024\\) to \\(256 \\times 256\\); the coarse grained figure remain statistically equivalent. This is known as self similar and scale invariance, two well known features of continuous phase transition.\nPower-law at \\(p_{c}\\) It is precisely from the observation of self-similar of the system at a large size limit that motivates the top-down derivation of the scaling-law in the percolation network.\nUnder coarse graining, we will be measuring every observables of the system with a new length scale. Self-similar thus refering to the invariant of the distribution under the change of length scale.\nLet \\(D(S)\\) be the distribution of the cluster with size \\(S\\). The statement of scale invariance then can be expressed as:\n$$D^{\\prime}(S) = D(S)$$\nwhere \\(D \\rightarrow D^{\\prime}\\) refering to the change of length scale. And hence, the functional of \\(D\\) has no explicit dependent on \\(l\\):\n$$\\partial D / \\partial l = 0$$\n$$ \\frac{dD}{dl} = \\frac{\\partial D}{\\partial S} \\frac{dS}{dl} + \\frac{\\partial D}{\\partial l} = \\frac{\\partial D}{\\partial S} \\frac{dS}{dl}$$\nThis is precisely the embodiment of self-similar in the spatial distribution, also known as the fixed point of the RG flow in the parameter space.\nHow about the functional dependent of of the distribution \\(D(S)\\) on the size of the distribution \\(S\\) under rescaling? Since the cluster size \\(S\\) is dependent on the length scale of the system. Under coarse graining it will be scaled down by some factor \\(C = 1 + cdl\\).\n$$ S^{\\prime} = S/C = S/(1+cdl) \\approx S-cSdl$$ $$ S^{\\prime} - S = dS = -cSdl $$ $$ dS/dl=-cS $$\nConsequently, measuring the distribution with coarse grained cluster size will have the following form:\n$$ D^{\\prime}(S^{\\prime}) = D(S^{\\prime}) = D(S- cSdl) = D(S + \\frac{dS}{dl} dl)=D(S)+\\frac{d D}{d S}dS$$\nWhen the cluster gets smaller, the number of cluster will surely increase, requiring \\(D(S^{\\prime})\\) to scale positively with some factor \\(A = 1+ adl\\).\n$$D(S^{\\prime}) = AD(S) = D(S)(1+adl) = D(S) + aDdl$$\nEquating the two equations above to give:\n$$ \\frac{dD}{dS}dS = a Ddl $$\nRearanging to remove the dependence on \\(dl\\) and define \\(\\tau = \\frac{a}{c}\\): $$ \\frac{dD}{D} = -\\frac{a}{c}\\frac{dS}{S} $$\n$$ D(S) \\propto S^{-a/c} = S^{-\\tau} $$\nThe distribution follows a scale-free dependent on the size of the cluster with exponent \\(\\tau = a/c\\). Testing the scaling relation with the theoretical exponent of \\(\\beta = 187/91\\)\nThe fluctuation on the experimental value is due to the finite size of the experiment. Each interval of \\(\\delta S\\) is binned. If there is insufficient samples, sometimes it will be gapped like shown on the graph above. But it is obvious that overall trend fluctuatues around the theoretical predicted value which indeed proves the claims that the distribution follows a power-law scaling (showing a straight line on a log-log plot). Noting that both grid percolation and triangular percolation do indeed from the same universality class; they have the same exponent at the critical region.\nPower-law near \\(p_{c}\\) Not only the distribution of the cluster size follow the power-law, the quantity \\(P(p)\\) as the function of \\(p\\) also follows a power-law. We could go through the similar derivation of scaling and yield the following relation:\n$$ P(p) \\sim (p- p_{c})^{\\beta} $$\nHowever, if we plot the actual distriubtion, the scaling exponent is quite off from the theoretical value of \\(\\beta = 5/36\\). What is going on?\nQualitatively, a graph could be sparse or dense depending on the number of shortcuts. For network of \\(L=500, k = 2\\). The left graph is produced by \\(p=0.001\\) and the right graph is produced by \\(p = 0.1\\)\nThis is because the model is not exactly in the thermodynamics limit. The phase transition and the scaling-law is well-defined only when the system is in the thermodynamics limit where the mean field theory holds.\nWe could verify this claim by finite-size scaling and write the distribution as a function of both the probability \\(p\\) and the size of the system \\(L\\). As the increase the size of the sytem, the \\(P(p, L)\\) will collapse to a single function. It will become more accurate as the system size increases.\nQualitatively, a graph could be sparse or dense depending on the number of shortcuts. For network of \\(L=500, k = 2\\). The left graph is produced by \\(p=0.001\\) and the right graph is produced by \\(p = 0.1\\)\nSome Flying Questions Why phase transition occur in these simulated model? In thermodynamics system, the macroscopic state is determined by the free energy of the system. Equilibrium happens when the free energy is minimized. At high temperature, entropy is maximized to minimize the free energy. At low temperature, the energy becomes the dominating term. So, the free energy is minimized by minimizing the energy. One might draw a similar analog in the case of percolation. At high probability of bond removal, the individual tend to form smaller cluster, a disorder favored state. In the low-p limit, nodes tend to form a large cluster to minimize \u0026ldquo;energy\u0026rdquo;.\nFor a given system, what is the relation between fractal, scaling-law and criticallity? In the percolation network, we observe scale invariant. And we propose a scaling procedure to derive the exponent. Is there a guarantee? If there is a continuous phase transition, there must be fractal pattern, or scale invariant at the critical point. Is the vice versa true? If we observe a fractal pattern in nature, then we say that the system is undergoing a phase transition.\nExplaining with Ginzburg-Landau Equation Continuous phase transition Different phases are characterized by different symmetries or different pattern that varies continuously through the phae boundary. Propose a order parameter so now the free energy becomes a functional of the order parameter. It leads to discontinuous thermodynamics quantity such as heat capacity and susceptibility. Notably, the correlation length, will diverge at the phase transition, essentially dominate all length scale at the phase transition ( Citation: Hohenberg\u0026#32;\u0026amp;\u0026#32;Krekhov,\u0026#32;2015 Hohenberg,\u0026#32; P.\u0026#32;\u0026amp;\u0026#32;Krekhov,\u0026#32; A. \u0026#32; (2015). \u0026#32;An introduction to the ginzburg–landau theory of phase transitions and nonequilibrium patterns. Physics Reports,\u0026#32;572.\u0026#32;1–42. https://doi.org/10.1016/j.physrep.2015.01.001 ) . This is the justification of scale invariance at the phase transition: the physical properties dominated by the correlation length will follow a scaling law.\nIn a computer simulated program, the free energy is not always straight forward. In the case of percolation network, the free energy could be written as\n( Citation: Cirigliano,\u0026#32;Timár \u0026amp; al.,\u0026#32;2024 Cirigliano,\u0026#32; L.,\u0026#32; Timár,\u0026#32; G.\u0026#32;\u0026amp;\u0026#32;Castellano,\u0026#32; C. \u0026#32; (2024). \u0026#32;Scaling and universality for percolation in random networks: A unified view. Physical Review E,\u0026#32;110(6). https://doi.org/10.1103/physreve.110.064303 ) Subsequently, the observables of the system could be expressed as derivatives of the free energy.\nFluctuation: Ginzburg Landau Equation The Landau functional is, after all, a mean-field approach to the problem. At situation where the fluctuation dominates, the mean-field approach breakdown. Then, the region of validity of Landau function is everywhere but the critical point. Right at the critical point, the correlation length, or the fluctuation will dominiate all length scale. So, how do we suppose to learn about the behavior at the critial region?\nScale invariance: renormalization group The renormalization group method do exactly that. It integrates out the unimportant degree of freedom, from macroscale to mesoscale. With each iteration, it mimic a flow in the parameter space, where each point denote the different form of the free energy functional dependent on the order parameter. Can we do the integration indefinitely? No, that\u0026rsquo;s not possible because at the critical point, the correlation length diverges, so the number of vibration modes also diverges, so it becomes scale invariance, or a fixed point in the abstract parameter space.\nFrom the fixed point, we could linearize it, and the fixed point will be characterize by the exponent that either goes into the fixed point or goes out from the fixed point. For a well-defined phase transition, it can only have two positive exponent, also called relevent fields. Other fields will scale to zero.\nFor system that renormalizes to the same fixed point, they belong to the same universality class. These systems will share the same critical exponents. Though they are physically irrelevent, they behave the same at the critical points.\nReferences: Cirigliano,\u0026#32; Timár\u0026#32;\u0026amp;\u0026#32;Castellano (2024) Cirigliano,\u0026#32; L.,\u0026#32; Timár,\u0026#32; G.\u0026#32;\u0026amp;\u0026#32;Castellano,\u0026#32; C. \u0026#32; (2024). \u0026#32;Scaling and universality for percolation in random networks: A unified view. Physical Review E,\u0026#32;110(6). https://doi.org/10.1103/physreve.110.064303 Sethna (2020) Sethna,\u0026#32; J.\u0026#32; (2020). \u0026#32; Entropy, Order Parameters, and Complexity (2). \u0026#32; Clarendon Press. Anderson (2019) Anderson,\u0026#32; P.\u0026#32; (2019). \u0026#32; Basic notions of condensed matter physics. \u0026#32; Taylor \u0026amp; Francis Limited. Last\u0026#32;\u0026amp;\u0026#32;Thouless (1971) Last,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;Thouless,\u0026#32; D. \u0026#32; (1971). \u0026#32;Percolation theory and electrical conductivity. Phys. Rev. Lett.,\u0026#32;27.\u0026#32;1719–1721. https://doi.org/10.1103/PhysRevLett.27.1719 Hohenberg\u0026#32;\u0026amp;\u0026#32;Krekhov (2015) Hohenberg,\u0026#32; P.\u0026#32;\u0026amp;\u0026#32;Krekhov,\u0026#32; A. \u0026#32; (2015). \u0026#32;An introduction to the ginzburg–landau theory of phase transitions and nonequilibrium patterns. Physics Reports,\u0026#32;572.\u0026#32;1–42. https://doi.org/10.1016/j.physrep.2015.01.001 Stauffer\u0026#32;\u0026amp;\u0026#32;Aharony (1994) Stauffer,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Aharony,\u0026#32; A.\u0026#32; (1994). \u0026#32; Introduction to percolation theory. \u0026#32; Taylor \u0026amp; Francis. ","permalink":"https://htsod.github.io/posts/percolation_network/","summary":"Abstract Site and bond percolation models exhibit a phase transition at a shared critical point, where both demonstrate self-similarity and scale invariance—hallmarks of continuous phase transitions. Using the Renormalization Group (RG) method in ( Citation: Sethna,\u0026#32;2020 Sethna,\u0026#32; J.\u0026#32; (2020). \u0026#32; Entropy, Order Parameters, and Complexity (2). \u0026#32; Clarendon Press. ) , which proposed a scaling procedure from the assumption of self-similar, we derive the scaling exponents for the percolation universality class.","title":"Universality and Scale Invaraince at the Criticality of Site-Bond Percolation Network"},{"content":"This investigation began as a problem sets in the excellent statistical mechanics textbook by James Sethna. ( Citation: 2021,\u0026#32;pp.\u0026nbsp;10-13 Sethna,\u0026#32; J.\u0026#32; (2021). \u0026#32; Statistical mechanics: Entropy, order parameters, and complexity. \u0026#32; OUP Oxford. ) In this blog, we start with a qualitative overview of the small-world effect, also known as the \u0026ldquo;six degrees of separation,\u0026rdquo; followed by a quantitative analysis using computational methods, and finally, a description of the phenomenon through the framework of phase transitions and critical phenomena. From the surface level to deeper insights, I aim to explore the mechanics underlying this phenomenon and how this understanding can be extended to other large-scale phenomena.\nSix Degree of Separation: An Inquiry into Social Cloesness If you know six people, preferably from different parts of the world in addition to your closest acquanintances, you can effectively connect to everyone on the planet. This concept, known as \u0026ldquo;six degrees of separation,\u0026rdquo; was first introduced by novelist Frigyes Karinthy. It describes the significant reduction in social distance when a few shortcuts are added in a social network. Below is a YouTube video by the youtuber Veritasium that introduces this idea (where I first learned about the concept of six degrees of separation).\nHowever, it mains as a qualitative thought in 1922 because computation and math theory is not available to quantify this social occurence.\nIt wasn\u0026rsquo;t until 1998 that Duncan J. Watts and Steven H. Strogatz introduced a random graph procedure to study this effect quantitatively. ( Citation: 1998 Watts,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Strogatz,\u0026#32; S. \u0026#32; (1998). \u0026#32;Collective dynamics of “small-world” networks. Nature,\u0026#32;393(6684).\u0026#32;440–442. https://doi.org/10.1038/30918 ) Networkx Simulation on Random Graphs The method involves generating a network (or graph interchangeably) with \\(L\\) nodes. The nearest neighbors of each node are well-defined: we arrange the nodes in a circle and connect them to their nearest neighbors, determined by the parameter \\(Z\\) (an even number for evenly distributed connections). To illustrate the procedure, below is a graph with \\(N = 20\\) and \\(Z = 4\\). The graph is generated using NetworkX, an extensive Python library for network theory.\nimport matplotlib.pyplot as plt import networkx as nx L = 20 Z = 4 p = 0 G = nx.newman_watts_strogatz_graph(L, Z, p) nx.draw(G, pos=nx.circular_layout(G)) plt.draw() plt.savefig(\u0026quot;simple_graph.png\u0026quot;) This is a simple connected network with 20 nodes, 4 nearest neighbors, and no shortcuts.\nAdding Shortcuts Now, how quickly we can travel from one point on the graph to another depends on the shortest path. By randomly rewiring the existing connections \\(Lkp\\) shortcuts between unconnected nodes, we can reduce the shortest travel distance. We characterize the connectivity by the average shortest distance (l) of the graph. The algorithm to calculate the graph\u0026rsquo;s average distance is as follows:\nStart from node \\(L_{i}\\) and measure the distance \\(d\\) to the nearest neighbors.\nAccumulate these distances, move on to those nearest neighbors, and repeat step 1 until all nodes are exhausted.\nDivide the cummulative distance by the total neighbor of \\(L_{i}\\) and yields the average distance of node \\(L_{i}\\)\nRepeat step 1 for the next nodes \\(N_{i+1}\\) until all nodes exhaust\nSum up the average distance of each nodes and divided by the number of nodes\nEffects of shortcuts on the Average Shortest Path Length NetworkX has an optimized built-in function to calculate the graph\u0026rsquo;s shortest distance, so we\u0026rsquo;ll use that. After we know the quantitative measurement of the separation distance, we rewired the existing edges probabilistically by \\(p\\). (Though we keep those wires being rewired to avoid disconnected component) On average, \\( p L k \\) shortcuts are added to the graph, where we defined \\(k\\) to be \\(k = \\frac{Z}{2}\\). Next, we illustrate the network qualitatively with different parameters. For a sparsely connected graph and a densely populated graph they are topologically very different from each other:\nQualitatively, a graph could be sparse or dense depending on the number of shortcuts. For network of \\(L=500, k = 2\\). The left graph is produced by \\(p=0.001\\) and the right graph is produced by \\(p = 0.1\\)\nFor the sparse graph above, the average seperation can go up to \\(23\\). Whereas for the densely populated graph, the distance of separation reduces to about \\(4\\).\nSo, the six degree of separation makes qualitative sense as observe from the above figure. Next, we want to know how adding the shortcuts will impact the average shortest path \\(l\\). To do this, we vary \\( p \\) from \\(0.001 \\) to \\( 0.1 \\) for a graph with \\(50\\) nodes, nearest neighbor connection \\(Z = 2\\). Plotting the result:\nN = 50 Z = 2 p = np.linspace(0.001, 1, 100) distance_distribution = [] for s in p: G = nx.newman_watts_strogatz_graph(N, Z, s) avg_shortest_path = nx.average_shortest_path_length(G) distance_distribution.append(avg_shortest_path) fig,ax = plt.subplots() ax.scatter(p, distance_distribution) ax.set_ylabel(\u0026quot;Separation\u0026quot;) ax.set_xlabel(\u0026quot;p\u0026quot;) ax.set_title(\u0026quot;Small-world effects\u0026quot;) plt.savefig(\u0026quot;small_world.png\u0026quot;) As \\(p\\) increases, the number of shortcuts \\(Lpk\\) also increases, which reduce the shortest distance with a scaling law as shown.\nWe observe a dramatic decrease in the average separation (s) in the small (p) region. Around (s = 6), the curve shifts sharply, transitioning from a drastic decrease to asymptotic behavior towards zero separation. From our computer simulation and calculations, we elaborate the vague conclusion from social science to a firm numerical justification. In what follows, we want to understand this phenomena with renormalization group, a model that studies critical behavior of systems.\nRenormalization Group Analysis of Random Graph In the renormalization group approach to this random network model, the transition from a regular graph to a random graph at \\(p = 0\\) can be treated as a continuous phase transition. Following this perspective, we refer to the work of ( Citation: 1999 Newman,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Watts,\u0026#32; D. \u0026#32; (1999). \u0026#32;Renormalization group analysis of the small-world network model. Physics Letters A,\u0026#32;263(4–6).\u0026#32;341–346. https://doi.org/10.1016/s0375-9601(99)00757-4 ) to elaborate on the scaling scheme used to derive the scaling exponent \\(\\tau = 1\\) in the form \\(\\xi \\propto p^{-\\tau}\\), where \\(\\xi\\) is analogous to the correlation length in a typical physical system.\nBefore diving into the technical details, we first ask the following questions:\nWhat kind of question we want to answer in this small world model? The six degrees of separation and the scaling graph suggest an analytic form that relates the shortest path length to system variables. However, it\u0026rsquo;s difficult to derive this form because the shortest path length is calculated via a computational routine, and no analytic form can be derived from the algorithm.\nWhy is the renormalization group method a good candidate for solving this small-world problem? The renormalization group studies the scaling behavior of systems near critical points, where the correlation length governs the entire system\u0026rsquo;s behavior. The small-world model demonstrates two regimes of behavior, suggesting a transition and a critical point. This motivates mapping the small-world problem to a phase transition problem.\nTwo regimes of behavior In the first regime of phase, the average distance scales linear with the size of the graph \\(L\\) because the only way to reach other nodes is by traversing through the neighbors.\nThe other regime is the random graph region, where the average distance scales approximately as lograithm of \\(L\\).\nBetween these two regimes, the graph undergoes critical behavior at \\(p=0\\) for a system size \\(L=\\xi\\), where the number of shortcuts approximately be \\(Lk\\xi \\approx 1\\). Here, \\(\\xi\\) is analogous to the correlation length in a typical physical system, which diverges at the critical point. For \\(pk\\xi \\approx 1\\) to hold at \\(p\\rightarrow 1\\), \\(\\xi\\) must goes to infinity at the critical point.\nTo better understand the concept of correlation length in a typical physical system, note that a continuous phase transition involves moving from a disordered state to an ordered state as a relevant parameter decreases. As the system approaches this transition, the correlation length, which measures the collective order, increases. At the critical region, the correlation length becomes very large because order tends to align throughout the system.\nIt is this diverging behavior of the correlation length near the critical point that allows us to formulate an analytic scaling relation between system parameters, enabling us to solve the small-world model using the renormalization group (RG) method.\nParameters of the system: Scaling form For those interested in learning more about the RG method and the justification for diverging correlation lengths dominating the behavior of continuous phase transitions, see ( Citation: Hohenberg\u0026#32;\u0026amp;\u0026#32;Halperin,\u0026#32;1977,\u0026#32;pp.\u0026nbsp;9-12 Hohenberg,\u0026#32; P.\u0026#32;\u0026amp;\u0026#32;Halperin,\u0026#32; B. \u0026#32; (1977). \u0026#32;Theory of dynamic critical phenomena. Rev. Mod. Phys.,\u0026#32;49.\u0026#32;435–479. https://doi.org/10.1103/RevModPhys.49.435 ) . This work also explains how scaling effectively removes irrelevant degrees of freedom from the system. This process reduces the degrees of freedom, bringing the system to a different one with a different correlation length. Except at the critical point—where the correlation length is infinite and remains unchanged under scaling—the scaling law relates systems with different correlation lengths. If this leads to a simplified solvable model, meaningful conclusions can be drawn about the complex system that we started with.\nIn our case, the regular graph region has zero shortcuts, while the random graph has more than one. Therefore, we expect the transition to occur when the first shortcut appears, \\(\\xi kp \\approx 1\\) where \\(\\xi = L\\) at some system size.\nAssuming homogeneity leads to the scaling form of the correlation length, with the relevant system parameters following a scaling form for a function \\(f(x, y)\\) that satisfies homogeneous:\n$$ f(x, y) = x^{\\lambda} g\\left(\\frac{y}{x^{\\mu}}\\right) $$\nThis is further discussed in ( Citation: Toulouse\u0026#32;\u0026amp;\u0026#32;Pfeuty,\u0026#32;1977 Toulouse,\u0026#32; G.\u0026#32;\u0026amp;\u0026#32;Pfeuty,\u0026#32; P.\u0026#32; (1977). \u0026#32; Introduction to the renormalization group and to critical phenomena. \u0026#32; Wiley. ) n our problem, the only relevant length scales are the average shortest path length \\(\\xi\\) and the number of nodes \\(L\\). Based on the assumptions in Newman\u0026rsquo;s paper, we obtain the following scaling equation:\n$$ l = L f\\left(\\frac{L}{\\xi} \\right) $$\nAlternatively, assuming \\(\\xi \\propto p^{-\\tau}\\):\n$$ l = L f\\left(L p^{-\\tau} \\right) $$\nWe propose a scaling procedure where \\(L^{\\prime} = \\frac{1}{2}L \\), reducing the system by half, and \\(p^{\\prime} = 2p\\), keeping the number of shortcuts \\(Lkp\\) unchanged.\nGrouping procedure proposed by Newman and Watts. Dot with the same color are merged by scaling.\nComparing the ratio \\(\\frac{l^{\\prime}}{l}\\) and solvig for \\(\\tau\\) we find:\n$$ \\tau = \\frac{\\log{(L/L^{\\prime})}}{\\log{(p^{\\prime}/p)}} = 1$$\nReaching the result \\(\\tau =1 \\) requires significant work, including making educated assumptions about the model. After all, the small-world model is not governed by simple universal laws but by collective behavior that blurs microscopic details. Newman and Watts verified the exponent \\(\\tau = 1\\) through extensive numerical experiments. They also compared the system\u0026rsquo;s behavior if the exponent was \\(\\tau = \\frac{2}{3}\\), derived from an alternative RG approach but yielding less accurate results.\nThe scaling behavior for \\(k = 1\\) and \\(k =5\\). For \\(\\tau = \\frac{2}{3}\\) is also tested in the top-right corner of the figure\nFor \\(k \u0026gt; 1\\), the authors adopted a different coarse-graining approach to handle computational limitations, but the same exponent was obtained. As an extension, they also derived the scaling and exponent for systems with higher dimensions. For interested readers, detailed derivations can be found in the paper ( Citation: 1999 Newman,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Watts,\u0026#32; D. \u0026#32; (1999). \u0026#32;Renormalization group analysis of the small-world network model. Physics Letters A,\u0026#32;263(4–6).\u0026#32;341–346. https://doi.org/10.1016/s0375-9601(99)00757-4 ) Remarks In this blog, we have revisited the arguments from Newman and Watts\u0026rsquo; paper, reinterpreting them within the framework of phase transitions. Along the way, we provided additional physical insights and explanations for steps that were either implicit or assumed in the original work.\nTo understand complex phenomena through the lens of phase transitions and the renormalization group (RG), it is crucial to first identify the distinct regimes of behavior, where the system’s qualitative properties change significantly. These transitions are marked by changes in symmetry—something that, according to condensed matter physics, cannot occur gradually. This is why conventional methods like perturbation theory are inadequate in such cases. Instead, a scaling law must be derived, capturing the relevant parameters at the critical point.\nHowever, deriving this scaling law involves substantial numerical work and carefully made assumptions. In our application of the RG method, we focused on the scaling form to extract the exponent, yet the deeper physical meaning of the RG flow itself remains somewhat elusive.\nThe true power of the RG method lies in its ability to connect systems that, despite appearing different, share the same critical behavior as they approach a diverging correlation length. This ability to group systems by their underlying topological structure is a key strength of the RG approach, but was not entirely clear how this aspect of the RG approach plays in the application on the small-world model.\nReference Newman\u0026#32;\u0026amp;\u0026#32;Watts (1999) Newman,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Watts,\u0026#32; D. \u0026#32; (1999). \u0026#32;Renormalization group analysis of the small-world network model. Physics Letters A,\u0026#32;263(4–6).\u0026#32;341–346. https://doi.org/10.1016/s0375-9601(99)00757-4 Hohenberg\u0026#32;\u0026amp;\u0026#32;Halperin (1977) Hohenberg,\u0026#32; P.\u0026#32;\u0026amp;\u0026#32;Halperin,\u0026#32; B. \u0026#32; (1977). \u0026#32;Theory of dynamic critical phenomena. Rev. Mod. Phys.,\u0026#32;49.\u0026#32;435–479. https://doi.org/10.1103/RevModPhys.49.435 Sethna (2021) Sethna,\u0026#32; J.\u0026#32; (2021). \u0026#32; Statistical mechanics: Entropy, order parameters, and complexity. \u0026#32; OUP Oxford. Toulouse\u0026#32;\u0026amp;\u0026#32;Pfeuty (1977) Toulouse,\u0026#32; G.\u0026#32;\u0026amp;\u0026#32;Pfeuty,\u0026#32; P.\u0026#32; (1977). \u0026#32; Introduction to the renormalization group and to critical phenomena. \u0026#32; Wiley. Watts\u0026#32;\u0026amp;\u0026#32;Strogatz (1998) Watts,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Strogatz,\u0026#32; S. \u0026#32; (1998). \u0026#32;Collective dynamics of “small-world” networks. Nature,\u0026#32;393(6684).\u0026#32;440–442. https://doi.org/10.1038/30918 ","permalink":"https://htsod.github.io/posts/small_world/","summary":"This investigation began as a problem sets in the excellent statistical mechanics textbook by James Sethna. ( Citation: 2021,\u0026#32;pp.\u0026nbsp;10-13 Sethna,\u0026#32; J.\u0026#32; (2021). \u0026#32; Statistical mechanics: Entropy, order parameters, and complexity. \u0026#32; OUP Oxford. ) In this blog, we start with a qualitative overview of the small-world effect, also known as the \u0026ldquo;six degrees of separation,\u0026rdquo; followed by a quantitative analysis using computational methods, and finally, a description of the phenomenon through the framework of phase transitions and critical phenomena.","title":"Phases Transition and Renormalization Group method applied in Random Graphs Model"},{"content":"This investigation started in the consideration of quantum solids which invariably have to solve for a many-bodies quantum problem. Many particles quantum mechanics, just like its classical counter parts, is desparately intractable. The exactly solvable problem, to name a few, are the free particle, harmonics oscilator and hydrogen atom. The question is then, to solve the many-particles quantum problem with the tools we have in these solvable models. All these solvable model has a potential energy term that is invariant under a symmetrical transformation, thus allowing for specifying the quantum system with integers labeling the irreducible representation.\nIt was this observation that leads to group theoretic consideration of the quantum problem. Group theory has play an important role in quantum mechanics. Wigner\u0026rsquo;s theorem relate the symmetries of the Hamiltonian with the functional form of the states. For example, consider a crystalline structure consisting of \\(N\\) atoms where they are placed periodically and having the translation invariant. Without the symmetrical insight, this will be a intractable quantum problem of solving . But the translational invariant potential energy allow the solution of a Bloch function, making it a tractable quantum problem.\nThis blog mainly focus on the symmetries in the quantum theory, starting from the simplest solution of hydrogen atom, a case that respects the rotational symmetry. Then, to more complicated situation such as in the molecular case and spatially repeating lattice, a case that respects translational symmetry, emphazising the connection between the symmetries of the problem and the form of the solutions.\nSymmetries in Quantum Mechanics Symmetries of Hamiltonian Given a group \\(G\\) with element \\(g\\) that leaves the Hamiltonian \\(\\mathcal{H}\\) invariant, regard \\(g\\) as an operator as well as a group element we have:\n$$ g^{-1} \\mathcal{H} g = \\mathcal{H} $$\nIn a mathematical sense, the linear transformation by the group \\(g\\) leaves the Hamiltonian invariant, we will call the group \\(G\\) as the symmtry operation on the Hamiltonian.\nSymmetries of the states In quantum mechanics, the eigenfunction, or wavefunction has a well defined norm denoting probability density:\n$$ \\int_{\\mathcal{V}}{\\psi^{\\ast}\\psi dr^{3}} = 1$$\nWe could see that the representation of unitary \\(U(1)\\) will leave the proability unchanged:\n$$ \\psi^{\\prime} = e^{i\\phi}\\psi $$\nTo see this, \\( \\int_{\\mathcal{V}}{{\\psi^{\\ast}}^{\\prime}\\psi^{\\prime} dr^{3}} = \\int_{\\mathcal{V}}{\\psi^{\\ast} e^{-i\\phi} e^{i\\phi}\\psi dr^{3}} = 1\\)\nHence, the symmetric transform of the state is defined by a phase rotation.\nWigner\u0026rsquo;s theorem One observation to make by these observation on the symmetry of states and symmetry of the Hamiltonian is that, the states not necessarily follows the symmetry of the equation of the motion.\nConsider the stationary Schrodinger\u0026rsquo;s Equation:\n$$ \\mathcal{H} \\left| \\psi \\right\\rangle = E_{n} \\left| \\psi \\right\\rangle $$\nLet \\(G\\) be the symmetry of the Hamiltonian, then \\(g\\) acting on the Hamiltonian would commute with the Hamiltonian:\n$$ g\\mathcal{H} \\left| \\psi \\right\\rangle = \\mathcal{H} (g\\left| \\psi \\right\\rangle) = E_{n} (g\\left| \\psi \\right\\rangle )$$\nThen the set of states \\(g_{i} \\left | \\psi \\right\\rangle\\) for \\(i\\) ranges from \\(1\\) to the order of the group \\(n\\), will also be an eigenstates of the energy \\(E_{n}\\).\nWriting more explicitly the representation/operator of \\(g\\) as a matrix \\(D_{ij}(g)\\). Then, the set of states will transform as the irreducible representation of the group \\(G\\):\n$$ D(g)(D(g)\\left | \\psi \\right\\rangle) = D(gg) \\left | \\psi \\right\\rangle = D(g) \\left | \\psi \\right\\rangle$$\nThis is the Wigner\u0026rsquo;s theorem:\nif the Hamiltonian operator, \\(\\mathcal{H}\\), of a quantum-mechanical system possesses a group, \\(G\\), of symmetry operations, then each of the eigenfunctions \\(\\psi\\) of \\(\\mathcal{H}\\) must belong to (that is, must transform according to) one of the irreducible representations of \\(G\\).\nWigner\u0026rsquo;s theorem relates the symmetry of the Hamiltonian to the functional forms of the eigenfunctions, that the eigenfunction must transforms as the irreducible representation of the group \\(G\\).\nHowever, recall the symmetires of the states do not distinguish a phase rotation, the state under a phase rotation is equivalent and do not create a distinctive state \\(\\psi \\rightarrow \\psi^{\\prime} = e^{i\\phi} \\psi \\).\nWe would like to remove the phase rotation from the group \\(G\\) to find all the distinctive states. Depending on the group, it is not possible to represent every group member as a phase rotation \\(e^{i\\phi}\\). Thus, it leaves the cosset of the group \\(G\\) with subgroup \\(H = U(1)\\) acting on the state to form a distinctive eigenstates that does not transform invariantly as the Hamiltonian does.\nSymmetry breaking and restriction to the subgroup Reduction to subgroup\nSpontaneous Symmetry Breaking\nhypothetical Hamiltonian $$ H = H_{k=0} + \\sum{H_{int}} $$\nRotation Symmetry and Hydrogen Atom From the Hamiltonian, rotational symmetry. Rotation group. Predict 2l+1 irreducible representation.\nThe radial part remains unresolved and it is related to the integer labeling the\nSO(3) Rotation group.\nMolecular Orbital Theory: Linear Combination of Atomic Orbitals Translation Symmetry and Crystal Space group and characterize the Crystal with symmetry Now we could define crystal structure as being translation invariant on a set of lattice. In addition, one could define a point group in a unit cell. These translations and rotation invariants combined will give us the space group, the operation of a space group could be expressed mathematically as:\n$$ \\{ R_{\\alpha} | \\tau \\} $$\nwhere \\(R_{\\alpha}\\) denotes the point group operations such as rotation, reflections, improper rotations and inversion. And \\(\\tau\\) representing the tranlsation operations.\nWhen this space group operate on a vector, we yield the following algebraic expression:\n$$ \\{ \\alpha | \\tau \\} \\vec{r} = \\overleftrightarrow{\\alpha} \\cdot \\vec{r} + \\vec{\\tau} $$\nHence, the group multiplication is expressed as:\n$$ \\{\\beta | \\tau^{\\prime} \\} \\{\\alpha | \\tau \\} = \\overleftrightarrow{\\beta}\\left[ \\overleftrightarrow{\\alpha} \\cdot \\vec{r} + \\vec{\\tau} \\right] + \\tau^{\\prime} = \\{ \\beta \\alpha | \\beta \\tau + \\tau^{\\prime} \\} $$\nWith well defined inverse and identity on either operation:\ninverse $$ \\{ \\alpha | \\tau \\}^{-1} = \\{ \\alpha^{-1} | -\\alpha^{-1}\\tau \\} $$\nidentity $$ \\{ \\epsilon | 0 \\} $$\nWith pure rotation defined as \\( \\{ \\alpha | 0 \\} \\) and pure translation defined as \\(\\{ \\epsilon | \\tau \\} \\).\nThese space group \\(G\\) contains a translation subgroup that defines the translation invariant of the lattice. This can be stated as the following:\nAll the elements of the space group \\(G\\) that are of the form \\( \\{ \\epsilon | \\tau \\} \\) constitute the translation group \\(T\\) that defines the Bravais lattice.\nIn 3-dimension, there are 14 Bravais lattice and a total of 230 space group. Of those 230 space group, we could further divide them into either a symmorphic group or non-symorphic group.\nA symmorphic group could be expressed as:\n$$ \\{ R_{\\alpha} | \\tau \\} = \\{R_{\\alpha}| R_{n}\\} = \\{ \\epsilon | R_{n} \\} \\{ R_{\\alpha} | 0 \\} $$\nWhere \\(R_{n}\\) denotes the translation along the Bravais lattice vector. On the other hand, a non-symmorphic group is expressed as \\( \\{ \\epsilon | R_{n} \\} \\{R_{\\alpha} | \\tau_{\\alpha} \\} \\) for nonzero \\(\\tau_{\\alpha}\\).\nThis suggests for a symmorphic group, we could write the space group operation into the product of translation and point group operation. This is in general does not apply to all space group. There is also a good reason to partition the space group into these two forms. This will carry over to the wavefunction in a crystal and we want to learn the degeneracy in the k-space by only the point group opeartion along.\nBloch Theorem to derive the realistic band structure (Free energy bands in accordance with symmetry restriction) Differed from the atomic case, in which the electron is under the spherical symmetric columbic potential, the electron in a crystal environment is under the influence of a periodic potential, which transform under the space group with translation subgroup. In this modified Hamiltonian, the energy level are labels by \\(k\\) and form a quasi-continuous bands in contrast to the energy level from the molecular solution.\nFrom the translation symmetry, we could write down a general wavefunction that satisfies this condition, and labeled the states with \\(k\\). The value of k, which labeled the irreducible representation, ranging from the number of tranlsation operation allowed given by the periodic boundary condition. The set of real space vector \\(R_{m}\\) defined the real space. While the \\(k_{i}= m_{i}2\\pi/ L\\) defined the reciprocal space.\nReciprocal Space For a plane wave solution that satisfies the periodical condition required by the given Bravais lattice, the sets of wave vectors \\(\\pmb{K}_{m}\\) must satisfies the following codition:\n$$ e^{i K_{m} \\cdot r}= e^{i K_{m} \\cdot r} $$\nholds for any \\(r\\), and for all \\(R_{n}\\) and \\(K_{m}\\) defining the real space and reciprocal space, respectively. For \\(R_{n} = \\sum{n_{i} a_{i}}\\) and \\(K_{n} = \\sum{n_{i}b_{i}}\\), it also suggests that\n$$ b_{j} \\cdot a_{i} = 2\\pi \\delta_{ij} $$\nBloch theorem For a electron being in a periodical lattice, this amounts to solving the following Schrodinger\u0026rsquo;s equation:\n$$ \\left(\\frac{\\hat{p}^{2}}{{2m}} + V(r) \\right) \\left| \\psi \\right\\rangle= E_{n} \\left| \\psi \\right\\rangle$$\nWith periodic potential\n$$ V(r+ T_{n}) = V(r) $$\nTo give a general form of the wavefunction that satisfies the potential under a crystalline environment, we note that the a crystal potential will follow from a space group symmetry. Since we derive from the previous section that space group has a translation subgroup, thus we could write the wavefunction into product of two parts: the first parts being the irreducible representaiton of the translation group, the other parts being a periodic function that transform under the space group.\nFrom a group theoretic perspective, the translation group is Abelian, so the number of the irreducible representaiton labeled by \\(k\\) will be the same with the number of translation allowed in a periodic boundary with length \\(L\\).\n$$ D^{k}(na) = e^{ikna/L} = e^{ik\\tau} $$\nWith \\(a\\) define as a Bravais vector and \\(n/L\\) labels the transformation operation.\nWe could write the second part as \\(u_{k}(r)\\), that has the same symmetry as the potential, \\(u_{k}(r+R_{n}) = u_{k}(r)\\). Together, by symmetry requirements the form of the solution is written in the form we called Bloch theorem:\n$$ \\psi_{k}(r) = e^{ik \\cdot r} u_{k}(r) $$\nThis is very different from the moelcular case, as in the molecular case we label the distinct states with three quantum number \\( (n, l, m)\\) while in this case crystalline case we labelled the states by the a quasi-continous variable \\(k\\) with a functional eigenvalues \\(E(k)\\) forming energy bands as we shall see in the next section.\nContinuous bands One features immediate follows if we consider \\(L\\) to be large and the number of translation in a crystall will generally be the order \\(\\sim 10^{23}\\) which furnish \\(\\sim 10^{23}\\) irreducible representation and distinct eigenvalues \\(E(k)\\).\nThis can be shown by expanding \\(k\\) to \\(k^{\\prime} = k + x\\) where \\(x\\) is small on the Schrodinger equations. Substitute, the Bloch wavefunction with the modified \\(k^{\\prime}\\) yields:\n$$ \\left(\\frac{\\hat{p}^{2}}{{2m}} + V(r) \\right) u_{k}(r)= \\left(E-\\frac{\\hbar^{2}x^{2}}{2m}\\right) u_{k}(r)$$\nThus, the energy eigenvalues will form a continuous distributed bands structure forming a very different landscape as compared to the molecular case.\n( Citation: 1973 Cracknell,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Wong,\u0026#32; K.\u0026#32; (1973). \u0026#32; The fermi surface: Its concept, determination, and use in the physics of metals. \u0026#32; Clarendon Press. ) Symmetries of the r-space, k-space and the bloch wavefunction The space group in the r-space, shows a complete description of the crystal spatial symmetry. These in general would have effects on the k-space and consequently affecting the bloch wavefunction.\nTo study these effects, we will investigate how the point group operation in space group carries over to the k-space and the wavefunction.\nk-space $$ R_{n} \\cdot K_{m} = 2\\pi N_{N_{1}} $$\n$$ P_{\\alpha} R_{n} \\cdot K_{m} = 2\\pi N_{N_{2}} $$\n$$ P_{\\alpha}^{-1} R_{n} \\cdot K_{m} = 2\\pi N_{N_{3}} $$\nSince constant do not transform, we could write:\n$$ P_{\\alpha} P_{\\alpha}^{-1} R_{n} \\cdot P_{\\alpha} K_{m} = 2\\pi N_{3} $$\nHence, it follows that opeartion \\(P_{\\alpha}\\) on r-space is equivalent to the operation of \\(P_{\\alpha}^{-1}\\) to the k-space:\n$$ P_{\\alpha}^{-1} R_{n} \\cdot K_{m} = R_{n} \\cdot P_{\\alpha} K_{m}$$\ngroup of k-vector The group of k-vector refer to all the point group operations acting on a single \\(k\\) vector yielding the set of \\(k\\) that transform to \\(k + K_{m}\\). For \\(k = 0\\), it has the enire symmetry of the point group. On the other extreme case, the point group operation of a general \\(k\\) vector will take to another distinctive \\(k\\) vector except for the idenity transformation. In this case the \\(k\\) vector only sees the translational symmetry.\nThe point group symmetry only takes effect on the \\(k\\) vector that resides on the symmetrical axis, then there will be more than one point group symmetrical operation that takes \\(k\\) to \\(k + K_{M}\\).\nBloch wavefunction For arbitrary \\(k\\) vector, the point group opeartion that acts on the wavefunction with \\(k\\) vector will take it to another distinct wavefunction.\n$$ P_{\\{R_\\alpha | 0 \\}} \\psi_{k}(r) = e^{iR_{\\alpha}k \\cdot r} u_{R_{\\alpha }k} (r) = \\psi_{R_{\\alpha }k}(r) $$\nFrom the relationship we derive earlier, \\(r \\cdot R_{\\alpha}k = R_{\\alpha}^{-1} r \\cdot k \\), we can prove that:\n$$ P_{\\{R_{\\alpha} | \\tau \\}} \\psi_{k}(r) = e^{iR_{\\alpha} k \\cdot \\tau} \\psi_{R_{\\alpha}k}(r) $$\n$$ P_{\\{R_{\\beta} | \\tau \\}} \\psi_{R_{\\alpha}k}(r) = e^{iR_{\\beta} R_{\\alpha} k \\cdot \\tau} \\psi_{R_{\\beta} R_{\\alpha}k}(r) $$\nThus, the set of eigenfunction \\( \\psi_{R_{\\alpha}k(r)} \\) obtained by taking the star of \\(k\\) spans the invariant subspace of the point group \\(g\\) since the product operation \\(R_{\\alpha}R_{\\beta}\\) is contained in \\(g\\).\nAlthough they the set of wavefunction will have the same energy eigenvalue, we do not consider them as degenerate. Instead, we write the function in the set, and consider the extra point group symetry to yield the relation \\(E(k) = E(R_{\\alpha}k)\\) for all \\(R_{\\alpha}\\). In this way, we guarantee that the energy \\( E(k) \\) will show the full point group symmetry of the reciprocal lattice.\nThe high symmetry point in the Brillouin zone are those \\(k\\) points that satisfy:\n$$ R_{\\alpha} k = k + K_{m} $$\nwhere \\(K_{m}\\) is the reciprocal lattice vector. At these points, the energy bands are tight together. As we move away from these high symmetry points, the band degeneracies are lifted to a general point in the Brillouin zone. This follows from a reduction in symmetry from a large group to its subgroup. The lifting of these degeneracies are called compatibility relations. We will introduce the concept of symmetry breaking and the calculation of reduction as a foundation to learn the compatibility relations.\nLifting of degeneracies from symmetry breaking When the group of the original Hamiltonian \\(G\\) is lowered by perturbation, the modified Hamiltonian will have a lower symmetry subgroup \\(G^{\\prime} \\in G\\). The effect, as we mentioned, will lift the degeneracies of the \\(G\\) by Wigner theorem. Since the wavefunction satisfies \\(H\\) will be the irreducible representation of the group \\(G\\). Then the irreducible representation will generally be a reducible representation of group \\(G^{\\prime}\\) and each irreducible representation in group \\(G\\) could be written as the linear combination of the irreducible representation of group \\(G^{\\prime}\\) hence the name reducible. This theme of group theoretic application in reduction in symmetry will be a recursive methodology in the study of quantum solids.\nSpherical symmetry to crystal space group For example, when atoms bond to form crystal, the rotational symmetry is broken. In exchange, the potential and the modified Hamiltonian will follow a space group symmetry which is a subgroup of the full rotation group. To relate the representaitons between these two groups, the irreducible representaiton of group \\(G\\) will be a reducible representations in \\(G^{\\prime}\\). And we could decompose the irreducible representation by character orthogonality required by group theory.\nReal space to the wavefunction So, to connect back to our conversation on the reciprocal space and wavefunction. We would like to know how the symmetry constrain acts on the reciprocal space and the Bloch wavefunction. The group of \\(k\\) vector will be the subgroup of the space group of the crystal lattice \\(G_{k} \\in G\\) and so we could parallel the method on we used previously to obtained how symmetry restrict the energy bands for different \\(k\\) values.\nIn a given Brioullion zone, we label the \\(k\\) points that have a high symmetry. For example, at \\(k=0\\), it has the highest symmetrical properties. As we move away from the symmetry points to some point with lower symmetry, that reduce the higher symmetry group to a lower symmetry group. This essentially will lift the energy degeneracy that was originally present in the high symmetry \\(k\\) points.\nMethod of crystal field splitting Before: n energy level and 2l+1 degenerate states. 2l+1 degenerate states becomes reducible representation of the space group, which is the combination of point group and translation group. In general, when a original Hamiltonian that follows a \\(G\\) symmetry reduces to a less symmetrical Hamiltonian with \\(G^{\\prime}\\) symmetry, the irreducible representation of \\(G\\) will be a reducible representation of \\(G^{\\prime}\\), lifting the degeneracy. (cite)\nIn a less rigorous fashion, assume a quantum system with Hamiltonian that has the symmetry of group \\(G\\), that would suggest:\n$$ G H \\left\\langle \\psi \\right\\rangle = E G \\left\\langle \\psi \\right\\rangle $$\nShow a procedure of how rotational group break into points group or space group. (work)\nFor example, consider the orthonomral group \\(O_{h}\\) with the following character table.\n(include table here)\nThese symmetrical operation, such as rotation and reflection, are the members of the \\(SO(3)\\) group. From the character of the full rotation group, we could calculate the characters for these symmetrical operation for the full rotation group:\n$$ \\frac{sin[(l+1/2)\\alpha]}{\\alpha/2} $$\nFor, \\(l = 2\\) and \\(\\alpha = 0\\), the character of the irreducible representation will be:\n$$ \\chi^{(2)}(\\alpha = 0) = 5 $$\nThese irreducible representation will generally be reducible in the \\(O_{h}\\) point group, partially lifting the \\(2l+1 = 5\\) degeneracies. To compute the corresponding weighting of each irreducible representation in the reducible representation, we make use the class orthogonality theorem and decompose them with the following:\n$$ a_{j} = \\frac{1}{h}\\sum_{k}{N_{k}\\chi^{\\Gamma_{j}} (C_{k})^{\\ast} \\chi^{reducible}(C_{k})} $$\nConsequently, for \\(A_{1}\\)\n$$ a_{A_{i}}= \\frac{1}{24}[5-8+3+6-6] = 0 $$\nSo, it does not decompose into the \\(A_{i}\\) representaiton. The non-zero summation suggests that the degeneracies that the broken symmetry has lift into. In this case of \\(l = 2\\), it was two-fold degeneracies of \\(E\\) and three-fold degeneracies \\(T_{2} \\) that the reducible representation breaks into.\nCompatibility How this group theory consideration carry over to the k space?\n(include an example and illustration)\nFor example, \\(Fm3m(O_{h})\\).\n(figure show first Brillion zone and corresponding symmetrical k points)\n(character table each k points symmetry)\n(example calculation)\n(graph of energy bands)\nTo explain the difference in the metals by the microscopic features of the metals. Differences including E.M. response, heat capacity, transport, superconductor transition. From a simplified model and spotting the important degree of freedom that correlates with the macroscopic observables.\nFrom simplified calculation yields the fermi surface. Then we say the fermi surface is a abstract representation of the active part of the materials. The question is, is the fermi surface a one to one correspondence to the properties of the metals. If that is the case, then the fermi surface does indeed reflect the faithful properties of the metal. Can we show that the relation is one to one? Microscopic information: number of electrons, the crystalline structure,\nUnder certain limit that don\u0026rsquo;t lead to , the external fields will change this representation of the metals. External simulus will alter the geometry of the fermi surface. Is it possible that by studying the fermi surface alone, we could infer the qualitative features of the metal. In addition, could we predict the change of the fermi surface by the external simulus, so we know the limit of the external field that would leads to a qualitative change of the metals.\nInformation of the fermi surface. Geometrical concepts. Information of a geometry. first is their distribution p(x, y, z) and their distribution relative to the first Brilloun zone. Closed or open surface. Discontinuity in the distribution. The derivative of the distribution. The discontinuity in the derivative of the distribution. The second derivative\u0026hellip; Curvature. Is there a general way to relate the physical properties to the geometrical content?\nConclusion References: Cracknell\u0026#32;\u0026amp;\u0026#32;Wong (1973) Cracknell,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Wong,\u0026#32; K.\u0026#32; (1973). \u0026#32; The fermi surface: Its concept, determination, and use in the physics of metals. \u0026#32; Clarendon Press. ","permalink":"https://htsod.github.io/posts/group_solid/","summary":"This investigation started in the consideration of quantum solids which invariably have to solve for a many-bodies quantum problem. Many particles quantum mechanics, just like its classical counter parts, is desparately intractable. The exactly solvable problem, to name a few, are the free particle, harmonics oscilator and hydrogen atom. The question is then, to solve the many-particles quantum problem with the tools we have in these solvable models. All these solvable model has a potential energy term that is invariant under a symmetrical transformation, thus allowing for specifying the quantum system with integers labeling the irreducible representation.","title":"Group Theory Application in Quantum Mechanics"},{"content":"Introduction Free Electron Model: Sommerfield Model Bands structure tells us the availible states in a unit cell. The number of electrons in a unit cell determine the occupation of those bands. One way to evaluate these effect is through the examination of the Fermi surface, which determine the equal energy surface of the electrons.\nFree electron surface Sommerfeld free electron model describe \\( N \\) number of fermions without any interaction, not even the columbic interaction between those \\(N\\) electron. Therefore, the model is more like the study of fermions statistics without charge.\n) The statistical properties of fermions with a occupation function of\n$$n_{F}(\\beta (E-\\mu))= \\frac{1}{e^{\\beta (E-\\mu)}+1}$$\nAlso meaning the number of electrons allowed at energy \\( E \\) and chemical potential \\( \\mu \\).\nFor free fermions, the energy is characterized by\n$$\\epsilon (\\vec{k}) = \\frac{\\hbar^{2}|\\vec{k}|^{2}}{2m}$$\nwhere \\(\\vec{k}\\) representing the wavevectors.\nSumming all the fermions at dffierent energy or equilvalently different wavevector \\( \\vec{k} \\)yields the total number of fermions\n$$ N = 2 \\sum_{\\vec{j}}{n_{F}(\\beta (\\epsilon (\\vec{k}))-\\mu)} $$\nNotice the multiple of two take the two spin states into account. Subsequently, we take the continuum limit of the summation \\( \\sum_{\\vec{k}} \\rightarrow \\frac{V}{(2\\pi)^{3}\\int{}} \\) and we get:\n$$ N = 2 \\frac{V}{(2\\pi)^{3}} \\int{d\\vec{k} n_{F}(\\beta (\\epsilon (\\vec{k})) - \\mu)} $$\nThe tricky part in evaulating this equation is that the temperature dependence of \\( \\mu \\) changes the qualitative feature of the occupation function, thus changing the numerical result of the integral.\nInclude graph here.\nTo simplify the anaysis, we evaulaute the limit of zero temperature. The resulting integral leads to step function:\n$$ N = 2\\frac{V}{(2\\pi)^{3}}\\int{d\\vec{k} (E_{F}- \\epsilon (\\vec{k}))}= 2\\frac{V}{(2\\pi)^{3}}\\int^{|\\vec{k}|\u0026lt;k_{F}}{d\\vec{k}} $$\nSumming over the \\(k\\) sphere yield the number of fermion in a low temperature limit:\n$$ N=2\\frac{V}{(2\\pi)^{3}}\\left( \\frac{4}{3}\\pi k^{3}_{F} \\right) $$\nThis is saying that in a universe that is only filled by a fixed number of fermions without interaction will share states in a spherical k-space ball with radius \\( k_{F} = (3\\pi^{2}n)^{1/3} \\) where \\(n= N/V\\) is the density.\nThe corresponding Fermi Energy \\(E_{F}= \\frac{\\hbar k_{F}^{2}}{2m}\\) is the energy on the Fermi surface.\nConnection between bands structure and the Fermi surface number of electrons distinguish metals non-metals. Overlapping with Broliou zone boundary. Experimental result related to the Fermi surface.\nPeriodic Potential and Lattice Formation of Lattice Transition to a energy favourable states In the high temperature limit \\(T\\) of the free energy with form \\(F = U - TS\\), the entropy \\( S\\), a quantity that gives measure to the disorder of the system, would tend to increase in order to minimize the free energy to a equilibrium state. In contrast, at the low temperature limit, the energy terms will be dominant which, a equilibrium state try to minimize. Hence, at low temperature state, a lowset energy state is favoured.\nRegular lattice as the lowest energy state Following this line of logic, from the transition from liquid to solid, the atoms will seek for the lowest energy state with pair interaction potential:\n$$ V_{tot} = \\sum_{ij}{(|r_{i}-r_{j}|)}$$\nWhich is the pair interaction between all combination of atoms. For all the possible configurations make up the sets:\n$$ C = \\{ r_{1}, r_{2},\u0026hellip;,r_{N} \\} $$\nAssume a local minimum configuration of \\(C_{optimum}\\) for \\(n\\) particles where small displacement will lead to harmonic potential that restores it:\n$$ V-V_{min} = \\frac{1}{2} \\sum_{ij}{V_{ij}^{\\prime \\prime} \\delta r_{i} \\delta r_{j}} $$\nHence, any displacement of atoms away from this minimum would have a increase of energy from the contribution of all the particles \\(n\\).\nNow consider a larger array of atoms \\(N=mn\\). A low-energy configuratins might be constructed by stacking boxes of \\(n\\) atoms together and remving the interior walls. This removal of wall will lead to a readjustment of position only scales as a surface energy, of order \\(n^{2/3}\\). Whereas to to modify the interior in a substantial way will have energy scale as \\(\\approx n\\), so there will be a cell size, \\(n\\), beyond which it will not pay to modify the internal configuratin; this then gives us a regular array.\nSpace group, Star of k and Compatibility Thermodynamics properties and transport properties of metals Justification to the semi-classical approach to the quasiparticles and measurement in general to tackle thermodynamics and transport property.\nThermodynamics property Density of state calculate from the fermi surface. With relation to the geometry.\nThermodynamics of conduction electrons\nQuantun Oscillation and Oscillation in general (weak field, strong field)\nTransport property Boltzemann equation with approximation from the fermi surface. With relation to the geometry.\nConductivity, thermo and electrical\nGalvanomagnetic phenomena, open and closed trajectory\nSkin effect\nSchemes to calculate fermi surface: theoretical and experimental Intrinsically n body, can\u0026rsquo;t predict the exact form of potential. Combination of theoretical and experimental tool.\nIntroduce some general theoretical calculation methods Free electron as first approximation\nSeveral other methods\nGreen method.\nPath Integral\nHarison Model\nExperimental technique to probe the fermi surface making use of quantum oscillation\nHarison Model\nApproximation method ab inito. Result and illustration from this method. Some analysis about the correlation between properties and fermi surface\nGeometrical interpretation of the fermi surface Geometry analysis. Points, lines and plane on fermi surface. Discontinuity in the fermi surface and breifly mention of ETT. Effective mass. Plane selection. Close and open orbit in Magnetic field. Hall effect.\nExample analysis of metals/conductors: Bismuth\nTransition metals\nGraphene\nDistingusih and scheme to classify materials and more accurate description to different metallic materials. To study materials in a lower dimensionality.\n","permalink":"https://htsod.github.io/posts/fermi_surface/","summary":"Introduction Free Electron Model: Sommerfield Model Bands structure tells us the availible states in a unit cell. The number of electrons in a unit cell determine the occupation of those bands. One way to evaluate these effect is through the examination of the Fermi surface, which determine the equal energy surface of the electrons.\nFree electron surface Sommerfeld free electron model describe \\( N \\) number of fermions without any interaction, not even the columbic interaction between those \\(N\\) electron.","title":"Fermi suface: A Geometry Interpretation of Conductors"},{"content":"Fourier Methods could be derived entirely from Group theory! As the title suggests, the entire concept of the Fourier transform can be derived if we understand some basic group theory. We start by introducing the group \\(Z_{N}\\) and its irreducible represenations. Using the orthogonality theorem, we will then derive the discrete Fourier transform (DFT) and the Fourier transform (FT).\nFourier analysis studies the periodicities of functions. Any continuous and differentiable function can be broken down into a linear combination of its frequency components, which is the foundation of Fourier series. The Fourier transform allows us to switch between real space and frequency space representations.\nApproaching this from group theory, we recognize that an arbitrary function is a reducible representation of the group \\(Z_{N}\\), or the group of integers modulo \\(n\\). By framing the description in this way, we essentially abstracted the Fourier method as a representaion of the \\(Z_{n}\\) symmetries.\n\\(Z_{n}\\) is the abelian discrete group of order \\(n\\). It captures the group structure of the periodic pattern on a interval of \\( n \\). The reducibility of a periodical function means that the representation could descompose into linear combination of the irreducible representation of the group \\(Z_{n}\\).\nTaking a simple example to illustrate the idea of reducibility. Consider an ar function \\(F(x)\\) in a real space that satisfies the periodic condition\n$$ F(x) = \\sum_{n} w_{n} f(x+na) = F(x) $$\nfor \\(n = 0, \\pm1, \\pm2, \u0026hellip;\\).\nA sinusoidal function as one of the fourier mode \\(f(x) = Asin(\\frac{2\\pi x}{a})\\) satisfies the above condition for \\(n = \\pm 1\\) with weight \\(w_{1} = A\\). In group theory we consider \\(F(x)\\) as the reducible representation of \\(Z_{n}\\) with \\(f(x)\\) being the irreducible representation of \\(Z(n = \\pm 1)\\).\nA slight hint of universality Light and sound waves, for example, can be broken down into Fourier components regardless of the difference in the microscopic details. For example, light is a form of electromagnetic wave with the underlying symmetry of continuous Lorentz group \\(SO(1, 3)\\). As withh the propagation of sound wave, it relies on the homogenity medium such as air or solid that has a continuous translation symmetry. Perturbing the system in a certain direction we break the continuous symmetry by favouring a direction, triggered the \u0026ldquo;soft mode\u0026rdquo; of the system causing a relaxation dynamics that leads to the a propagating Fourier wave. It will be interesting to investigate these symmetries breaking effect but that is another topics for another day.\nIn the next section, we’ll briefly touch on results from representation theory and use them to derive some key theorems related to the Fourier method.\nRepresentation Theory: The Core Tool One of the most significant results in group theory is Schur\u0026rsquo;s lemma. It states that if an operator \\(H\\) commutes with a group representation \\(D(g)\\), then \\(H\\) must be diagonal, with \\ \\(H = \\lambda I\\). Using this lemma, we can derive some powerful theorems in group representation theory. (For a slow derivation, see chapter 2 on ( Citation: Zee,\u0026#32;2016 Zee,\u0026#32; A.\u0026#32; (2016). \u0026#32; Group theory in a nutshell for physicists. \u0026#32; Princeton University Press.\u0026#32;Retrieved from\u0026#32; https://books.google.co.jp/books?id=FWkujgEACAAJ ) or a get-to-the-point derivation see chapter 2 on ( Citation: Dresselhaus\u0026#32;\u0026amp;\u0026#32;Dresselhaus,\u0026#32;2002 Dresselhaus,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Dresselhaus,\u0026#32; G.\u0026#32; (2002). \u0026#32; Group theory: Application to the physics of condensed matter. \u0026#32; Springer. ) )\n$$ \\sum_{g}{D^{(r)\\dagger} (g)_ {j}^{i}D^{(s)}(g)_ {k}^{l}} = \\frac{N(G)}{d}\\delta_{l}^{i}\\delta_{j}^{k} $$\nThis relation holds when summing over group element \\(g\\) and representations \\((r)\\) and \\((s)\\) are orthogonal to each other. Some important corollaries include:\nDimensions of the irreducible representations: $$ \\sum_{r}{d_{r}^{2}} = N(G) $$\nOrthogonality of irreducible representations: $$ \\sum_{c}{n_{c}(\\chi^{(r)}(c))^{\\ast} \\chi^{(s)}(c)} = N(G) \\delta ^{rs} $$\nOrthogonality of classes: $$ \\sum_{r}{\\chi^{(r)}(c)^{\\ast} \\chi^{(r)}(c^{\\prime})} = \\frac{N(G)}{n_{c}}\\delta^{cc^{\\prime}}$$\nNumber of irreducible representation equal to the number of class in the group:\n$$ N(C) = N(R) $$\nA test on reducibility: $$ \\sum_{c}{n_{c} \\chi^{\\ast(r)}\\chi(c)} = N(G)n_{r} $$\nSymmetry and \\(Z_{N}\\) Our first exposure to the Fourier transform is often linked to periodicity, a pattern repeating in time. However, group theory doesn\u0026rsquo;t care if the pattern is in time or any other variable, as long as the symmetry holds. Consider partitioning a rotation of \\(2\\pi\\) into \\( N \\) parts on the complex plane, represented as \\(e^{i\\frac{2\\pi}{N}j}\\) for \\(j = 0, 1, \u0026hellip;, N-1\\). This is the \\(Z_{N}\\) group where \\(g^{N} = I\\). Since \\(Z_{N}\\) is an abelian, each element forms its own class, and each irreducible representation has a dimension of 1.\nBy guessing an irreducible representation that satisfies the multiplication structure: $$ D(e^{i\\frac{2\\pi}{N}j})D(e^{i\\frac{2\\pi}{N}j^{\\prime}}) = D(e^{i\\frac{2\\pi}{N}(j+j^{\\prime})}) $$\nwe propose:\n$$ D(e^{i\\frac{2\\pi}{N}j}) = e^{i\\frac{2\\pi}{N}jk} \\quad k = 0, 1, \u0026hellip;, N-1$$\nNow, using orthogonality:\n$$ \\frac{1}{N}\\sum_{j=0}^{N}{e^{-i\\frac{2\\pi}{N}jk} e^{i\\frac{2\\pi}{N}jk^{\\prime}}} = \\delta^{kk^{\\prime}} $$\nThis gives us the discrete Fourier transform (DFT) and its inverse.\nThis is the same old discrete fourier transform and inverse discrete fourier transform but entirely from group theory along.\nOkay, but how about the idea that for a given function that compose of different vibration mode. Recall that we could break down that function into weighted component of fourier parts. What do group theoretic approach has to do with this?\nWe recognize this given function as an reducible representation that contains various irreducible representation \\(n_{r}\\) times.\nSo, given the reducible representation \\(\\chi(j)\\) applying the test of reducibility gives the weight of the fourier component \\( n_{k} \\)\n$$ \\frac{1}{N} \\sum_{k=0}^{N-1}{e^{-i\\frac{2\\pi}{N}jk} \\chi(j)}= n_{k} $$\nThen, the reducible representation can be written as the sum of each of irreducible representation with weight \\( n_{k} \\)\n$$ \\chi(j) = \\sum_{k=0}^{N-1}{n_{k}e^{i\\frac{2\\pi}{N}jk}} $$\nAlso known as the Fourier series.\nContinum limit: from \\(Z_{N}\\) to \\(U(1)\\) and from DFT to FT To extend the DFT to the continuous Fourier transform (FT), we take the limit \\(N \\rightarrow \\infty\\). The discrete group \\(Z_{N}\\) becomes \\(U(1)\\), and the summation becomes integration:\n$$\\sum_{k=0}^{N-1} \\rightarrow \\int_{0}^{\\infty}d\\mu(g)$$\nwhere \\(\\mu(g)\\) is the group measure.\nTo evaluate the orthogonality relation in the continuous case, one has to find the trace and the group measure as demonstrated as follow.\nfinding the trace \\(U(1)\\) group has the property that \\(U^{\\dagger}U = I\\). For \\(U(1)\\), it furnishes one dimension representation only and hence the trace of the representation is just the representation itself. We propose that \\(D(\\theta) = \\chi(\\theta, k) = e^{i\\theta k}\\). It clearly satisfies the unitary condition and preserve the algebraic structure of the group multiplication. The only difference is that now \\(\\theta\\) runs as continuous variable from \\(0 \u0026lt; \\theta \u0026lt;2\\pi\\).\nfinding the group measure The purpose of finding a group measure is to make the integral cover the group manifold. Hence, we could intepret the group measure \\(d\\mu(g)\\) runs over the group manifold. In this trivial case of \\(U(1)\\), the group manifold is merely a cycle and hence to cover the manifold we propose the following integral \\(\\int_{0}^{2\\pi}{d\\theta}\\).\nApplying orthogonality, we obtain:\n$$ \\int_{U(1)}{\\chi^{\\ast}(k,g)\\chi(k^{\\prime},g)d\\mu(g)} = \\int_{0}^{2\\pi}{e^{-i\\theta k^{\\prime}}e^{i\\theta k}d\\theta} = 2\\pi \\delta_{kk^{\\prime}}$$\nThus, we arrive at the Fourier transform.\nFinal Remark To summarize, from group theory, we recover the following results:\nFrom Discrete Group\nDFT:\\( \\frac{1}{N}\\sum_{j=0}^{N}{e^{-i\\frac{2\\pi}{N}jk} e^{i\\frac{2\\pi}{N}jk^{\\prime}}} = \\delta^{kk^{\\prime}} \\)\nInverse DFT:\\( \\sum_{k=0}^{N}{e^{-i\\frac{2\\pi}{N}jk} e^{i\\frac{2\\pi}{N}j^{\\prime}}} = \\delta^{jj^{\\prime}} \\)\nFourier Series: \\( \\chi(j) = \\sum_{k=0}^{N-1}{n_{k}e^{i\\frac{2\\pi}{N}jk}} \\)\nFrom Continuous Group\nFourier Transform: \\( \\int_{U(1)}{\\chi^{\\ast}(k,g)\\chi(k^{\\prime},g)d\\mu(g)} = \\int_{0}^{2\\pi}{e^{-i\\theta k^{\\prime}}e^{i\\theta k}d\\theta} = 2\\pi \\delta_{kk^{\\prime}} \\) One advantage of viewing the Fourier method through the lens of group theory is that it reveals Fourier methods as simply a consequence of translational symmetry. This perspective allows for potential generalizations to other orthogonality theorems, such as those based on symmetries like \\(SU(3)\\) in particle physics, which underpins the Standard Model.\nBibliography Dresselhaus\u0026#32;\u0026amp;\u0026#32;Dresselhaus (2002) Dresselhaus,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Dresselhaus,\u0026#32; G.\u0026#32; (2002). \u0026#32; Group theory: Application to the physics of condensed matter. \u0026#32; Springer. Zee (2016) Zee,\u0026#32; A.\u0026#32; (2016). \u0026#32; Group theory in a nutshell for physicists. \u0026#32; Princeton University Press.\u0026#32;Retrieved from\u0026#32; https://books.google.co.jp/books?id=FWkujgEACAAJ ","permalink":"https://htsod.github.io/posts/fourier/","summary":"Fourier Methods could be derived entirely from Group theory! As the title suggests, the entire concept of the Fourier transform can be derived if we understand some basic group theory. We start by introducing the group \\(Z_{N}\\) and its irreducible represenations. Using the orthogonality theorem, we will then derive the discrete Fourier transform (DFT) and the Fourier transform (FT).\nFourier analysis studies the periodicities of functions. Any continuous and differentiable function can be broken down into a linear combination of its frequency components, which is the foundation of Fourier series.","title":"Fourier Transforms: A Group Theoretic Perspective"},{"content":" Anyone is feeling weird? The contradiction between classical physics and microscopic phenomena is one of the most fascinating episodes in the history of science, reshaping our fundamental understanding of waves and particles. To grasp the weirdness of wave-particle duality, let’s start with a simple analogy.\nImagine shooting bullets at a wall with two equally spaced gaps. As expected, the bullets passing through each gap will behave independently, forming two distinct patterns on a measurement panel behind the wall. Each pattern would look like a Gaussian distribution centered around the corresponding slit. The resulting measurement would be a straightforward combination of these two distributions:\n$$ \\rho_{12}(x) = \\rho_{1}(x) + \\rho_{2}(x) $$\nThis is because the events are independent, and the densities of bullet impacts simply add up.\nNow, let’s replace the bullets with waves—perhaps water waves or sound waves—propagating through the same two slits. Experimentally, something entirely different occurs. Instead of the waves passing independently through the slits, they interfere with each other, producing a distinctive interference pattern on the panel.\nIf we describe the waves mathematically, let \\( h_{1}(x)e^{i2\\pi \\nu t} \\) and \\( h_{2}(x)e^{i2\\pi \\nu t} \\) represent the waves passing through the first and second slits, respectively. When both slits are open, the total wave is: $$ (h_{1}(x) + h_{2}(x))e^{i2\\pi \\nu t} $$\nThe intensity of this combined wave is given by the square of the total wave function:\n$$ I_{12} = |h_{1}(x) + h_{2}(x)|^{2} = |h_{1}(x)|^{2} + |h_{2}(x)|^{2} + h_{1}(x)h_{2}^{\\ast} + h_{1}(x)^{\\ast}h_{2} $$\nThis reveals something new: the presence of interference terms. Unlike the case with bullets, where the results simply add, waves interact, creating regions of constructive and destructive interference:\n$$ I_{12}(x) = I_{1}(x) + I_{2}(x) + I_{interference}(x) \\neq I_{1}(x) + I_{2}(x) $$\nThe Wave-Particle Mystery This wave-particle duality extends beyond classical waves. Experiments with electrons—and more recently, with larger molecules like \\( C_{60} \\) (the largest known entity to show this duality)—reveal that quantum particles exhibit both wave-like and particle-like behaviors. As the famous physicist Richard Feynman put it:\n\u0026hellip; a phenomennon which is impossible, absolutely impossible, to explain in any classical way, and which has in the heart of quantum mechanics, \u0026hellip; We can not make the mystery go away by \u0026rsquo;explaining\u0026rsquo; how it works. We will just tell you how it works.\nThis duality isn’t the only strange aspect of quantum mechanics. Quantum effects are also evident in blackbody radiation and spectroscopy. In the case of blackbody radiation, it became necessary to assume that energy levels are discrete to match experimental data. In spectroscopy, quantum theory governs the probability of transitions between these discrete energy levels.\nTwo Paths to Quantum Mechanics: Schrödinger and Heisenberg Faced with these puzzling phenomena, physicists developed two different mathematical frameworks to describe quantum mechanics. The first was the matrix mechanics approach, formulated by Heisenberg, Born, and Jordan. The second was Schrödinger’s wave mechanics, which, despite starting from a different perspective, led to the same numerical results.\nFor the sake of clarity in our ongoing discussion of wave-particle duality, we will first explore Schrödinger’s wave equation, which builds directly on the wave-like nature of quantum systems. Heisenberg’s matrix mechanics, while equally valid, feels more like the \u0026ldquo;black magic\u0026rdquo; of theoretical physics—it’s powerful but requires more effort to follow. We’ll dive into that after setting the stage with Schrödinger\u0026rsquo;s more intuitive approach.\n( Citation: Zeng,\u0026#32;2008 Zeng,\u0026#32; J.\u0026#32; (2008). \u0026#32; 量子力学教程. \u0026#32; 科学出版社.\u0026#32;Retrieved from\u0026#32; https://books.google.co.jp/books?id=foQcPwAACAAJ ) The \u0026ldquo;Wave Equation\u0026rdquo; Approach to Quantum Mechanics: Abandoning Preconceived Notions Louis de Broglie proposed that every particle possesses a wavelength, which is related to its wave vector \\( |\\vec{k}| = 2\\pi \\lambda \\).\nFor a free particle, the energy is given by \\( E = p^{2}/2m \\). We can relate energy to angular frequency \\( \\omega \\) using \\( w=E/\\hbar \\) connect the wave vector \\( |\\vec{k}| \\) to momentum through \\( \\vec{k} = \\vec{p}/\\hbar \\). Schrödinger found inspiration in these ideas as he sought to explain wave-particle duality.\nHis journey toward the formulation of the Schrödinger equation began with a question from P. Debye:\nYou speak about waves, but where is the wave equation?\nHistorically, Schrödinger derived the wave function from the classical action principle. However, we will focus directly on the wave function ansatz to derive the Schrödinger equation.\nDeriving the Schrödinger Equation Consider a free particle with energy \\( E = \\frac{p^{2}}{2m} \\). According to de Broglie\u0026rsquo;s principle, this particle has an angular frequency \\( w = \\frac{E}{\\hbar} \\) and a wave vector \\( \\vec{k} = \\frac{\\vec{p}}{\\hbar} \\). We can suggest an ansatz for the wave function as a plane wave:\n$$ \\psi(r,t) \\sim e^{i(\\vec{k}\\cdot\\vec{r}-wt)} = e^{i(\\vec{p}\\cdot\\vec{r} -Et)/\\hbar} $$\nFrom this, we can derive:\n$$ i\\hbar \\frac{\\partial}{\\partial t}\\psi = E \\psi $$ $$ -i\\hbar \\nabla \\psi = \\vec{p}\\psi, -\\hbar^{2}\\nabla ^{2} \\psi = p^{2}\\psi $$\nFor a free particle, since \\(E = p^{2}/2m\\), we can equate these to obtain:\n$$ \\left( i\\hbar \\frac{\\partial}{\\partial t} + \\frac{\\hbar^{2}}{2m}\\nabla^{2} \\right)\\psi = \\left( E - \\frac{p^{2}}{2m} \\right)\\psi = 0$$\nThis leads to the Schrödinger equation for a free particle:\n$$ -i\\hbar \\frac{\\partial}{\\partial t} \\psi = \\frac{\\hbar^{2}}{2m}\\nabla^{2} \\psi $$\nSuperposition of Solutions The equation is linear, allowing for the superposition of plane wave solutions:\n$$ \\psi(\\vec{r}, t) = \\frac{1}{(2\\pi \\hbar)^{3/2}} \\int{\\varphi(\\vec{p}) e^{i(\\vec{p}\\cdot\\vec{r} - Et)/\\hbar} d^{3}p} $$\nSubstituting this expression confirms that it satisfies the Schrödinger equation:\n$$ i\\hbar \\frac{\\partial}{\\partial t}\\psi = \\frac{1}{(2\\pi \\hbar)^{3/2}}\\int{\\varphi(\\vec{p}) E e^{i(\\vec{p}\\cdot\\vec{r} - Et)/\\hbar} d^{3}p } $$\n$$ -\\frac{\\hbar^{2}}{2m}\\nabla^{2} \\psi = \\frac{1}{(2\\pi \\hbar)^{3/2}}\\int{\\varphi(\\vec{p}) p^{2} e^{i(\\vec{p}\\cdot\\vec{r} - Et)/\\hbar} d^{3}p } $$\nThus, we find:\n$$ \\left( i\\hbar \\frac{\\partial}{\\partial t} + \\frac{\\hbar^{2}}{2m}\\nabla^{2} \\right)\\psi = \\int{\\varphi(\\vec{p}) \\left( E - \\frac{p^{2}}{2m} \\right)e^{i(\\vec{p}\\cdot\\vec{r} - Et)/\\hbar} d^{3}p } = 0$$\nAny linear combination of wave packets will satisfy the Schrödinger equation for free particles.\nPromoting Observables to Operators In this derivation, we promote observables namely energy and momentum to operators acting on the wave function:\n$$ E \\rightarrow i\\hbar \\frac{\\partial}{\\partial t}, \\vec{p} \\rightarrow -i\\hbar \\nabla $$\nThe Case of a Particle in a Potential Field Now, let’s consider a particle in a potential field \\( V(\\vec{r}) \\). Based on the nonrelativistic relation of total energy:\n$$ E= \\frac{1}{2m}p^{2} + V(\\vec{r}) $$\nPromoting these quantities to operators yields Schrödinger\u0026rsquo;s equation:\n$$ i\\hbar \\frac{\\partial}{\\partial t}\\psi(\\vec{r}, t) = \\left[ -\\frac{\\hbar^{2}}{2m}\\nabla^{2} + V(\\vec{r})\\right] \\psi(\\vec{r}, t) $$\nImplications of the Schrödinger Equation wave-particle duality: The Schrödinger equation offers a fresh interpretation of matter and waves. In this framework, we retain properties such as mass and charge while moving away from classical trajectories—something we can never truly observe at the microscopic level. Instead, we adopt a probabilistic view of reality. Measurements on a dynamical system with observable \\(O(x)\\) are now defined probabilistically: $$ \\int_{-\\infty}^{\\infty}{\\psi^{\\ast}(x) O(x) \\psi(x)dx} $$\nHere, the statistical interpretation of the wave function leads to the normalization condition:\n$$ \\int_{-\\infty}^{\\infty}{\\psi^{\\ast}(x)\\psi(x)dx } = 1 $$\ndiscrete energy level The Schrödinger equation naturally leads to discrete energy levels due to \u0026ldquo;boundary conditions\u0026rdquo; imposed on \\( \\psi \\) when confined in an infinite potential well. The allowed energy states, or \u0026ldquo;characteristic values,\u0026rdquo; arise from the requirement that the wave function remains bounded. For example, in polar coordinates, the ordinary differential equation has singularities at \\(r=0\\) and \\(r = \\infty\\). The solutions are constrained to those that remain bounded, resulting in discrete energy levels.\nThe matrix calculus approach to quantum mechanics: only the measureables matter The gut of Heisenberg vec attempt to derive his quantum perspective is best summarized the following phrase\nDiscard all hope of observing hitherto unobservable quantities\nIn the case of the elecctron, we dispose unobservable quantities such as the position and period in the theory but leaving behind observables such as energy in stationary states together with the associated frequencies defined only upon two variables that characterized the transition\n$$ w(n, n - \\alpha) = \\frac{1}{\\hbar} { W(n) - W(n - \\alpha) } $$\nwhich shows the algebraic structure of the frequency \\(w\\) with the transition in energy level \\(W\\).\nFrom the perspective of Heisenberg, there is not thing wrong with the classical theory, it is the classical variables have to redefine to match the algebraic structure of a quantum variable\nCarrying on with this logic, Heisenberg promote kinetic variables with the observables he defined to derive the quantum equivalent of some classical theories. For example\nA simple one-dimensional model of an atom consisting of an electron undergoes periodic motion For a state characterized by the label \\( n \\), fundamental frequency \\( \\omega(n) \\) and coordinate \\( x(n, t) \\), ne can represent \\(x(n,t) \\) as a Fourier series\n$$ x(n,t) = \\sum_{\\alpha=-\\infty}^{\\infty}{X_{\\alpha}(n)exp[i\\omega(n)\\alpha t]} $$\nThen, Heisenberg asks the question: \u0026lsquo;how is the quantity \\(x(t)^{2}\\)\u0026rsquo; to be represented?\u0026rsquo;. In classical theory, it would be\n$$ [x(t)]^{2} = \\sum_{\\alpha}{\\sum_{\\gamma}{X_{\\alpha}(n)X_{\\gamma}(n)e^{i\\omega(n)(\\alpha + \\gamma)t}}} = \\sum_{\\beta}{Y_{\\beta}(n)e^{i\\omega(n)\\beta t}}$$\nIn classical theory, the frequencies simply add up. The resulting algebric structure is then\n$$ Y_{\\beta}(n) = \\sum_{\\alpha}{X_{\\alpha}(n)X_{\\beta - \\alpha}(n)} $$\n$$ \\omega(n)\\beta = \\omega(n)\\alpha + \\omega(n)(\\beta - \\alpha) $$\nNote that these quantities could not be combined in the same way in the case of quantum, one crucial different is how the frequencies would combine like\n$$ \\omega(n, n-\\alpha) + \\omega(n-\\alpha, n-\\beta) = \\omega(n, n-\\beta) $$\nHence, promoting these varibales to quantum compatibles, yield the following algebraic relation\n$$ Y(n, n-\\beta) = \\sum_{\\alpha}{X(n, n-\\alpha)X(n-\\alpha, n-\\beta)} $$\nAlso known as the Heisenberg\u0026rsquo;s law for multiplying transition amplitude together. One profound characterisitics is that these promoted quantities don\u0026rsquo;t commute. These non-commutativity define the Heisenberg algrebra that could essentially allow us to solve discrete energy eigenvalues.\nLet\u0026rsquo;s see how this commutation relation be applied define the momentum and position operator to be \\(p = i\\hbar \\nabla \\) and \\( q = x \\). Its communtation relation \\( [p, q] = pq - qp = i\\hbar \\) can be verified by acting on a wavefunction \\(\\psi(x)\\).\n$$ [p, q] = -i\\hbar \\frac{\\delta }{\\delta x}(x\\psi(x)) + i \\hbar x \\frac{\\delta }{\\delta x} {\\psi(x)} $$ $$ = -i\\hbar \\psi -i\\hbar x \\psi ^{\\prime} + i\\hbar x\\psi^{\\prime} = -i \\hbar \\psi = -i \\hbar $$\nIgonoring the planck constant \\(\\hbar\\) we yield the Heisenberg algebrba \\( [ p, q ] = i\\). We move a step forward defining an operator \\(a, a^{\\dagger}\\) from the Heisenberg algebra\n$$ a = \\frac{1}{\\sqrt{2}}(q+ip), a^{\\dagger} = \\frac{1}{\\sqrt{2}}(q-ip) $$\nThe communtation relation \\( [a, a^{\\dagger}]\\) known as Dirac algebra. Defining the Hermitian operator \\(N = aa^{\\dagger}\\). It could be diagonlized with eigenvalues of \\(n\\) and eigenvector \\(\\left| n \\right\\rangle \\).\nConsider the following communtation relation:\n$$ [a, N ] = aa^{\\dagger}a - a^{\\dagger}aa= (aa^{\\dagger} - a^{\\dagger}a)a = [a, a^{\\dagger}]a$$\nHence,\n$$ Na \\left | n \\right\\rangle = (aN - a) \\left | n \\right\\rangle = (n-1)a \\left |n \\right\\rangle$$\nThus \\(a \\left | n \\right\\rangle\\) is an eigenstate of \\( N \\) with eigenvalue equal to \\((n-1)\\).\nWrite the state \\( a\\left | n \\right\\rangle = C_{n} \\left | n-1 \\right\\rangle \\) with \\(C_{n}\\) some normalization factor. Hermitian conjuating both side yields \\( \\left\\langle n \\right| a^{\\dagger} = \\left\\langle n-1 \\right| C_{n}^{\\ast}\\). Squaring \\(a\\left| n \\right\\rangle\\)\n$$ \\left\\langle n \\right| a^{\\dagger}a \\left | n \\right\\rangle = \\left\\langle n\\right| N \\left| n \\right\\rangle = n = \\left\\langle n-1 \\right | |C_{n}^{2}| \\left | n-1 \\right\\rangle$$\nHence, the normalization factor \\(C_{n} = \\sqrt{n}\\) and the recursion relation\n$$ a \\left | n \\right\\rangle = \\sqrt{n-1} \\left| n -1 \\right\\rangle$$\nand for \\(a^{\\dagger}\\), simiarly\n$$ a^{\\dagger} \\left | n \\right\\rangle = \\sqrt{n+1} \\left| n + 1 \\right\\rangle $$\nFor \\(n\\) being a positive integer, it suggests there exists an eigenvector \\( a\\left| 1 \\right\\rangle = \\left| 0 \\right\\rangle \\) and unbounded upper eigenvector. There are two observation to be made from this creation and annihilation operators approach.\nA inifinite dimension matrix: Hilbert space The Dirac algebra can be relaized in terms of an infinite-dimensional matrix \\(A\\) with element \\(A_{n-1, n} = \\left\\langle n-1 \\right | a \\left| n \\right\\rangle \\) above the diagonal. So, does the operators \\(p\\) and \\(q\\) in the Heisenerg algebra. The operators in quantum mechanics live in the infinite dimensional space which we call it the Hilbert space.\nTop-down derivation of harmonics potential The eigenvalues of the harmonics potential is given by \\(\\frac{1}{2}(N + 1)\\). Starting from here and assuming we do not know the functional structure of the harmonics potential,\n$$ H = \\frac{1}{2}(N + 1) = \\frac{1}{2}a^{\\dagger}a + \\frac{1}{2} = \\frac{1}{2}\\left( (q-ip)(q+ip) + 1 \\right) $$ $$ H = \\frac{1}{2}(p^{2} + q^{2}) = -\\frac{1}{2} \\frac{d^{2}}{dx^{2}} + \\frac{1}{2}x^{2}$$\nPreciesely the Hamiltonian of the harmonic oscillator.\nNaturally, the next question we ask is if this method is general. Meaning that for any particular system with \\(k\\) degree of freedom, with matrix \\( q_{1},\u0026hellip;,q_{k},p_{1},\u0026hellip;,p_{k} \\) that satisfies the communitation rules. And for these matrices, we always find a matrix \\(H(q_{1},\u0026hellip;,q_{k},p_{1},\u0026hellip;,p_{k})\\) that could be diagonlized. In order to compare the Schrodinger wavefunction formalism and the matrix theory, this will be our starting point to transform these two distinct interpretation into one coherent quantum picture.\n( Citation: Aitchison,\u0026#32;MacManus \u0026amp; al.,\u0026#32;2004 Aitchison,\u0026#32; I.,\u0026#32; MacManus,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Snyder,\u0026#32; T. \u0026#32; (2004). \u0026#32;Understanding heisenberg’s ’magical’ paper of july 1925: A new look at the calculational details. ) Equivalency of these two methods Solving the eigenvalue problem\n\\(F_{z}\\) and \\(F_{\\Omega}\\) space and Hilbert space\nIn a nutshell, so far we have introduce the Schrodinger wavefunction interpretation of quantum system characterized by the Schrodinger equation\n$$ \\hat{H} \\psi(q_{1},\u0026hellip;,q_{k}, p_{1},\u0026hellip;,p_{k}) = E \\psi(q_{1},\u0026hellip;,q_{k}, p_{1},\u0026hellip;,p_{k})$$\nAnd we demonstrate the matrix method first by how Heisenberg initiate its thought on promoting the classical theory to matrix and found out that the commutation relation between observables. By an example of the communtation relation \\([p, q] = i\\) and a diagonizable matrix \\(H(p, q)\\), we were able to find out the inifinite set of eigenvalues and eigenvector that agree nicely with the wavefunction methods.\nTo show how these two methods compare to each other, we first transform the matrix method into a equivalent math problem of solving eigenvalues and eigenvectors. Subsequently we compare formalism and their math representation.\nMatrix method? eigenvalues problem in disguise! First, seek the matrices \\( q_{1},\u0026hellip;,q_{k},p_{1},\u0026hellip;,p_{k} \\) that satisfy the commutation rules. And combined to give a matrix\n$$ \\bar{H} = H(\\bar{q_{1}},\u0026hellip;,\\bar{q_{k}},\\bar{p_{1}},\u0026hellip;,\\bar{p_{k}}) $$\nwould not be a diagonal matrix. The diagonalized form could be obtained by similarity transformation\n$$ q_{i} = S^{-1}\\bar{q_{i}}S, \\quad p_{i} = S^{-1}\\bar{p_{i}}S $$\nThe communtation relation will carry over, to see this\n$$ [q, p] = S^{-1}\\bar{q}SS^{-1}\\bar{p}S - S^{-1}\\bar{p}SS^{-1}\\bar{q}S = S^{-1} [\\bar{q}, \\bar{p}] S$$\nHence, \\(\\bar{H}\\) goes over into \\( H \\) with \\(S^{-1}\\bar{H}S = H\\)\nThe only requirement from the above relation on \\(S\\) is that \\( S^{-1}\\bar{H}S \\) be a diagonal matrix where \\( \\bar{H} \\) is given.\nLet the matrix \\( \\bar{H} \\) have the elements \\(h_{\\mu \\nu}\\). the desired matrix \\(S \\) has element \\(S_{\\mu\\nu}\\), and the diagonal matrix \\(H\\) has element \\(w_{\\mu}\\), writing the matrix multiplication explicitly we got\n$$ \\sum_{\\nu}{h_{\\mu\\nu} s_{\\nu\\rho}} = w_{\\rho}\\cdot s_{\\nu\\rho} $$\nWe recognize that \\(s_{\\mu\\rho} \\) is the column vector \\( s_{1\\rho}, s_{2\\rho},\u0026hellip; \\). Hence, to specify the transformation \\(S\\) is equivalent in solving eigenvalues problem which runs as follows:\n$$ \\sum_{\\nu}{h_{\\mu\\nu}x_{\\nu}} = \\lambda \\cdot x_{\\mu} \\quad \\quad (\\mu = 1, 2, \u0026hellip;) $$\nThese set of eigenvalues and eigenvectors are essentially the only solutions. The knowledge of \\( S, H \\) determine all the solutions of the eigenvalue problem, but conversly, we can also determine \\( S, H \\) as soon as we have solved the eigenvalue problem completely.\nThe fundamental problem of the matrix theory is then the solution of the eigenvalue equation\n$$ \\sum_{nu}{h_{\\mu\\nu}x_{\\nu}} = E \\cdot x_{\\mu} \\quad \\quad (\\mu = 1, 2, \u0026hellip;) $$\nThe remaining task for us is to check that how this transformation to eigenvalues problems matches with the wavefunction formulism.\nWavefunction looking alike but not the same The defining charactersitics of the wave equation is the following\n$$ \\hat{H}\\psi(q_{1}, \u0026hellip;, q_{k}) = \\lambda \\psi(q_{1}, \u0026hellip;, q_{k}) $$\nIn which \\(H\\) is the Hamiltonian differential operator. We seek all solutions \\(\\psi(q_{1},\u0026hellip;,q_{k})\\) and \\( \\lambda \\). At first sight, this is very similar to what was required in the eigenvalue equation, which we could regard it as a function \\(x_{\\nu}\\) of the \u0026ldquo;discontinuous\u0026rdquo; variable \\(\\nu\\) which ranges over \\(1, 2,\u0026hellip;\\) corresponds to the function \\(\\psi(q_{1}, \u0026hellip;, q_{k})\\) with the \u0026ldquo;continuous\u0026rdquo; variables \\(q_{1}, \u0026hellip;, q_{k}\\), with \\(\\lambda\\) playing the same role each time.\nUpon further inspection on how these two objects transform, they do differ in a subtle way. The matrix method leads to a vector representation of the quantum state.\n$$ x_{\\mu} \\rightarrow \\sum_{\\nu}{h_{\\mu\\nu}x_{\\nu}} $$\nBut how does the wavefunction transform? We know probability is conserved in a closed quantum system and hence quantum state must undergo unitary transformation to conserve the probability.\nTo show, we first recall the norm is defined for the wavefunction\n$$ \\int{\\psi^{\\ast}(\\vec{r})\\psi(\\vec{r})d(\\vec{r})} = 1 $$\nWe expect that under transformation the norm is preserved or equivalently the total probability is conserved\n$$ \\int{\\bar{\\psi}^{\\ast}(\\vec{r})\\bar{\\psi}(\\vec{r})d\\vec{r}} = 1 $$\nLet\u0026rsquo;s define the one dimension representation of the \\(U(1)\\) be \\(e^{i\\vec{r}\\cdot\\vec{p}}\\). Then, under unitary transformation, the norm becomes\n$$ \\int{\\psi^{\\ast}(\\vec{r})e^{-i\\vec{r}\\cdot\\vec{p}}e^{i\\vec{r}\\cdot\\vec{p}}\\psi(\\vec{r})d\\vec{r}} = 1 $$\nAlso we recall that \\(\\psi(\\vec{r}) \\propto \\int{\\phi(\\vec{p})e^{i\\vec{r}\\cdot\\vec{p}}d\\vec{p}}\\). The wavefunction transform into different basis by fourier method.\nWe can indeed draw a parallel between these function. The first being vector space with well defined norm. The Schrodinger picture could also be understood as a vector with orthogonality vector and its weight. These properties are known as the Hilbert space.\n( Citation: Neumann,\u0026#32;1955 Neumann,\u0026#32; J.\u0026#32; (1955). \u0026#32; Mathematical foundations of quantum mechanics. \u0026#32; Princeton University Press.\u0026#32;Retrieved from\u0026#32; https://books.google.co.jp/books?id=JLyCo3RO4qUC ) Final Remark In the spirit of Feynnman, it will be a waste of time to put effort in making different interpretation of the quantum derivation. Nevertheless, it is still a great practice to be more thoroughly understand the historical context and mathematical motivation. Especially, when most quantum textbook often introduces these two concepts at its convience and yet never compare how these two methods differ. For it might be crucial in understanding what appraoch might be more appropriate in solving a particular quantum system.\nTo emphazie one more important aspect of such attempt to compare methods, it is that a new era of quamtum computation which heavily relies on the John Von Neumann density matrix representation of the quantum state. For this particular representation, operation of quantum mixed state and pure state is easily computed with mathematical convinence. And the ponding question of how the matrix thoery and wavefunction approach settle into one unify picture of microscopic phenomena is what drive Von Neumann to derive a representation that is irrevelent on how it\u0026rsquo;s presented.\nWe neglect some eariler motivation from Schrodinger and Heisenberg because in the development stage of a brand new theory it often took a lot of guess work to keep pushing.\nSome aspects are being abbrivated such as Schrodinger\u0026rsquo;s thought on the relevence of action principle to the formulation of quantum mechanics. But it was later developed by Dirac and Feynnman that the quantum version of the action principle could be devised starting from a notion of summing of all possible paths along the initial and final states but that\u0026rsquo;s another topic.\nBiblography Zeng (2008) Zeng,\u0026#32; J.\u0026#32; (2008). \u0026#32; 量子力学教程. \u0026#32; 科学出版社.\u0026#32;Retrieved from\u0026#32; https://books.google.co.jp/books?id=foQcPwAACAAJ Aitchison,\u0026#32; MacManus\u0026#32;\u0026amp;\u0026#32;Snyder (2004) Aitchison,\u0026#32; I.,\u0026#32; MacManus,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Snyder,\u0026#32; T. \u0026#32; (2004). \u0026#32;Understanding heisenberg’s ’magical’ paper of july 1925: A new look at the calculational details. Neumann (1955) Neumann,\u0026#32; J.\u0026#32; (1955). \u0026#32; Mathematical foundations of quantum mechanics. \u0026#32; Princeton University Press.\u0026#32;Retrieved from\u0026#32; https://books.google.co.jp/books?id=JLyCo3RO4qUC ","permalink":"https://htsod.github.io/posts/wave_particle/","summary":"Anyone is feeling weird? The contradiction between classical physics and microscopic phenomena is one of the most fascinating episodes in the history of science, reshaping our fundamental understanding of waves and particles. To grasp the weirdness of wave-particle duality, let’s start with a simple analogy.\nImagine shooting bullets at a wall with two equally spaced gaps. As expected, the bullets passing through each gap will behave independently, forming two distinct patterns on a measurement panel behind the wall.","title":"Quantum Mechanical Historian"},{"content":"Entropy, Order Parameters and Complexity What is statistical mechanics? Ensemble How to connect microscopic law with macroscopic phenomenon? The concept of ensemble provides a method that connects the two. Treating a large system(macroscopic) as collection of similarly prepared systems(microscopic).\nEntropy Entropy to be defined as a function of probability distribution \\(p_{i}\\) must satisfy the following:\nMaximum at \\(\\frac{1}{p_{i}}\\) Minimum at \\(p_{i} = 0 , S = 0\\) Minimum at \\(p_{i} = 1, S = 0\\) The unique function that satisfies the above requirement is \\(S=-p_{i}\\log{p_{i}}\\), giving rise to the information interpretation of entropy. From the first requirement, we could see that entropy is maximized when the distribution is more even, or in other words, more mixed, leading to the disorder interpretation of entropy. If entropy is zero, the probability distribution is either \\(0\\) or \\(1\\). Then, if the value of entropy is the only thing we know about the system, we could easily reproduce the system distribution with certainty. As entropy increases, it will be less likely that we could reproduce the system from scratch, making the system irreversible. And this gives rise to the reversibility interpretation of entropy.\nQuantum Statistical Mechanics Quantum mechanics is the law that governs microscopic evolution and particles type. Whereas in everyday life where temperature is sufficiently high, we do not have to worry about quantum mechanical effect. The underlying reason is that the asymetrical states that are created by quantum mechanics effect is thermalized to equally occupied states. So, in sufficiently high temperature, we could treat particles as ideal gas that have no internal structure.\nThe reverse is true. As we cool object, the states might settle into one of the asymetrical states and shows bewildering behavior.\nMonte Carlo It allow the computer to find ensemble averages in systems far too complicated to allow analytical evaluation.\nPhases Different phases are categorized by different symmetries. Matters with different symmetries cannot cross by perturbation theory. So far, there are two recognized way for matter to transit from one state to another. Namely the abrupt phase transition and continous phase transition.\nFluctuations and correlations How a system reponse is related to the correlation function of the system. THe correlation function measure the alignment of state within the system and it condensed the information more compactly.\nAbrupt phase transition By the name suggests, it happens when there is a discontinouity at the first derivative of the free energies because the phase boundary must have equal free energy but above and below it is a different free energy function.\nCriticality The second type of phase changes is by continuous phase transition. This happens when the symmetries of matter change. At critical temperature where the transition occurs, the system fluction is singular at the zero frequency vibration mode. At this point of transition, the system is self-similar. This phenomena is universal across different physical system.\nRandom walks and emergent properties Random walks is defined as taking a random steps in a given manifold randomly for every time step. This simple microscopic evolution rule\nTemperature and equilibrium Why do systems approach equilibrium?. When one of my classmates asked this question in a highschool chemistry class, I was astonished by how naturally we took in the concept that system eventually reaches equilibrium but never asked why. This inquiry of equilibrium had stucked in my head for quite a while until reading upon a statistical mechanics textbook which offers a elegant explanation to equilibrium. The short answer is that the states of equilibrium is much more probable than some other non-equlibrium states for microcanonical ensembles. One could imagine a system dynamics as flow in a network diagram with each nodes representing a possible state.\nAs time proceeds, the flow on each node has probability to evolve into some other states and will eventually reach a time-independent state which we called it equilibrium. Formally speaking, when we say a system has reached a state of equilibrium, we mean that the observable of the time average and the ensemble averages equal. Alternatively, no matter how the flow initially biased towards a particular set of states, as time proceeds, it eventually becomes uniform across all the possible states (a microcannonical ensemble). Noting that the above diagram has a equal number of nodes at the start and at the end. This is because the system is isolated and the total possible states remain unchanged.\nTo illustrate how the above model could be used to explain why a microcannonical system will reach equilibrium eventually,imagining mixing two types of particles with a total particle number of N. Initially, \\(\\frac{N}{2}\\) of particle A and \\(\\frac{N}{2}\\) of particle B each occupies half of the box with a partition in between.\nIgnoring the momentum degree of freedom, the possible states are the volume configuration \\( \\Omega_{unmixed} = \\frac{((V/2)^{N/2})^{2}}{(N/2)!(N/2)!} = \\frac{1}{2^{N}} \\frac{(V)^{N}}{(N/2)!(N/2)!} \\). The nominator states that there is \\( (V/2)^{N/2} \\) ways of configurations allowed for \\(N/2\\) particle in a volume \\(V/2\\). The denominator states that in those indistinguishable \\(N/2\\) particle A and \\( N/2 \\) particle B, they can freely exchange their states without changing the configuration of the system. Now by removing the partition, a total of N particle could fill the entire volume. The mixed state becomes \\( \\Omega_{mixed} = \\frac{(V)^{N}}{(N/2)!(N/2)!} = 2^{N} \\Omega_{unmixed} \\). Provided N is a large number of order \\( \\sim 10^{23} \\). The mixed state is much more likely than the unmixed state by simply out numbering the possibles state of an un mixed state. Hence, as the system evolves (by how we do not care), it is much more likely to fill the mixed states than the unmixed state reaching equilibrium.\nFraming this mixing process with the network model we defined above. Before mixing, we have \\( \\Omega_{unmixed} \\) states to start with. As we remove the partition, the system is no longer isolated and as a result the total number of states have increased to \\( \\Omega_{mixed} = 2^{N} \\Omega_{unmixed} \\). Because the flow is distributed uniformly across all possible states in equilibrium, the mixed states are far more likely than the unmixed states.\nThen, how likely the system will remain in the non-equlibrium unmixed state after mixing? With a crazy small probability of \\(P_{unmixed} = \\frac{1}{2^{N}} \\). Take \\( N = 10 \\), \\( P_{unmixed} = 0.001 \\). Not to mention that N usually of order \\( \\sim 10^{23} \\)\nPhase-space dynamics and ergodicity One might consider further and ponder what kind of system will eventually reach equilibrium where macroscopic observable remain statistically fixed except for rare thermal fluctuation. Or equivalently, for what kind of system does its time average behavior and ensemble behavior equal. Let\u0026rsquo;s start with a most common system. A classical \\(N\\) particle system in which its evolution is descirbed by its Hamiltonian. \\( H(P,Q) = \\sum_{\\alpha}{p_{\\alpha}^{2}/2m_{\\alpha} + U(q_{1}, \u0026hellip;, q_{3N})} \\). We want to study how Hamiltonian evolution will modify the \\(\\rho(q_{1},\u0026hellip;,q_{3N},p_{1},\u0026hellip;,p_{3N})\\) \\(6N\\) dimensional phase space density distribution.\nConsider the total time derivative on \\(\\rho\\)\n$$ \\frac{d \\rho}{dt} = \\frac{\\partial \\rho}{\\partial t} + \\vec{\\nabla} \\cdot \\vec{J} $$\nwhere \\(\\vec{J} = (\\rho \\dot{P}, \\rho \\dot{Q}\\)) is the phase space probability current. For probability to conserve, total derivative of \\(\\rho\\) must vanishes giving the continuity equation\n$$ \\frac{\\partial \\rho}{\\partial t} = -\\sum_{\\alpha=1}^{3N}{\\frac{\\partial \\rho}{\\partial q_{\\alpha}}q_{\\alpha} + \\frac{\\partial q_{\\alpha}}{\\partial q_{\\alpha}}\\rho_{\\alpha} + \\frac{\\partial \\rho}{\\partial p_{\\alpha}}p_{\\alpha} + \\frac{\\partial p}{\\partial p_{\\alpha}}\\rho_{\\alpha}} $$\nSo far we did the discussion is completely general to the \\(6N\\)-dimensional system. Observing the terms above we see two of them are rather strange looking \\( \\frac{\\partial q_{\\alpha}}{\\partial q_{\\alpha}}\\rho_{\\alpha} \\) and \\( \\frac{\\partial p}{\\partial p_{\\alpha}}\\rho_{\\alpha} \\). Indeed, for Hamiltonian system these term cancel out leading to the Liouville\u0026rsquo;s theorem\n$$ \\frac{\\partial \\rho}{\\partial t} = -\\sum_{\\alpha=1}^{3N}{\\frac{\\partial \\rho}{\\partial q_{\\alpha}}q_{\\alpha} + \\frac{\\partial \\rho}{\\partial p_{\\alpha}}p_{\\alpha} = 0} $$\nFor folks that familiar with fluid dynamics this is like saying fluid only flow around but cannot be compressed.(changing its density by going somewhere in space) In this case, the \\(6N\\)-dimensional phase space behaves as the incompressible fluid; some states become less likely only by \u0026ldquo;flowing\u0026rdquo; its likeliness to its neighbor states.\nEntropy Free energies Ever being involved in drawing a color ball from a mysterious box and winning a price only when a certain color or combination are drawn? Statistical mechanics from another perspective is fairly similar to this concept of drawing color ball except that some functional constraints have been applied to the color ball ensuring that this model is compatible with some physical process in interest.\nIn microcanonical ensemble, the states of the system are labelled by its position and momentum, which together gives the position and momentum dependent energy \\(E_{s} = \\frac{p^{2}}{2m} + U(r)\\) of that state.\nQuantum statistical mechanics Ongoing\nBibliography Bibliography called, but no references ","permalink":"https://htsod.github.io/review/statistical_mechanics/","summary":"Entropy, Order Parameters and Complexity What is statistical mechanics? Ensemble How to connect microscopic law with macroscopic phenomenon? The concept of ensemble provides a method that connects the two. Treating a large system(macroscopic) as collection of similarly prepared systems(microscopic).\nEntropy Entropy to be defined as a function of probability distribution \\(p_{i}\\) must satisfy the following:\nMaximum at \\(\\frac{1}{p_{i}}\\) Minimum at \\(p_{i} = 0 , S = 0\\) Minimum at \\(p_{i} = 1, S = 0\\) The unique function that satisfies the above requirement is \\(S=-p_{i}\\log{p_{i}}\\), giving rise to the information interpretation of entropy.","title":"Study Notes on Statistical Mechanics"},{"content":"Preview This blog provides a concise overview of some key noise-resilient, intermediate-scale quantum algorithms, designed for a general audience. We\u0026rsquo;ll first explore the core mechanics of these algorithms, followed by their applicability to existing classical counterparts. Lastly, we\u0026rsquo;ll discuss the hardware requirements necessary to implement these algorithms, with a focus on managing noise in quantum systems.\nVariational Quantum Eigenvalue (VQE) With the quantum hardware progressing, promising quantum algorithm has been devised to harness the exotic nature of quantum computer. Notably the Shor\u0026rsquo;s algorithm and Grover search algorthm which demonstrate significant improvement over classical computer. However, those algorithms require quantum resources that are far-fetching in today quantum hardware standard. Therefore, there are several quantum algorithms that have been devised to be sufficiently useful in the noise Intermediate-scale quantum(nisq) stage of quantum information industry. One prominent algorithm is variational quantum eigensolver. It is based on the variational principle which says that an approximation quantum state that is close to the ground state of a quantum system yields a sufficiently close solution to the eigenvalues of the ground state.\n$$ \\left \\langle \\psi(\\theta) \\right | H \\left | \\psi(\\theta)\\right\\rangle = \\lambda_{approx} \\ge \\lambda_{g} = \\left \\langle \\psi_{g} \\right | H \\left | \\psi_{g} \\right\\rangle$$\nHere we parameterized the state $\\psi(\\theta)$ to be trained on a classical computation iteratively by measuring the eigenvalue using a quantum circuit. By doing so we expect the eigenvalue would close up the gap with the true ground state of the Hamiltonian. Gearing with the general idea of how it works, here we list the motivation to develop VQE:\nIs it theoretically faster than classical computer?\nWhat kind of realistic problem can it solve?\nWe don\u0026rsquo;t know yet it is faster or not but solving for the ground state for a state vector is a difficult task for a classical computer.\nAs for the second question, it was said that chemical interaction happens quantum mechanical and hence the simulation must also be done quantum mechanically. VQE so far has been apply for solving the ground state energy or excited of a molecular configuration. Though it does not limit to simulation problem, for problem that could map to this structure could be used equally well.\nHere are the steps to implement\nEncode the problem Hamiltonian into circuit element\nMapping the problem state vector to a parameterized state living in the \\(2^{n}\\) computational basis\nMeasure the energy eigenvalue\nUse classical optimizer to minimize the eigenvalue by varying the parameters.\nrepeat step 1 to step 5 until it saturates\nIn the case of molecular simulation where the system in interest is based on fermions, the encoding could done by Jordan-Wigner or Brivate-\nBibliography called, but no references ","permalink":"https://htsod.github.io/life_trivia/nisq/","summary":"Preview This blog provides a concise overview of some key noise-resilient, intermediate-scale quantum algorithms, designed for a general audience. We\u0026rsquo;ll first explore the core mechanics of these algorithms, followed by their applicability to existing classical counterparts. Lastly, we\u0026rsquo;ll discuss the hardware requirements necessary to implement these algorithms, with a focus on managing noise in quantum systems.\nVariational Quantum Eigenvalue (VQE) With the quantum hardware progressing, promising quantum algorithm has been devised to harness the exotic nature of quantum computer.","title":"Variational Method as Approximation to Simplify Quantum Circuit"},{"content":"Network Theory (or Graph Theory in computer science) is the study of nodes connected by edges. At first glance, this simple framework may not seem physically insightful. However, historically, this abstraction has found wide-ranging applications across fields like social science, biology, and physics. Why can such a simplified model be physically useful? To understand this, let\u0026rsquo;s first explore how physical events are understood.\nPhysics seeks to reveal the fundamental interactions between matter. Yet, as matter interacts in complex systems, emergent behaviors arise—phenomena that are unpredictable from knowledge of fundamental interactions alone. Even if we had perfect mastery over quantum mechanics, predicting collective behaviors in social sciences, for instance, would remain a formidable challenge. Could a supercomputer, encoding every physical detail, simulate the world precisely? Such an idea is not just impractical, but impossible, due to the overwhelming complexity involved.\nThe key insight is that not every microscopic interaction contributes to observable macroscopic behavior. Much of the detail is irrelevant, allowing us to \u0026ldquo;coarse-grain\u0026rdquo; reality into a simpler model that captures its essential features. This is where network theory proves invaluable. By drastically simplifying the system in question, we represent its components as indistinguishable entities—nodes—and focus on their connectivity with one another, ignoring unnecessary details.\nWith this basic framework, the study of networks unfolds in several key directions:\nAdding Complexity to Simple Networks:\nTo account for relevant degrees of freedom, concepts like weighted and directed networks are introduced, representing more intricate interactions.\nQuantifying Network Structures:\nTechniques like clustering coefficients, centrality measures, and shortest-path algorithms help analyze the structure of networks.\nModeling Collective Dynamics:\nNetwork theory is used to understand how systems behave dynamically, with examples like synchronization in many-body systems.\nStudying Ensembles of Networks:\nVarious models, such as random and scale-free networks, explore how networks evolve under structural constraints.\nSimple networks The most basis network contains \\(n\\) nodes and edges connecting the nodes. The connection could be represented by a \\( n \\times n \\) matrix with entries \\( a_{ij} = 0\\) or \\(1\\) depending on the conection is on or off. We call this matrix to be the adjacency matrix \\(A\\).\nThe adjacency matrix Components Independent paths, connectivity and cuts sets Graph laplacian 1. Adding complexity to simple networks There are few ways we could add in complexity to the simplest network. We could add in either direction or weight to give a more detail information of the connection.\nIn addition, we could add in more layers of network.\nWeighted networks $$ a_{ij} \\subset [0, 1] $$\nDirected networks $$ a_{ij} = 0, 1, -1$$\nHypergraphs $$ a^{\\alpha \\beta}_{ij} $$\nBipartite networks Trees Planner networks 2. Quantifying network structure Centrality Groups of nodes Transitivity and the clustering coefficient Reciprocity Signed edges and structural balance Similarity Homophily and assortative mixing Cyclicity 3. Modelling collective dynamics 4. Ensembles of networks Small-world effect Scale-free networks Bibliography called, but no references ","permalink":"https://htsod.github.io/life_trivia/network/","summary":"Network Theory (or Graph Theory in computer science) is the study of nodes connected by edges. At first glance, this simple framework may not seem physically insightful. However, historically, this abstraction has found wide-ranging applications across fields like social science, biology, and physics. Why can such a simplified model be physically useful? To understand this, let\u0026rsquo;s first explore how physical events are understood.\nPhysics seeks to reveal the fundamental interactions between matter.","title":"Study Notes on Network Theory"},{"content":"Preview If one struggle to convince themselves the sanity of quantum mechanics, the best advice to them is to move on and put yourself into practical question and work out the math. That\u0026rsquo;s the core spirit of this reivew note about. All we need to know is the necessary axioms which were verified long ago, and then to make use of these axioms to draw meaningful observation on the theory itself or practical problems to draw in more intuition.\nAxioms of Quantum Mechanics: Get to the points Rule 1: Hilbert space A dynamical system corresponds to a Hilbert space in such a way that a definite state of the system corresponds to a definite ray in the space.\na state \\(\\left| u \\right\\rangle \\)if it corresponds to the ray in the direction of vector \\(\\left| u \\right\\rangle\\) complex factor and phase factor don\u0026rsquo;t change its state linear combination of definite state produces another possible state linear superposition principle relative phase between states matters An isolated quantum mechanical system can evolve with time unitarily. The evolution is continuous and causal.\nRule 2: Time evolution An isolated quantum state evolves continously and casually in time by a unitary operator U.\n$$ \\left| u(t) \\right\\rangle = U(t, t_{0}) \\left | u(t_{0}) \\right\\rangle $$\nwhere \\( U(t, t_{0}) = e^{\\left[-\\frac{i}{\\hbar}\\int_{t_{0}}^{t}H(t^{\\prime})dt^{\\prime}\\right]}\\)\nStarting from this axiom, one can derive the Schrodinger equation which we shall show.\nLet \\( t\u0026gt;t_{0} \\), then unitary evolution \\(U(t, t_{0})\\) of the wavefunction \\( u(t_{0})\\) to \\(u(t)\\) can be stated as the following\n$$ \\left| u(t) \\right\\rangle = U(t, t_{0})\\left| u(t_{0}) \\right\\rangle$$\nIn Lie\u0026rsquo;s approach, we try to cover the unitary transformation with infinitestimal change from the identity, hence infinitestimal change of unitary operation can be written as\n$$ U(t_{0} + \\delta t, t_{0}) = I + K \\delta t $$\nFrom the definition of unitary, \\( U \\) must satisfy \\( U^{\\ast}U = I \\), which yields\n$$ U^{\\ast}U = (I + K)^{\\ast}(I+K)= I + K^{\\ast} + K = I $$\n$$ K^{\\ast} = -K $$ Requiring , $K$ to be antihermitian. We could as well write \\(K = iH\\) and \\( H \\) to be hermitian.\nTo cover the time evolution up to \\(t\\), we carry the infinitestimal operations up to \\(N\\) times, and taking \\( N \\) to be infinte.\n$$ U(t) = \\lim_{N\\rightarrow \\infty}(U(t/N))^{N}= \\lim_{N\\rightarrow \\infty}\\left(I - iH \\frac{t}{N}\\right)^{N}= e^{-iHt} $$\nNow we could rewrite the evolution of the wavefunction to derive the Schrodinger equation,\n$$ U\\left| u(t) \\right\\rangle = \\left( I - \\frac{i}{\\hbar} H \\delta t \\right) \\left| u(t) \\right\\rangle = \\left | u(t+\\delta t) \\right\\rangle $$\n$$ H\\left|u(t) \\right\\rangle = i\\hbar \\frac{\\delta \\left|u(t)\\right\\rangle}{\\delta t} $$\nAs promised.\nRule 3: Observables Each observable of a system is associated with a Hermitian operator on the Hilbert space of the system. We assume that the eigenstates of each observable form a complete set.\nTo get a intuitive picture of this axiom, we first note that the statistical properties of the wavefunction allows us to weight the contribution of the wavefunction on to a obervable. Stating the the statistical properties:\nProbability distribution sum to one $$ \\int^{\\infty}_{-\\infty}{\\psi^{\\ast}(x) \\psi(x)dx } = 1 $$\nGiven observable \\( O(x) \\), $$ \\int^{\\infty}_{-\\infty}{\\psi^{\\ast}(x)O(x) \\psi(x)dx } = \\left\u0026lt; O(x)\\right\u0026gt; $$\nWriting in Dirac notation, \\( \\left\u0026lt; O(x)\\right\u0026gt; = \\left\\langle \\psi \\right| O(x) \\left| \\psi \\right\\rangle \\).\nHermitian conjugate both side and subtract it from its originall form\n$$ \\left\\langle \\psi \\right| O^{\\dagger}(x) - O(x) \\left| \\psi \\right\\rangle = \\left\u0026lt; O(x)\\right\u0026gt; - \\left\u0026lt; O(x)\\right\u0026gt; = 0$$\nHence, \\( O^{\\dagger}(x) = O(x)\\) is hermitian.\nThe second statement is saying that for any set of commuting observables, \\(A_{1}, A_{2},\u0026hellip; \\), the system can be expressed as the superposition of \\( \\psi = \\sum_{a}{a_{a}\\psi_{a}} \\)\nRule 4: Measurement The result of any given measurement of an observabble is an eigenvalue of the corresponding Hermitian operator.\nRule 5: Statistical The average \\( \\bar{A} \\equiv \\left\\langle A \\right\\rangle \\) of a number of measurements of \\(A\\) is\n$$ \\bar{A} \\equiv \\left\\langle A \\right\\rangle = \\frac{\\left\\langle u|A| u \\right\\rangle}{\\left\\langle u|u \\right\\rangle} $$\nWith \\( u \\) an eigenstate of \\( \\Rightarrow \\left| u \\right\\rangle = \\left| \\phi_{i} \\right\\rangle \\), \\( A \\left| \\phi_{i} \\right\\rangle = \\lambda_{i} \\left|\\phi_{i} \\right\\rangle \\) $$ \\bar{A} = \\frac{\\left\\langle \\phi_{i}|A| \\phi_{i} \\right\\rangle}{\\left\\langle \\phi_{i}|\\phi_{i} \\right\\rangle} = \\lambda_{i} \\Rightarrow \\triangle^{2} = \\left\\langle A^{2} \\right\\rangle - \\left\\langle A \\right\\rangle ^{2} $$\nWith \\( u \\) not an eigenstate of A $$ \\left| u \\right\\rangle = \\sum_{i} \\left| \\phi_{i} \\right\\rangle \\left\\langle \\phi_{i} \\right| \\left| u \\right\\rangle $$\n\\(u\\) in basic of \\( \\phi_{i} \\)\n$$ \\bar{A} = \\sum_{i, j} \\frac{\\langle u | \\phi_{i} \\rangle \\langle \\phi_{i} | A | \\phi_{j} \\rangle \\langle \\phi_{j} | u \\rangle}{\\langle u | u \\rangle} = \\frac{\\sum_{i} \\lambda_{i}|\\langle u | \\phi_{i}\\rangle |^{2}}{\\sum_{i} | \\langle u | \\phi_{i} \\rangle |^{2}}$$\n$$ \\bar{A} = \\frac{\\sum_{i} P_{i} \\lambda_{i}}{\\sum_{i} P_{i}} $$\nRule 7: Wavefunction collapse A measurement of observable \\( A \\) resulting in eigenvalue \\( \\lambda_{i} \\) projects the state vector from \\( |u\\rangle \\) to that subspace of the Hilbert space associated with \\( \\lambda_{i} \\). If the eigenvalue is nondegenerate, the subspace is just a single eiggenstate \\( \\phi_{i} \\) associated with \\( \\lambda_{i} \\). If \\( |u\\rangle \\) is already an eigenstate of \\( A \\), it\u0026rsquo;s not changed by the measreument. If \\( u \\) is not an eigenstate of \\( A \\), then in the nondegenerate case it \u0026lsquo;collapses\u0026rsquo; to \\( \\phi_{i} \\) with probability \\( |\\langle u | \\phi_{i} \\rangle |^{2} \\)\nRule2 and Rule 7 macroscopic objectification problem\nIf \\( [A, B] = 0 \\), then \\(A\\) and \\(B\\) are simultaneously diagonalized by the same unitary transformation\nIf A, B, C, \u0026hellip; form a complete set of commuting (compatible) observables then we know everythig there is to know about the steate of the system when we specify the eigenvalue of each of A, B, C, \u0026hellip; for that state\nThis is called a complete set of quantum numbers for that state\n$$ \\frac{d\\langle A \\rangle}{dt} = \\left(\\frac{ d \\langle u |}{dt}\\right) A |u\\langle + \\langle u | A \\left(\\frac{d |u \\rangle}{dt}\\right) + \\langle u | \\left(\\frac{\\delta A}{\\delta t}\\right) | u \\rangle $$\nEmploying \\( H|u\\rangle = i\\hbar \\delta_{t} |u\\rangle \\) and \\( \\langle u | H = -i \\hbar \\delta_{t} \\langle u | \\)\n$$ \\frac{d\\langle A \\rangle}{dt} = \\frac{i}{\\hbar} \\left[ \\langle u | HA | u \\rangle - \\langle u | AH | u \\rangle \\right] + \\left\\langle u \\left| \\frac{\\delta A}{\\delta t} \\right| u \\right\\rangle $$\n$$ = \\frac{i}{\\hbar} \\langle u | [A, H] | u \\rangle + \\left\\langle u \\left| \\frac{\\delta A}{\\delta t} \\right| u \\right\\rangle $$\nCarry out unitary transformation into a new basic in which \\( u \\) is a constant.\n$$ S(t, t_{0}) = U^{\\dagger}(t, t_{0}) = e^{\\left[ -\\frac{i}{\\hbar}\\int_{t_{0}}^{t} H(t^{\\prime}) dt^{\\prime} \\right]^{\\dagger}} $$\nGiving us the Heisenberg equation for uncertainty\n$$ A^{\\prime} = SAS^{\\dagger} = U^{\\dagger}AU \\Rightarrow \\frac{dA^{\\prime}}{dt} = \\frac{1}{i\\hbar} [A^{\\prime}, H] + \\frac{\\delta A^{\\prime}}{\\delta t}$$\nProbability is conserved by some kind of flow Probability conserved by flow The evolution of closed quantum system is deterministic Feynman Dirac sum over all paths To further elaborate the rule 2 of the quantum axioms, that is, a isolated quantum mechanical system will evolve unitarily, we will look at the wavefunction representation of the quantum states.\nRecall $$ \\left| u(t) \\right\\rangle = U(t, t_{0}) \\left | u(t_{0}) \\right\\rangle $$\nGiven the Schrodinger equation, nameing it eqn 1\n$$ -i\\hbar \\frac{\\partial}{\\partial t} \\psi = (-\\frac{\\hbar^{2}}{2m} \\nabla ^{2} + V) \\psi $$\nand its complex conjugate and naming it eqn 2\n$$ -i\\hbar \\frac{\\partial}{\\partial t} \\psi^{\\ast} = (-\\frac{\\hbar^{2}}{2m} \\nabla ^{2} + V) \\psi^{\\ast} $$\nMultiply Schrodinger equation from the left by \\( \\psi^{\\ast} \\) and multiply the complex conjugate of the Schrodinger equation form the left by \\( \\psi \\) and subtracting these two expression yields\n$$ i\\hbar \\frac{\\delta}{\\delta t}(\\psi^{\\ast}\\psi) = -\\frac{\\hbar^{2}}{2m} \\left(\\psi^{\\ast} \\nabla ^{2}\\psi + \\psi \\nabla ^{2} \\psi^{\\ast} \\right) $$ $$ = -\\frac{\\hbar^{2}}{2m} \\nabla \\cdot \\left(\\psi^{\\ast} \\nabla \\psi + \\psi \\nabla \\psi^{\\ast} \\right) $$\nthen in a closed integration, by Gauss theorem, the right hand side could be converted into surface integral as the following\n$$ i\\hbar \\frac{\\delta}{\\delta t} \\int_{\\tau}{\\psi^{\\ast}\\psi d\\tau} = -\\frac{\\hbar^{2}}{2m} \\oint_{S}{\\left(\\psi^{\\ast} \\nabla \\psi + \\psi \\nabla \\psi^{\\ast} \\right) \\cdot d\\vec{S}} $$\nlet \\( \\rho = \\psi^{\\ast}\\psi \\) and \\(\\vec{j}= \\psi^{\\ast} \\nabla \\psi + \\psi \\nabla \\psi^{\\ast} \\), then equation becomes the familar continuity equation in fluid dynamics\n$$ \\frac{\\delta}{\\delta t} \\rho + \\vec{\\nabla} \\cdot \\vec{j} = 0$$\nFor \\( \\rho = \\psi^{\\ast} \\psi \\), we have a fairly good intuition about it, however, the flow term is bit more tricky.\nBibliography Bibliography called, but no references ","permalink":"https://htsod.github.io/review/quantum_mechanics/","summary":"Preview If one struggle to convince themselves the sanity of quantum mechanics, the best advice to them is to move on and put yourself into practical question and work out the math. That\u0026rsquo;s the core spirit of this reivew note about. All we need to know is the necessary axioms which were verified long ago, and then to make use of these axioms to draw meaningful observation on the theory itself or practical problems to draw in more intuition.","title":"Study Notes on Quantum Mechanics"},{"content":"Relativity\u0026rsquo;s action principle constrains how light and matter interact within spacetime. By placing the potential term either inside or outside the action square root, we derive the familiar interactions of gravity or electromagnetism between matter. This post offers a top-down view of electromagnetism\u0026rsquo;s derivation from these fundamental principles.\nRelativistic Action To incorporate relativistic effects, the action is written as:\n$$ S = -m \\int{ \\sqrt{-\\eta_{\\mu \\nu} dx^{\\mu} dx^{\\nu}} } = -m \\sqrt{dt^{2} - d\\vec{x}^{2}} $$\nIn comparison, the classical action with an added potential term is:\n$$ S_{NR} = \\int{ dt \\left(\\frac{1}{2}m \\left( \\frac{d\\vec{x}}{dt} \\right)^{2} - V(x) \\right) } $$\nTo include interactions in relativistic spacetime, we consider adding the potential term either outside or inside the square root.\nOutside the square root $$ S = -\\int{ [-m \\sqrt{-\\eta_{\\mu \\nu} dx^{\\mu} dx^{\\nu} } + V(x)dt] } $$ By promoting the scalar potential to the vector potential and completing the symmetry, we get:\n$$ S = -\\int{ [-m \\sqrt{-\\eta_{\\mu \\nu} dx^{\\mu} dx^{\\nu} } + A_{\\mu}(x)dx^{\\mu}] } $$\nInside the square root $$ S = -\\int{ -m \\sqrt{\\left(1+\\frac{2V}{m}\\right)dt^{2} - d\\vec{x}^{2}} } $$ Here, symmetry and promotion to Lorentz invariance give us the curved spacetime version:\n$$ S = -m \\int{ \\sqrt{-g_{\\mu \\nu} dx^{\\mu} dx^{\\nu}} } $$\nMaxwell\u0026rsquo;s Equations in Hindsight Extremizing the action and requiring the variation to vanish gives:\n$$ \\delta S = \\delta \\left(-m\\int{d\\tau \\sqrt{-\\eta_{\\mu \\nu} \\frac{dx^{\\mu}}{d\\tau} \\frac{dx^{\\nu}}{d\\tau}} + \\int{d\\tau A_{\\mu}(x(\\tau)) \\frac{dx^{\\mu}}{d\\tau}}} \\right) $$\nFirst term $$ \\delta \\left( -m\\int{d\\tau \\sqrt{-\\eta_{\\mu \\nu} \\frac{dx^{\\mu}}{d\\tau} \\frac{dx^{\\nu}}{d\\tau}} } \\right) = m \\int{d\\tau \\eta_{\\mu \\nu} \\frac{dx^{\\mu}}{d\\tau}\\frac{dx^{\\nu}}{d\\tau} = -m \\int{d\\tau \\eta_{\\mu \\rho} \\frac{d^{2}x^{\\mu}}{d\\tau ^{2}}dx^{\\rho}}}$$\nSecond term $$ \\delta \\int{d\\tau A_{\\mu}(x) \\frac{dx^{\\mu}}{d\\tau}} = \\int{d\\tau \\left[A_{\\mu}\\frac{d\\delta x^{\\mu}}{d\\tau} + \\delta_{\\nu}A_{\\mu}\\delta x^{\\nu} \\frac{dx^{\\mu}}{d\\tau}\\right]} $$\n$$ \\int{d\\tau A_{\\mu}(x)\\frac{d\\delta x^{\\mu}}{d\\tau}} = -\\int{d\\tau \\frac{dA_{\\mu}(x)}{d\\tau}\\delta x^{\\mu}} = -\\int{d\\tau \\delta_{\\nu} A_{\\mu}(x) \\frac{dx^{\\nu}}{d\\tau} \\delta x^{\\mu}}$$\n$$ \\delta \\int{d\\tau A_{\\mu}(x) \\frac{dx^{\\mu}}{d\\tau}} = \\int{d\\tau \\left( \\delta_{\\mu}A_{\\nu} - \\delta_{\\nu}A_{\\mu} \\right) \\frac{dx^{\\nu}}{d\\tau} \\delta x^{\\mu}} $$\nDefining the antisymmetric tensor field\n$$ F_{\\mu \\nu}(x) \\equiv \\delta_{\\mu}A_{\\nu}(x) - \\delta_{\\nu}A_{\\mu}(x) $$\nThus, the variation of the action becomes:\n$$ \\delta S = \\int{d\\tau \\left( -m \\eta_{\\mu \\rho} \\frac{d^{2}x^{\\mu}}{d\\tau^{2}} \\delta x^{\\rho} + F_{\\mu \\nu}\\frac{dx^{\\nu}}{d\\tau} \\delta x^{\\mu} \\right)} $$\nDefine\n$$ F_{\\nu}^{\\mu} \\equiv \\eta^{\\mu \\lambda} F_{\\lambda \\nu} \\Rightarrow F_{\\nu}^{\\mu} \\frac{dx^{\\nu}}{d\\tau} \\eta_{\\mu \\rho} \\delta x^{\\rho} $$\nThis results in the Lorentz force law in tensor form:\n$$ m\\frac{d^{2}x^{\\mu}}{d\\tau^{2}} = F_{\\nu}^{\\mu}(x) \\frac{dx^{\\nu}}{d\\tau} $$\nFields and Particle Interaction The action for multiple particles becomes:\n$$ S = -\\sum_{a} m_{a} \\int{d\\tau_{a} \\sqrt{-\\eta_{\\mu \\nu} \\frac{dx_{a}^{{\\mu}}}{d\\tau_{a}} \\frac{dx_{a}^{\\nu}}{d\\tau_{a}} }} + \\sum_{a} e_{a} \\int{d\\tau_{a}A_{\\mu}(x_{a}(\\tau_{a}))\\frac{dx_{a}^{\\mu}}{d\\tau_{a}}} $$\nThe field **\\(A_{\\mu}\\) covers the entire space, but each particle couples to it locally with strength \\( e_{a} \\) which we recognize as the particle\u0026rsquo;s charge.\nGauge Invariant Focusing on the second term \\( \\int{A_{\\mu}(x)dx^{\\mu}} \\)\nThis is invariant under the gauge transformation:\n$$ A_{\\mu}(x) \\rightarrow \\tilde{A_{\\mu}} = A_{\\mu}(x) + \\delta_{\\mu} \\Lambda (x) $$\nThe variation of this term vanishes:\n$$ \\int{\\delta_{\\mu} \\Lambda (x) dx^{\\mu}} = \\int_{\\tau_{i}}^{\\tau_f}d \\tau \\frac{dx^{\\mu}}{d\\tau}\\delta_{\\mu} \\Lambda(x) = \\int_{\\tau_{i}}^{\\tau_{f}} d\\tau \\frac{d}{d\\tau} \\Lambda (x)=0$$\nFrom this, we conclude that \\( A_{\\mu}(x) \\) contains non-physical degrees of freedom that can be removed by choosing an appropriate \\( \\Lambda (x) \\). Varying this gauge-invariant action leads to \\( F_{\\mu \\nu}\\) which is itself gauge-invariant.\nSquaring \\( F_{\\mu \\nu} \\) gives the Lorentz scalar: $$ \\int{d^{4}x \\left( -\\frac{1}{4}F^{\\mu \\nu} F_{\\mu \\nu} \\right)} $$\nThis is Maxwell\u0026rsquo;s Lagrangian:\n$$ \\mathcal{L} = -\\frac{1}{4}F^{\\mu \\nu} F_{\\mu \\nu} $$\nMaxwell\u0026rsquo;s Lagrangian Varying the Lagrangian results in Maxwell\u0026rsquo;s equations in free space:\n$$ \\delta_{\\mu} F^{\\mu \\nu} = 0 $$\nIn the presence of currents, we obtain:\n$$ \\delta_{\\mu} F^{\\mu \\nu}(x) = -J^{\\nu}(x)$$\nMaxwell\u0026rsquo;s Equation in Vector Forms For \\( \\nu = 0 \\), $$ \\delta_{i}F^{i0}=-\\delta_{i} E^{i} = - \\vec{\\triangledown} \\cdot \\vec{E} = \\rho $$\nFor \\( \\nu = 3 \\), $$ \\delta_{\\mu}F^{\\mu 3} = \\delta_{0}F^{03} + \\delta_{1}F^{13} + \\delta_{2}F^{23} = \\delta_{0}E^{3} - \\delta_{1}E^{2} + \\delta_{2}B^{1}$$\n$$ \\vec{\\triangledown} \\times \\vec{B} = \\frac{\\delta \\vec{B}}{\\delta t} + \\vec{J}$$\nA Fun Game to Play Exploring the relationship between spacetime and matter interaction offers exciting possibilities. Imagine if our spacetime structure changes:\n$$\\eta_{\\mu\\nu} \\rightarrow \\eta_{\\mu\\nu}^{\\prime}$$\nWhat new interactions or fields could emerge? It\u0026rsquo;s a compelling thought experiment, highlighting the beauty of the theory\u0026rsquo;s flexibility.\nBibliography Bibliography called, but no references ","permalink":"https://htsod.github.io/posts/maxwell_eqns_derive/","summary":"Relativity\u0026rsquo;s action principle constrains how light and matter interact within spacetime. By placing the potential term either inside or outside the action square root, we derive the familiar interactions of gravity or electromagnetism between matter. This post offers a top-down view of electromagnetism\u0026rsquo;s derivation from these fundamental principles.\nRelativistic Action To incorporate relativistic effects, the action is written as:\n$$ S = -m \\int{ \\sqrt{-\\eta_{\\mu \\nu} dx^{\\mu} dx^{\\nu}} } = -m \\sqrt{dt^{2} - d\\vec{x}^{2}} $$","title":"Top-down derivation of Electromagnetism"},{"content":"Renormalization Group Approach in Dynamical System The renormalization group (RG) method is an approximation technique initially developed for solving strongly interacting many-body problems in quantum field theory, where perturbative solutions deviate from the actual solutions. The fundamental concept of the renormalization group approach is to eliminate irrelevant degrees of freedom in a physical system while preserving its essential characteristics ( Citation: P. Kopietz,\u0026#32;2010 P. Kopietz,\u0026#32; F.\u0026#32; (2010). \u0026#32; Introduction to the Functional Renormalization Group (1). \u0026#32; Springer, Berlin Heidelberg 2010. https://doi.org/10.1007/978-3-642-05094-7 ) . This method has been extended to the field of statistical mechanics, providing a quantitative description of universality and scale invariance phenomena.\nUniversality and Scale Invariance Universality refers to the observation that distinct physical systems can exhibit similar behavior near their critical points. Scale invariance occurs when the qualitative features of a system remain unchanged as we vary the system\u0026rsquo;s size ( Citation: Sethna,\u0026#32;2020 Sethna,\u0026#32; J.\u0026#32; (2020). \u0026#32; Entropy, Order Parameters, and Complexity (2). \u0026#32; Clarendon Press. ) . These concepts offer a framework for categorizing phenomena that exhibit self-replicating patterns in space (fractal structures) and time (period doubling). In this review, we will explore the relationship between changing the scale of a system and the emergence of chaos at certain parameter limits. By applying the renormalization group method, we can establish the close connection between these concepts and provide a unifying language for their analysis.\nRenormalization Procedure: Decimation and Rescaling The renormalization group method involves two key steps: decimation (mode reduction) and rescaling. Decimation eliminates irrelevant degrees of freedom in the system, while rescaling ensures that the remaining parameters are adjusted in such a way that the essential properties of the system are preserved.\nWe can describe the action of the renormalization group on a system using the operator \\(R(b;g)\\), where $g$ represents the relevant parameters or coupling terms that describe the system, and \\(b\\) defines the scaling operation. By applying this operator recursively, we can track the evolution of the parameters \\(g\\) as we scale the system by a factor of \\(b\\).\n$$ \\vec{g}^{\\prime} = \\vec{R}(b;\\vec{g}) $$ $$ \\vec{g}^{\\prime\\prime} = \\vec{R}(b^{\\prime};\\vec{g}^{\\prime}) = \\vec{R}(b^{\\prime};\\vec{R}(b;\\vec{g})) = \\vec{R}(b^{\\prime}b;\\vec{g}^{\\prime}) $$\nBy repeatedly applying this operation $n$ times, we obtain the relationship:\n$$ \\vec{g}^{(n)} = \\vec{R}(b;\\vec{g})^{n-1}=\\vec{R}(b^{(n)};\\vec{g}) $$\nThis equation signifies that by removing irrelevant degrees of freedom, we modify the system\u0026rsquo;s coupling parameters \\(\\vec{g}\\). To ensure that \\(\\vec{g}\\) remains fixed at each iteration, the scaling operation $b$ must be recursively applied.\nThe conceptual understanding of the renormalization group method is straightforward, but the challenge lies in determining the form of the operator \\(\\vec{R}(b;\\vec{g})\\), as we will see in the case of the logistic map.\nExample I: One-dimensional Ising Model In the case of one dimension Ising model, every resursive process scales the total number of lattice by \\( \\frac{1}{2} \\) while keeping the form of partition function fixed. As a result, the coupling constant \\( g \\equiv \\frac{J}{T} \\) is mapped to \\( g^{\\prime} \\) accordingly to match the partition function.\nBy scaling the lattice point by half, the transfer matrix is rescaled as such\n$$ T^{\\prime}=e^{f^{\\prime}}[[e^{g^{\\prime}+h^{\\prime}}, e^{-g^{\\prime}}], [e^{-g^{\\prime}}, E^{g^{\\prime} - h^{\\prime}}]] = e^{2f}[e^{2g+2h} + e^{-2g}] $$\nWhich gives us three equations to solve for the three new coupling constant in terms of the old coupling constant. Writting out explicitly.\nThe renormalized external magnetic field $$ h^{\\prime} = h + \\frac{1}{2}\\ln{\\left[ \\frac{\\cosh{(2g+h)}}{\\cosh{2g-h}} \\right]} $$ In the absent of external magnetic field\nThe renormalized free energy $$ f^{\\prime} = 2f + + \\ln{\\left( 2 \\sqrt{2\\cosh{2g}} \\right)} $$\nThe renormalized nearest-neighbor interaction $$ \\tanh{g^{\\prime}} = \\tanh^{2}g $$\nThe RG flow and fixed points Defining \\( x_{0} \\) to be\n$$ x_{0} = \\tanh{\\left( \\frac{J}{T} \\right)} = \\tanh{g} $$\nwhich follows that\n$$ \\tanh{g_{1}} = x_{1} = x_{0}^{2} $$\nAnd we obtain the recursion relation of the fraction \\( g \\)\n$$ x_{n+1} = x_{n}^{2} $$ $$ x_{n} = \\tanh{\\left( \\frac{J}{T_{n}} \\right)} $$\nSince hyperbolic tangent is asmptotic to -1 and +1, hence the recursion will map the hyperbolic tangent from \\( |1| \\) to \\( 0 \\), or zero temperature to infinite temperature as we shrinks the system.\nAlternatively, the RG flow could be captured by calculating the correlation function \\( G(r_{i} - r_{j}) \\) and the correlation length \\( \\xi \\). Recall that \\( \\xi \\) is defined in terms of the asymptotic behavor of the correlation function \\( G(r_{i} - r_{j}) \\) for \\( |r_{i} - r_{j}| \\rightarrow \\infty \\), or, equivalently, in terms of the behavior of its Fourier transfrom \\( G(k) \\) for small wave vectors.\n$$ \\xi^{\\prime} \\equiv \\xi (x^{\\prime}) = \\frac{\\xi (x)}{b} $$\n$$ \\xi (x^{2}) = \\frac{\\xi (x)}{2} $$\n$$ \\xi (x) = - \\frac{a_{0}}{\\ln{x}} $$\nHence\n$$ \\xi \\sim \\frac{a}{2} e^{2J/T} $$\nExample II: Logistic Map and Period Doubling The logistic map is an example of a discrete-time dynamical system defined by the recursion law:\n$$ x_{n+1} = f(x_{n})=\\mu x (1-x) $$\nAlthough the equation appears simple, it exhibits a captivating phenomenon known as period doubling as the bifurcation parameter $\\mu$ increases, ultimately leading to chaos at the limit $\\mu_{\\infty}$, as illustrated in Figure below.\nInterestingly, a constant $\\delta$ known as the Feigenbaum constant emerges and remains fixed across certain families of functions. It can be stated as in the limit of $n \\rightarrow \\infty$\n$$ \\delta = \\lim_{n\\rightarrow \\infty} \\frac{\\mu_{n}-\\mu_{n-1}}{\\mu_{n+1}-\\mu_{n}} $$\nThrough numerical analysis, the Feigenbaum constant has been calculated to be a value close to $4.66914$.\nThe RG method provides a quantitative approach to calculating the Feigenbaum constant and generalizing the behavior of period doubling phenomena to a function space. However, the mathematical derivations involved can be complex, and interested readers are encouraged to consult relevant papers ( Citation: Sfondrini,\u0026#32;2012 Sfondrini,\u0026#32; A. \u0026#32; (2012). \u0026#32;Introduction to universality and renormalization group techniques.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1210.2262 ) for more detailed information.\nTo keep it simple, we will only be focusing on providing a descriptive account of the steps involved in calculating the Feigenbaum constant, with the emphasis on the interpretation of the results and their significance in the dynamical analysis of chaos.\nTo summarize the steps, we first make observations about the logistic map, recognizing it as a unimodal function. We also note that other unimodal functions exhibit period doubling behavior. We define a functional subspace of unimodal functions and seek to apply the RG method in a way that preserves this characteristic of being unimodel. Once we have the form of \\(R(b;g)\\) at hand, we evaluate the behavior of the fixed point \\(R(\\phi^{ast})=\\phi^{ast}\\) where $\\phi$ denotes the flow. Intuitively, we expect to find one unstable manifold with eigenvalues of modulus greater than one that correspond to bifurcation variable \\(\\mu_{\\infty}\\) that destabilizes the orbit. And an infinite stable manifold with eigenvalues of modulus less than one corresponds to \\(mu_{n}\\) that corresponds to stable oribit with period \\(2^{n}\\). By visualizing the mathematical spaces (refer to the figure below), we can observe that they share similar features with period doubling, albeit in different domains. The spacing of the stable and unstable manifolds follows a geometric series, defining the Feigenbaum constant. By making suitable ansatz for the sequence and fitting it with \\(R(\\phi)\\), we are able to calculate \\(\\sigma=4.66914\\).\nThis derivation clarifies some confusions and provides a unique perspective on the period doubling phenomenon. It demonstrates how the logistic map, initially perceived as a one-dimensional system that cannot exhibit chaotic behavior, actually resides in an infinite-dimensional space that converges to chaos. Moreover, it reveals that unimodal mapping is the underlying universality group characterized by the Feigenbaum constant, with the logistic map representing just one special case. In simpler derivations, numerical mappings of one set of \\(\\phi_{n}$ to $\\phi_{n-1}\\)inform us of the functional form of the renormalization group operator \\(R(\\phi)=-ag(g(-z/a))\\). Without making further assumptions, this form also enables the calculation of the Feigenbaum constant. In this sense, the renormalization group operator captures the essential structure of the onset of chaos behavior.\nBibliography Sfondrini (2012) Sfondrini,\u0026#32; A. \u0026#32; (2012). \u0026#32;Introduction to universality and renormalization group techniques.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1210.2262 Sethna (2020) Sethna,\u0026#32; J.\u0026#32; (2020). \u0026#32; Entropy, Order Parameters, and Complexity (2). \u0026#32; Clarendon Press. P. Kopietz (2010) P. Kopietz,\u0026#32; F.\u0026#32; (2010). \u0026#32; Introduction to the Functional Renormalization Group (1). \u0026#32; Springer, Berlin Heidelberg 2010. https://doi.org/10.1007/978-3-642-05094-7 ","permalink":"https://htsod.github.io/posts/rg_method/","summary":"Renormalization Group Approach in Dynamical System The renormalization group (RG) method is an approximation technique initially developed for solving strongly interacting many-body problems in quantum field theory, where perturbative solutions deviate from the actual solutions. The fundamental concept of the renormalization group approach is to eliminate irrelevant degrees of freedom in a physical system while preserving its essential characteristics ( Citation: P. Kopietz,\u0026#32;2010 P. Kopietz,\u0026#32; F.\u0026#32; (2010). \u0026#32; Introduction to the Functional Renormalization Group (1).","title":"Renormalization Group: A Descriptive Overview"}]