[{"content":"Table of Content I. Passive Components Passive Components: Physical Laws, Transient Dynamics, and Resonance\n1. Resistor circuit Current law Voltage law Voltage and Current Relation Conductor Resistor and Ohm\u0026rsquo;s law Thevenin\u0026rsquo;s theorem 2. RCL circuit Capacitance Inductance Linear ODE form by R, C and L Time Series Analysis Transfer function Example: Band-stop and Band-pass II. Nonlinear Devices 3. Diode Rectifier Zener diode Shottky diodes 4. Simple Modelling of Bipolar Junction Transistor Bipolar Junction Transistor 5. Mathematical Model of BJT Ebers-Moll-Early Model Gummel-Poon Model 6. Field Effect Transistor FET behavior Other FET Model III. Operational Amplifier Basic 7. Ideal Op-Amp Op-Amp basic Negative feedback loop Non-ideal Op-Amp 8. Noise Analysis Instrumental operation amplifier 9. Oscillator Positive feedback loop IV. Control Theory 10. PID Control linear control plants Proportional-Integral (PI) Control Proportional-Integral-Derivative (PID) Control 11. Optimal Control Theory Calculus of variation Pontryagin\u0026rsquo;s minimum principle V. Digital Electronics VI. Information Processing Physics and Mathematics Appendics Bibliography Steck (2019) Steck,\u0026#32; D.\u0026#32; (2019). \u0026#32; Analog and digital electronics. \u0026#32; https://steck.us/teaching/. Horowitz\u0026#32;\u0026amp;\u0026#32;Hill (2015) Horowitz,\u0026#32; P.\u0026#32;\u0026amp;\u0026#32;Hill,\u0026#32; W.\u0026#32; (2015). \u0026#32; The art of electronics (3). \u0026#32; Cambridge University Press. Floyd (2015) Floyd,\u0026#32; T.\u0026#32; (2015). \u0026#32; Digital fundamentals (11). \u0026#32; Pearson Education. ","permalink":"https://htsod.github.io/ee/ee_core/","summary":"Table of Content I. Passive Components Passive Components: Physical Laws, Transient Dynamics, and Resonance\n1. Resistor circuit Current law Voltage law Voltage and Current Relation Conductor Resistor and Ohm\u0026rsquo;s law Thevenin\u0026rsquo;s theorem 2. RCL circuit Capacitance Inductance Linear ODE form by R, C and L Time Series Analysis Transfer function Example: Band-stop and Band-pass II. Nonlinear Devices 3. Diode Rectifier Zener diode Shottky diodes 4. Simple Modelling of Bipolar Junction Transistor Bipolar Junction Transistor 5.","title":"Study Notes in Electrical Engineering"},{"content":"Site and bond percolation models exhibit a phase transition at a shared critical point, where both demonstrate self-similarity and scale invariance—hallmarks of continuous phase transitions. Using the Renormalization Group (RG) method in ( Citation: Sethna,\u0026#32;2020 Sethna,\u0026#32; J.\u0026#32; (2020). \u0026#32; Entropy, Order Parameters, and Complexity (2). \u0026#32; Clarendon Press. ) , which proposed a scaling procedure from the assumption of self-similar, we derive the scaling exponents for the percolation universality class. While this top-down approach offers pedagogical simplicity, it lacks the physical intuition provided by the Ginzburg-Landau framework. To bridge this gap, we explore a cross-referenced analysis of these two approaches, aiming to deepen our understanding of network percolation phenomena.\nIntroduction Percolation theory studies the connected compoents of a damaged graph \\(G=(V, E)\\), where \\(V\\) denotes the number of nodes and \\(E\\) denotes the number of edges. The damaging process is done by removing the edges with probability \\(p\\) - this is called bond percolation - Or removing the nodes with probability \\(p\\) - this is called site percolation.\nThe percolation process is closely related to the continuous phase transition discussed in statistical mechanics. One physical example of percolation is the sharpe drop of electrical conductivity as one punches holes on a metal sheet to its critical value. ( Citation: Last\u0026#32;\u0026amp;\u0026#32;Thouless,\u0026#32;1971 Last,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;Thouless,\u0026#32; D. \u0026#32; (1971). \u0026#32;Percolation theory and electrical conductivity. Phys. Rev. Lett.,\u0026#32;27.\u0026#32;1719–1721. https://doi.org/10.1103/PhysRevLett.27.1719 ) The conductivity of the metal sheet can be written as a function of the area of the holes and it embodies continuous change of physical behavior.\nConsider having a few holes to a intact sheet. Current could always find ways to go around the holes featuring the conducting regime. As the number of holes increases to the critical value, the allowed conduction path narrowed down, and eventually the metal sheet falls apart and become fully disconnected. The conductivity varies continuously as area of the holes are being punched through, meanwhile also exhibit two regimes of behavior as the parameter is varied. This experimental set-up is analogous to a percolation problem.\nPercolation does not limit to a particular physical phenomena. In fact, it is a common trait of many network system. For example, when modelling the spread of forest fire and infectious diseases ( Citation: Stauffer\u0026#32;\u0026amp;\u0026#32;Aharony,\u0026#32;1994 Stauffer,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Aharony,\u0026#32; A.\u0026#32; (1994). \u0026#32; Introduction to percolation theory. \u0026#32; Taylor \u0026amp; Francis. ) , percolation transitions are frequently observed. So, it will be convinient to have a simple random graph model to solely study the percolation effect. In this brief review, we will be considering two dimensional bond percolation and site percolation from a top-down approach. We simulate the model and quantify the observables to derive the scaling law at and near the critical point. Then, we will be justifying some of the observation under the framework of Ginzburg-Landau Equation.\nExperiment Set-up Network Simulation In a bond percolation network, we initialize a fully connected \\(L \\times L\\) grid lattice and enforcing the periodic boundary condition. Each bond would have probability \\(p\\) to be removed or equivalently having \\(1- p\\) probability to be added. For a site percolation, we initialze the graph on a fully connected triangular \\(L \\times L\\) lattice. Though this time we are removing the site instead of the bond.\nimport main # Plot a grid network with dimension m x n. Edges have 0.5 probability to be removed m, n, p = 20, 20, 0.5 main.plot_grid_clusters(m, n, p) # Plot a grid network with dimension m x m. Edges have 0.5 probability to be removed # By construction the triangular graph must have L x 2L to be have equivalent sites as the grid network m, n, p = 20, 40, 0.5 G = main.triangular_percolation(m, n, p) main.plot_triangular_cluster(G, m, n) On the left of the graph shows the bond percolation, where each bond between the nodes have probability \\(p=1/2\\) to be removed. On the right shows the site percolation with the same probability \\(p=1/2\\). Components of the graph is color coded by their relative size. The larger cluster tend to have a darker color.\nAs observe from the figure above, the system parameters to control the percolation are the same (the same lattice size \\(L\\) and probability of removal \\(p\\)), the microscopic detail, such as the number of nearest neighbor, is different. For the bond percolation, each node has four nearest neighbor whereas for site percolation, each node has six nearest neighbor.\nPhases near the critical point How do we tell that these model exhibit phase transition? To answer this question, we investigate the observables of the graph, such as the distribution of the size of the network components \\(n(S)\\), where \\(S\\) refers to the size of the component. As well as the normalized largest components for ensembles with \\(P(p) = S_{largest}(p)/L^{2}\\), where \\(S_{largest}\\) representing the largest cluster as a function of the probability \\(p\\).\nWe distinguish different phases by the change of symmetry of the system in interest, which often manifest itself through the scaling law of the observables near the criticla point. For example, the melting of ice exhibit a broken translation symmetries changing from liquid to solid. Symmetry, as we know it, cannot change smoothly ( Citation: Anderson,\u0026#32;2019 Anderson,\u0026#32; P.\u0026#32; (2019). \u0026#32; Basic notions of condensed matter physics. \u0026#32; Taylor \u0026amp; Francis Limited. ) . Hence, the observables will have a sharp transition, known as scaling, that separate two regimes of behavior.\nTwo regimes of behavior To see that the two regimes of behavior exhibit, we take the limit of these observables as \\(p\\) ranging from \\(0\\) to \\(1\\). At \\(p\\rightarrow 0\\), it is a fully connected graph with \\(P(p) = 1\\) and \\(n(S)= \\delta_{S=L\\times L}L \\times L\\). As \\(p \\rightarrow 1\\), it will become a sparsely connected graph and the corresponding \\(P(p) = 0\\) and \\(n(S) = 1\\) in the large \\(L\\) limit.\nHowever, checking the limits will only tell us that the system exhibit two regimes of behavior but necessarily tell us that there must be a critical point separating the two phases. How do we know that there indeed exists a critical point at the phase transition but not some smooth varying function that changes polynomial with \\(p\\), such as \\(P(p) \\propto (1-p) \\). In general, it is fairly difficult to come up with the analytic form of the observables, and in the case of percolation, it is impossible to have analytic form of the function that covers the entire domain. However, it is possible to derive the behavior near the critical point using the method of Renormalization Group (RG) to approximate the critical behavior. As promosed, from a top-down approach, we would rely on numerical simulation to decide the existence of phases, and then justify the use of RG approach.\nNumerical simulation Running simulation with \\(L = 20\\) for different probability \\(p\\) and comparing the distribution. One will find that the grid percolation has a sharp transition about the critical point \\(p_{c} \\sim 0.5\\). Plotting the grid percolation near the critical point:\nFor \\(p \u0026lt; 0.5\\), as shown for the bond percolation on the left figure, the largest cluster nearly covers the entire graph showing a phase with large \\(P(p=0.4)\\) value. In constrast, at \\(p \u0026gt; 0.5\\) as shown on the right figure, clusters broken down in smaller proportion, showing a distinct phase with much smaller \\(P(p=0.6)\\) value. Even as we close up the difference to the critica value of \\(0.5\\), these two phases persist, which validates our claim about the system having a critical point at \\(p_{c}=0.5\\) through a numerical method.\nOn contrast, we could derive the value of transition if we assume the system must exhibit a critical point where the system drastically changes its behavior. Suppose grid network \\(A\\) percolates at \\(p_{c}\\) for \\(p_{c} \\in (0, 1) \\). We could always construct an dual grid network \\(B\\) by shifting all the nodes to the diagonal and re-connect the nodes where there isn\u0026rsquo;t any connection in \\(A\\). The two graph is complementary by construction. So, \\(p_{A} = 1- p_{B}\\). In addition, both model is statistically equivalent, that implies both network percolate at the same critical value \\(p_{c}\\). Hence, \\(p_{c} = 1 - p_{c}\\), yielding the critical value at \\(p_{c} = \\frac{1}{2}\\).\nScaling-law Behavior about the critical point Right at the critical points, it becomes increasingly difficult to tell the phases of the system. However, there are some interesting observation that could help us get started in the analysis of criticallity.\nFor the largest cluster with the simulation running at \\(L = 1024\\) and \\(p = 0.5\\), no matter if we running with grid or triangular lattice, it displays roughly the same pattern. This is known as universality. In the thermodynamics limit, the macroscopic observables are incensitive to the microscopic detail; the microscopic details behave as a fluctuation that goes away when summed over a large system size.\nOne the left column, we have bond percolation(top) and edge percolation(bottom) at a relatively small system size of \\(L = 20\\). On the right column, we have the largest cluster from the corresponding bond percolation(top) and edge percolation(bottom) at a much larger system size with \\(L = 1024\\). Though differed in their density in the large system limit, both models are statistically similar, demonstrating universality.\nTo quantify the pattern that we are seeing, we propose a coarse grain procedure and it goes as follow: we average over the pixel values of four lattices in squre and replacing the four lattices with a larger square with the average of the four, essentially shrinking the lattice from a size of \\(1024 \\times 1024\\) to \\(256 \\times 256\\); the coarse grained figure remain statistically equivalent. This is known as self similar and scale invariance, two well known features of continuous phase transition.\nPower-law at \\(p_{c}\\) It is precisely from the observation of self-similar of the system at a large size limit that motivates the top-down derivation of the scaling-law in the percolation network.\nUnder coarse graining, we will be measuring every observables of the system with a new length scale. Self-similar thus refering to the invariant of the distribution under the change of length scale.\nLet \\(D(S)\\) be the distribution of the cluster with size \\(S\\). The statement of scale invariance then can be expressed as:\n$$D^{\\prime}(S) = D(S)$$\nwhere \\(D \\rightarrow D^{\\prime}\\) refering to the change of length scale. And hence, the functional of \\(D\\) has no explicit dependent on \\(l\\):\n$$\\partial D / \\partial l = 0$$\n$$ \\frac{dD}{dl} = \\frac{\\partial D}{\\partial S} \\frac{dS}{dl} + \\frac{\\partial D}{\\partial l} = \\frac{\\partial D}{\\partial S} \\frac{dS}{dl}$$\nThis is precisely the embodiment of self-similar in the spatial distribution, also known as the fixed point of the RG flow in the parameter space.\nHow about the functional dependent of of the distribution \\(D(S)\\) on the size of the distribution \\(S\\) under rescaling? Since the cluster size \\(S\\) is dependent on the length scale of the system. Under coarse graining it will be scaled down by some factor \\(C = 1 + cdl\\).\n$$ S^{\\prime} = S/C = S/(1+cdl) \\approx S-cSdl$$ $$ S^{\\prime} - S = dS = -cSdl $$ $$ dS/dl=-cS $$\nConsequently, measuring the distribution with coarse grained cluster size will have the following form:\n$$ D^{\\prime}(S^{\\prime}) = D(S^{\\prime}) = D(S- cSdl) = D(S + \\frac{dS}{dl} dl)=D(S)+\\frac{d D}{d S}dS$$\nWhen the cluster gets smaller, the number of cluster will surely increase, requiring \\(D(S^{\\prime})\\) to scale positively with some factor \\(A = 1+ adl\\).\n$$D(S^{\\prime}) = AD(S) = D(S)(1+adl) = D(S) + aDdl$$\nEquating the two equations above to give:\n$$ \\frac{dD}{dS}dS = a Ddl $$\nRearanging to remove the dependence on \\(dl\\) and define \\(\\tau = \\frac{a}{c}\\): $$ \\frac{dD}{D} = -\\frac{a}{c}\\frac{dS}{S} $$\n$$ D(S) \\propto S^{-a/c} = S^{-\\tau} $$\nThe distribution follows a scale-free dependent on the size of the cluster with exponent \\(\\tau = a/c\\). Testing the scaling relation with the theoretical exponent of \\(\\beta = 187/91\\)\nThe fluctuation on the experimental value is due to the finite size of the experiment. Each interval of \\(\\delta S\\) is binned. If there is insufficient samples, sometimes it will be gapped like shown on the graph above. But it is obvious that overall trend fluctuatues around the theoretical predicted value which indeed proves the claims that the distribution follows a power-law scaling (showing a straight line on a log-log plot). Noting that both grid percolation and triangular percolation do indeed from the same universality class; they have the same exponent at the critical region.\nPower-law near \\(p_{c}\\) Not only the distribution of the cluster size follow the power-law, the quantity \\(P(p)\\) as the function of \\(p\\) also follows a power-law. We could go through the similar derivation of scaling and yield the following relation:\n$$ P(p) \\sim (p- p_{c})^{\\beta} $$\nHowever, if we plot the actual distriubtion, the scaling exponent is quite off from the theoretical value of \\(\\beta = 5/36\\). What is going on?\nQualitatively, a graph could be sparse or dense depending on the number of shortcuts. For network of \\(L=500, k = 2\\). The left graph is produced by \\(p=0.001\\) and the right graph is produced by \\(p = 0.1\\)\nThis is because the model is not exactly in the thermodynamics limit. The phase transition and the scaling-law is well-defined only when the system is in the thermodynamics limit where the mean field theory holds.\nWe could verify this claim by finite-size scaling and write the distribution as a function of both the probability \\(p\\) and the size of the system \\(L\\). As the increase the size of the sytem, the \\(P(p, L)\\) will collapse to a single function. It will become more accurate as the system size increases.\nQualitatively, a graph could be sparse or dense depending on the number of shortcuts. For network of \\(L=500, k = 2\\). The left graph is produced by \\(p=0.001\\) and the right graph is produced by \\(p = 0.1\\)\nSome Flying Questions Why phase transition occur in these simulated model? In thermodynamics system, the macroscopic state is determined by the free energy of the system. Equilibrium happens when the free energy is minimized. At high temperature, entropy is maximized to minimize the free energy. At low temperature, the energy becomes the dominating term. So, the free energy is minimized by minimizing the energy. One might draw a similar analog in the case of percolation. At high probability of bond removal, the individual tend to form smaller cluster, a disorder favored state. In the low-p limit, nodes tend to form a large cluster to minimize \u0026ldquo;energy\u0026rdquo;.\nFor a given system, what is the relation between fractal, scaling-law and criticallity? In the percolation network, we observe scale invariant. And we propose a scaling procedure to derive the exponent. Is there a guarantee? If there is a continuous phase transition, there must be fractal pattern, or scale invariant at the critical point. Is the vice versa true? If we observe a fractal pattern in nature, then we say that the system is undergoing a phase transition.\nExplaining with Ginzburg-Landau Equation Continuous phase transition Different phases are characterized by different symmetries or different pattern that varies continuously through the phae boundary. Propose a order parameter so now the free energy becomes a functional of the order parameter. It leads to discontinuous thermodynamics quantity such as heat capacity and susceptibility. Notably, the correlation length, will diverge at the phase transition, essentially dominate all length scale at the phase transition ( Citation: Hohenberg\u0026#32;\u0026amp;\u0026#32;Krekhov,\u0026#32;2015 Hohenberg,\u0026#32; P.\u0026#32;\u0026amp;\u0026#32;Krekhov,\u0026#32; A. \u0026#32; (2015). \u0026#32;An introduction to the ginzburg–landau theory of phase transitions and nonequilibrium patterns. Physics Reports,\u0026#32;572.\u0026#32;1–42. https://doi.org/10.1016/j.physrep.2015.01.001 ) . This is the justification of scale invariance at the phase transition: the physical properties dominated by the correlation length will follow a scaling law.\nIn a computer simulated program, the free energy is not always straight forward. In the case of percolation network, the free energy could be written as\n( Citation: Cirigliano,\u0026#32;Timár \u0026amp; al.,\u0026#32;2024 Cirigliano,\u0026#32; L.,\u0026#32; Timár,\u0026#32; G.\u0026#32;\u0026amp;\u0026#32;Castellano,\u0026#32; C. \u0026#32; (2024). \u0026#32;Scaling and universality for percolation in random networks: A unified view. Physical Review E,\u0026#32;110(6). https://doi.org/10.1103/physreve.110.064303 ) Subsequently, the observables of the system could be expressed as derivatives of the free energy.\nFluctuation: Ginzburg Landau Equation The Landau functional is, after all, a mean-field approach to the problem. At situation where the fluctuation dominates, the mean-field approach breakdown. Then, the region of validity of Landau function is everywhere but the critical point. Right at the critical point, the correlation length, or the fluctuation will dominiate all length scale. So, how do we suppose to learn about the behavior at the critial region?\nScale invariance: renormalization group The renormalization group method do exactly that. It integrates out the unimportant degree of freedom, from macroscale to mesoscale. With each iteration, it mimic a flow in the parameter space, where each point denote the different form of the free energy functional dependent on the order parameter. Can we do the integration indefinitely? No, that\u0026rsquo;s not possible because at the critical point, the correlation length diverges, so the number of vibration modes also diverges, so it becomes scale invariance, or a fixed point in the abstract parameter space.\nFrom the fixed point, we could linearize it, and the fixed point will be characterize by the exponent that either goes into the fixed point or goes out from the fixed point. For a well-defined phase transition, it can only have two positive exponent, also called relevent fields. Other fields will scale to zero.\nFor system that renormalizes to the same fixed point, they belong to the same universality class. These systems will share the same critical exponents. Though they are physically irrelevent, they behave the same at the critical points.\nReferences: Cirigliano,\u0026#32; Timár\u0026#32;\u0026amp;\u0026#32;Castellano (2024) Cirigliano,\u0026#32; L.,\u0026#32; Timár,\u0026#32; G.\u0026#32;\u0026amp;\u0026#32;Castellano,\u0026#32; C. \u0026#32; (2024). \u0026#32;Scaling and universality for percolation in random networks: A unified view. Physical Review E,\u0026#32;110(6). https://doi.org/10.1103/physreve.110.064303 Sethna (2020) Sethna,\u0026#32; J.\u0026#32; (2020). \u0026#32; Entropy, Order Parameters, and Complexity (2). \u0026#32; Clarendon Press. Anderson (2019) Anderson,\u0026#32; P.\u0026#32; (2019). \u0026#32; Basic notions of condensed matter physics. \u0026#32; Taylor \u0026amp; Francis Limited. Last\u0026#32;\u0026amp;\u0026#32;Thouless (1971) Last,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;Thouless,\u0026#32; D. \u0026#32; (1971). \u0026#32;Percolation theory and electrical conductivity. Phys. Rev. Lett.,\u0026#32;27.\u0026#32;1719–1721. https://doi.org/10.1103/PhysRevLett.27.1719 Hohenberg\u0026#32;\u0026amp;\u0026#32;Krekhov (2015) Hohenberg,\u0026#32; P.\u0026#32;\u0026amp;\u0026#32;Krekhov,\u0026#32; A. \u0026#32; (2015). \u0026#32;An introduction to the ginzburg–landau theory of phase transitions and nonequilibrium patterns. Physics Reports,\u0026#32;572.\u0026#32;1–42. https://doi.org/10.1016/j.physrep.2015.01.001 Stauffer\u0026#32;\u0026amp;\u0026#32;Aharony (1994) Stauffer,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Aharony,\u0026#32; A.\u0026#32; (1994). \u0026#32; Introduction to percolation theory. \u0026#32; Taylor \u0026amp; Francis. ","permalink":"https://htsod.github.io/posts/percolation_network/","summary":"Site and bond percolation models exhibit a phase transition at a shared critical point, where both demonstrate self-similarity and scale invariance—hallmarks of continuous phase transitions. Using the Renormalization Group (RG) method in ( Citation: Sethna,\u0026#32;2020 Sethna,\u0026#32; J.\u0026#32; (2020). \u0026#32; Entropy, Order Parameters, and Complexity (2). \u0026#32; Clarendon Press. ) , which proposed a scaling procedure from the assumption of self-similar, we derive the scaling exponents for the percolation universality class. While this top-down approach offers pedagogical simplicity, it lacks the physical intuition provided by the Ginzburg-Landau framework.","title":"Universality and Scale Invaraince at the Criticality of Site-Bond Percolation Network"},{"content":"In this blog, we start with a qualitative overview of the small-world effect, also known as the \u0026ldquo;six degrees of separation,\u0026rdquo; followed by a quantitative analysis using computational methods, and finally, a description of the phenomenon through the framework of phase transitions and critical phenomena. From the surface level to deeper insights, I aim to explore the mechanics underlying this phenomenon and how this understanding can be extended to other large-scale phenomena.\nSix Degree of Separation: An Inquiry into Social Cloesness If you know six people, preferably from different parts of the world in addition to your closest acquanintances, you can effectively connect to everyone on the planet. This concept, known as \u0026ldquo;six degrees of separation,\u0026rdquo; was first introduced by novelist Frigyes Karinthy. It describes the significant reduction in social distance when a few shortcuts are added in a social network. Below is a YouTube video by the youtuber Veritasium that introduces this idea (where I first learned about the concept of six degrees of separation).\nHowever, it mains as a qualitative thought in 1922 because computation and math theory is not available to quantify this social occurence.\nIt wasn\u0026rsquo;t until 1998 that Duncan J. Watts and Steven H. Strogatz introduced a random graph procedure to study this effect quantitatively. ( Citation: 1998 Watts,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Strogatz,\u0026#32; S. \u0026#32; (1998). \u0026#32;Collective dynamics of “small-world” networks. Nature,\u0026#32;393(6684).\u0026#32;440–442. https://doi.org/10.1038/30918 ) Networkx Simulation on Random Graphs The method involves generating a network (or graph interchangeably) with \\(L\\) nodes. The nearest neighbors of each node are well-defined: we arrange the nodes in a circle and connect them to their nearest neighbors, determined by the parameter \\(Z\\) (an even number for evenly distributed connections). To illustrate the procedure, below is a graph with \\(N = 20\\) and \\(Z = 4\\). The graph is generated using NetworkX, an extensive Python library for network theory.\nimport matplotlib.pyplot as plt import networkx as nx L = 20 Z = 4 p = 0 G = nx.newman_watts_strogatz_graph(L, Z, p) nx.draw(G, pos=nx.circular_layout(G)) plt.draw() plt.savefig(\u0026quot;simple_graph.png\u0026quot;) This is a simple connected network with 20 nodes, 4 nearest neighbors, and no shortcuts.\nAdding Shortcuts Now, how quickly we can travel from one point on the graph to another depends on the shortest path. By randomly rewiring the existing connections \\(Lkp\\) shortcuts between unconnected nodes, we can reduce the shortest travel distance. We characterize the connectivity by the average shortest distance (l) of the graph. The algorithm to calculate the graph\u0026rsquo;s average distance is as follows:\nStart from node \\(L_{i}\\) and measure the distance \\(d\\) to the nearest neighbors.\nAccumulate these distances, move on to those nearest neighbors, and repeat step 1 until all nodes are exhausted.\nDivide the cummulative distance by the total neighbor of \\(L_{i}\\) and yields the average distance of node \\(L_{i}\\)\nRepeat step 1 for the next nodes \\(N_{i+1}\\) until all nodes exhaust\nSum up the average distance of each nodes and divided by the number of nodes\nEffects of shortcuts on the Average Shortest Path Length NetworkX has an optimized built-in function to calculate the graph\u0026rsquo;s shortest distance, so we\u0026rsquo;ll use that. After we know the quantitative measurement of the separation distance, we rewired the existing edges probabilistically by \\(p\\). (Though we keep those wires being rewired to avoid disconnected component) On average, \\( p L k \\) shortcuts are added to the graph, where we defined \\(k\\) to be \\(k = \\frac{Z}{2}\\). Next, we illustrate the network qualitatively with different parameters. For a sparsely connected graph and a densely populated graph they are topologically very different from each other:\nQualitatively, a graph could be sparse or dense depending on the number of shortcuts. For network of \\(L=500, k = 2\\). The left graph is produced by \\(p=0.001\\) and the right graph is produced by \\(p = 0.1\\)\nFor the sparse graph above, the average seperation can go up to \\(23\\). Whereas for the densely populated graph, the distance of separation reduces to about \\(4\\).\nSo, the six degree of separation makes qualitative sense as observe from the above figure. Next, we want to know how adding the shortcuts will impact the average shortest path \\(l\\). To do this, we vary \\( p \\) from \\(0.001 \\) to \\( 0.1 \\) for a graph with \\(50\\) nodes, nearest neighbor connection \\(Z = 2\\). Plotting the result:\nN = 50 Z = 2 p = np.linspace(0.001, 1, 100) distance_distribution = [] for s in p: G = nx.newman_watts_strogatz_graph(N, Z, s) avg_shortest_path = nx.average_shortest_path_length(G) distance_distribution.append(avg_shortest_path) fig,ax = plt.subplots() ax.scatter(p, distance_distribution) ax.set_ylabel(\u0026quot;Separation\u0026quot;) ax.set_xlabel(\u0026quot;p\u0026quot;) ax.set_title(\u0026quot;Small-world effects\u0026quot;) plt.savefig(\u0026quot;small_world.png\u0026quot;) As \\(p\\) increases, the number of shortcuts \\(Lpk\\) also increases, which reduce the shortest distance with a scaling law as shown.\nWe observe a dramatic decrease in the average separation (s) in the small (p) region. Around (s = 6), the curve shifts sharply, transitioning from a drastic decrease to asymptotic behavior towards zero separation. From our computer simulation and calculations, we elaborate the vague conclusion from social science to a firm numerical justification. In what follows, we want to understand this phenomena with renormalization group, a model that studies critical behavior of systems.\nRenormalization Group Analysis of Random Graph In the renormalization group approach to this random network model, the transition from a regular graph to a random graph at \\(p = 0\\) can be treated as a continuous phase transition. Following this perspective, we refer to the work of ( Citation: 1999 Newman,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Watts,\u0026#32; D. \u0026#32; (1999). \u0026#32;Renormalization group analysis of the small-world network model. Physics Letters A,\u0026#32;263(4–6).\u0026#32;341–346. https://doi.org/10.1016/s0375-9601(99)00757-4 ) to elaborate on the scaling scheme used to derive the scaling exponent \\(\\tau = 1\\) in the form \\(\\xi \\propto p^{-\\tau}\\), where \\(\\xi\\) is analogous to the correlation length in a typical physical system.\nBefore diving into the technical details, we first ask the following questions:\nWhat kind of question we want to answer in this small world model? The six degrees of separation and the scaling graph suggest an analytic form that relates the shortest path length to system variables. However, it\u0026rsquo;s difficult to derive this form because the shortest path length is calculated via a computational routine, and no analytic form can be derived from the algorithm.\nWhy is the renormalization group method a good candidate for solving this small-world problem? The renormalization group studies the scaling behavior of systems near critical points, where the correlation length governs the entire system\u0026rsquo;s behavior. The small-world model demonstrates two regimes of behavior, suggesting a transition and a critical point. This motivates mapping the small-world problem to a phase transition problem.\nTwo regimes of behavior In the first regime of phase, the average distance scales linear with the size of the graph \\(L\\) because the only way to reach other nodes is by traversing through the neighbors.\nThe other regime is the random graph region, where the average distance scales approximately as lograithm of \\(L\\).\nBetween these two regimes, the graph undergoes critical behavior at \\(p=0\\) for a system size \\(L=\\xi\\), where the number of shortcuts approximately be \\(Lk\\xi \\approx 1\\). Here, \\(\\xi\\) is analogous to the correlation length in a typical physical system, which diverges at the critical point. For \\(pk\\xi \\approx 1\\) to hold at \\(p\\rightarrow 1\\), \\(\\xi\\) must goes to infinity at the critical point.\nTo better understand the concept of correlation length in a typical physical system, note that a continuous phase transition involves moving from a disordered state to an ordered state as a relevant parameter decreases. As the system approaches this transition, the correlation length, which measures the collective order, increases. At the critical region, the correlation length becomes very large because order tends to align throughout the system.\nIt is this diverging behavior of the correlation length near the critical point that allows us to formulate an analytic scaling relation between system parameters, enabling us to solve the small-world model using the renormalization group (RG) method.\nParameters of the system: Scaling form For those interested in learning more about the RG method and the justification for diverging correlation lengths dominating the behavior of continuous phase transitions, see ( Citation: Hohenberg\u0026#32;\u0026amp;\u0026#32;Halperin,\u0026#32;1977,\u0026#32;pp.\u0026nbsp;9-12 Hohenberg,\u0026#32; P.\u0026#32;\u0026amp;\u0026#32;Halperin,\u0026#32; B. \u0026#32; (1977). \u0026#32;Theory of dynamic critical phenomena. Rev. Mod. Phys.,\u0026#32;49.\u0026#32;435–479. https://doi.org/10.1103/RevModPhys.49.435 ) . This work also explains how scaling effectively removes irrelevant degrees of freedom from the system. This process reduces the degrees of freedom, bringing the system to a different one with a different correlation length. Except at the critical point—where the correlation length is infinite and remains unchanged under scaling—the scaling law relates systems with different correlation lengths. If this leads to a simplified solvable model, meaningful conclusions can be drawn about the complex system that we started with.\nIn our case, the regular graph region has zero shortcuts, while the random graph has more than one. Therefore, we expect the transition to occur when the first shortcut appears, \\(\\xi kp \\approx 1\\) where \\(\\xi = L\\) at some system size.\nAssuming homogeneity leads to the scaling form of the correlation length, with the relevant system parameters following a scaling form for a function \\(f(x, y)\\) that satisfies homogeneous:\n$$ f(x, y) = x^{\\lambda} g\\left(\\frac{y}{x^{\\mu}}\\right) $$\nThis is further discussed in ( Citation: Toulouse\u0026#32;\u0026amp;\u0026#32;Pfeuty,\u0026#32;1977 Toulouse,\u0026#32; G.\u0026#32;\u0026amp;\u0026#32;Pfeuty,\u0026#32; P.\u0026#32; (1977). \u0026#32; Introduction to the renormalization group and to critical phenomena. \u0026#32; Wiley. ) n our problem, the only relevant length scales are the average shortest path length \\(\\xi\\) and the number of nodes \\(L\\). Based on the assumptions in Newman\u0026rsquo;s paper, we obtain the following scaling equation:\n$$ l = L f\\left(\\frac{L}{\\xi} \\right) $$\nAlternatively, assuming \\(\\xi \\propto p^{-\\tau}\\):\n$$ l = L f\\left(L p^{-\\tau} \\right) $$\nWe propose a scaling procedure where \\(L^{\\prime} = \\frac{1}{2}L \\), reducing the system by half, and \\(p^{\\prime} = 2p\\), keeping the number of shortcuts \\(Lkp\\) unchanged.\nGrouping procedure proposed by Newman and Watts. Dot with the same color are merged by scaling.\nComparing the ratio \\(\\frac{l^{\\prime}}{l}\\) and solvig for \\(\\tau\\) we find:\n$$ \\tau = \\frac{\\log{(L/L^{\\prime})}}{\\log{(p^{\\prime}/p)}} = 1$$\nReaching the result \\(\\tau =1 \\) requires significant work, including making educated assumptions about the model. After all, the small-world model is not governed by simple universal laws but by collective behavior that blurs microscopic details. Newman and Watts verified the exponent \\(\\tau = 1\\) through extensive numerical experiments. They also compared the system\u0026rsquo;s behavior if the exponent was \\(\\tau = \\frac{2}{3}\\), derived from an alternative RG approach but yielding less accurate results.\nThe scaling behavior for \\(k = 1\\) and \\(k =5\\). For \\(\\tau = \\frac{2}{3}\\) is also tested in the top-right corner of the figure\nFor \\(k \u0026gt; 1\\), the authors adopted a different coarse-graining approach to handle computational limitations, but the same exponent was obtained. As an extension, they also derived the scaling and exponent for systems with higher dimensions. For interested readers, detailed derivations can be found in the paper ( Citation: 1999 Newman,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Watts,\u0026#32; D. \u0026#32; (1999). \u0026#32;Renormalization group analysis of the small-world network model. Physics Letters A,\u0026#32;263(4–6).\u0026#32;341–346. https://doi.org/10.1016/s0375-9601(99)00757-4 ) Remarks In this blog, we have revisited the arguments from Newman and Watts\u0026rsquo; paper, reinterpreting them within the framework of phase transitions. Along the way, we provided additional physical insights and explanations for steps that were either implicit or assumed in the original work.\nTo understand complex phenomena through the lens of phase transitions and the renormalization group (RG), it is crucial to first identify the distinct regimes of behavior, where the system’s qualitative properties change significantly. These transitions are marked by changes in symmetry—something that, according to condensed matter physics, cannot occur gradually. This is why conventional methods like perturbation theory are inadequate in such cases. Instead, a scaling law must be derived, capturing the relevant parameters at the critical point.\nHowever, deriving this scaling law involves substantial numerical work and carefully made assumptions. In our application of the RG method, we focused on the scaling form to extract the exponent, yet the deeper physical meaning of the RG flow itself remains somewhat elusive.\nThe true power of the RG method lies in its ability to connect systems that, despite appearing different, share the same critical behavior as they approach a diverging correlation length. This ability to group systems by their underlying topological structure is a key strength of the RG approach, but was not entirely clear how this aspect of the RG approach plays in the application on the small-world model.\nReference Newman\u0026#32;\u0026amp;\u0026#32;Watts (1999) Newman,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Watts,\u0026#32; D. \u0026#32; (1999). \u0026#32;Renormalization group analysis of the small-world network model. Physics Letters A,\u0026#32;263(4–6).\u0026#32;341–346. https://doi.org/10.1016/s0375-9601(99)00757-4 Hohenberg\u0026#32;\u0026amp;\u0026#32;Halperin (1977) Hohenberg,\u0026#32; P.\u0026#32;\u0026amp;\u0026#32;Halperin,\u0026#32; B. \u0026#32; (1977). \u0026#32;Theory of dynamic critical phenomena. Rev. Mod. Phys.,\u0026#32;49.\u0026#32;435–479. https://doi.org/10.1103/RevModPhys.49.435 Toulouse\u0026#32;\u0026amp;\u0026#32;Pfeuty (1977) Toulouse,\u0026#32; G.\u0026#32;\u0026amp;\u0026#32;Pfeuty,\u0026#32; P.\u0026#32; (1977). \u0026#32; Introduction to the renormalization group and to critical phenomena. \u0026#32; Wiley. Watts\u0026#32;\u0026amp;\u0026#32;Strogatz (1998) Watts,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Strogatz,\u0026#32; S. \u0026#32; (1998). \u0026#32;Collective dynamics of “small-world” networks. Nature,\u0026#32;393(6684).\u0026#32;440–442. https://doi.org/10.1038/30918 ","permalink":"https://htsod.github.io/posts/small_world/","summary":"In this blog, we start with a qualitative overview of the small-world effect, also known as the \u0026ldquo;six degrees of separation,\u0026rdquo; followed by a quantitative analysis using computational methods, and finally, a description of the phenomenon through the framework of phase transitions and critical phenomena. From the surface level to deeper insights, I aim to explore the mechanics underlying this phenomenon and how this understanding can be extended to other large-scale phenomena.","title":"Phases Transition and Renormalization Group method applied in Random Graphs Model"},{"content":"Fourier Methods could be derived entirely from Group theory! As the title suggests, the entire concept of the Fourier transform can be derived if we understand some basic group theory. We start by introducing the group \\(Z_{N}\\) and its irreducible represenations. Using the orthogonality theorem, we will then derive the discrete Fourier transform (DFT) and the Fourier transform (FT).\nFourier analysis studies the periodicities of functions. Any continuous and differentiable function can be broken down into a linear combination of its frequency components, which is the foundation of Fourier series. The Fourier transform allows us to switch between real space and frequency space representations.\nApproaching this from group theory, we recognize that an arbitrary function is a reducible representation of the group \\(Z_{N}\\), or the group of integers modulo \\(n\\). By framing the description in this way, we essentially abstracted the Fourier method as a representaion of the \\(Z_{n}\\) symmetries.\n\\(Z_{n}\\) is the abelian discrete group of order \\(n\\). It captures the group structure of the periodic pattern on a interval of \\( n \\). The reducibility of a periodical function means that the representation could descompose into linear combination of the irreducible representation of the group \\(Z_{n}\\).\nTaking a simple example to illustrate the idea of reducibility. Consider an ar function \\(F(x)\\) in a real space that satisfies the periodic condition\n$$ F(x) = \\sum_{n} w_{n} f(x+na) = F(x) $$\nfor \\(n = 0, \\pm1, \\pm2, \u0026hellip;\\).\nA sinusoidal function as one of the fourier mode \\(f(x) = Asin(\\frac{2\\pi x}{a})\\) satisfies the above condition for \\(n = \\pm 1\\) with weight \\(w_{1} = A\\). In group theory we consider \\(F(x)\\) as the reducible representation of \\(Z_{n}\\) with \\(f(x)\\) being the irreducible representation of \\(Z(n = \\pm 1)\\).\nA slight hint of universality Light and sound waves, for example, can be broken down into Fourier components regardless of the difference in the microscopic details. For example, light is a form of electromagnetic wave with the underlying symmetry of continuous Lorentz group \\(SO(1, 3)\\). As withh the propagation of sound wave, it relies on the homogenity medium such as air or solid that has a continuous translation symmetry. Perturbing the system in a certain direction we break the continuous symmetry by favouring a direction, triggered the \u0026ldquo;soft mode\u0026rdquo; of the system causing a relaxation dynamics that leads to the a propagating Fourier wave. It will be interesting to investigate these symmetries breaking effect but that is another topics for another day.\nIn the next section, we’ll briefly touch on results from representation theory and use them to derive some key theorems related to the Fourier method.\nRepresentation Theory: The Core Tool One of the most significant results in group theory is Schur\u0026rsquo;s lemma. It states that if an operator \\(H\\) commutes with a group representation \\(D(g)\\), then \\(H\\) must be diagonal, with \\ \\(H = \\lambda I\\). Using this lemma, we can derive some powerful theorems in group representation theory. (For a slow derivation, see chapter 2 on ( Citation: Zee,\u0026#32;2016 Zee,\u0026#32; A.\u0026#32; (2016). \u0026#32; Group theory in a nutshell for physicists. \u0026#32; Princeton University Press.\u0026#32;Retrieved from\u0026#32; https://books.google.co.jp/books?id=FWkujgEACAAJ ) or a get-to-the-point derivation see chapter 2 on ( Citation: Dresselhaus\u0026#32;\u0026amp;\u0026#32;Dresselhaus,\u0026#32;2002 Dresselhaus,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Dresselhaus,\u0026#32; G.\u0026#32; (2002). \u0026#32; Group theory: Application to the physics of condensed matter. \u0026#32; Springer. ) )\n$$ \\sum_{g}{D^{(r)\\dagger} (g)_ {j}^{i}D^{(s)}(g)_ {k}^{l}} = \\frac{N(G)}{d}\\delta_{l}^{i}\\delta_{j}^{k} $$\nThis relation holds when summing over group element \\(g\\) and representations \\((r)\\) and \\((s)\\) are orthogonal to each other. Some important corollaries include:\nDimensions of the irreducible representations: $$ \\sum_{r}{d_{r}^{2}} = N(G) $$\nOrthogonality of irreducible representations: $$ \\sum_{c}{n_{c}(\\chi^{(r)}(c))^{\\ast} \\chi^{(s)}(c)} = N(G) \\delta ^{rs} $$\nOrthogonality of classes: $$ \\sum_{r}{\\chi^{(r)}(c)^{\\ast} \\chi^{(r)}(c^{\\prime})} = \\frac{N(G)}{n_{c}}\\delta^{cc^{\\prime}}$$\nNumber of irreducible representation equal to the number of class in the group:\n$$ N(C) = N(R) $$\nA test on reducibility: $$ \\sum_{c}{n_{c} \\chi^{\\ast(r)}\\chi(c)} = N(G)n_{r} $$\nSymmetry and \\(Z_{N}\\) Our first exposure to the Fourier transform is often linked to periodicity, a pattern repeating in time. However, group theory doesn\u0026rsquo;t care if the pattern is in time or any other variable, as long as the symmetry holds. Consider partitioning a rotation of \\(2\\pi\\) into \\( N \\) parts on the complex plane, represented as \\(e^{i\\frac{2\\pi}{N}j}\\) for \\(j = 0, 1, \u0026hellip;, N-1\\). This is the \\(Z_{N}\\) group where \\(g^{N} = I\\). Since \\(Z_{N}\\) is an abelian, each element forms its own class, and each irreducible representation has a dimension of 1.\nBy guessing an irreducible representation that satisfies the multiplication structure: $$ D(e^{i\\frac{2\\pi}{N}j})D(e^{i\\frac{2\\pi}{N}j^{\\prime}}) = D(e^{i\\frac{2\\pi}{N}(j+j^{\\prime})}) $$\nwe propose:\n$$ D(e^{i\\frac{2\\pi}{N}j}) = e^{i\\frac{2\\pi}{N}jk} \\quad k = 0, 1, \u0026hellip;, N-1$$\nNow, using orthogonality:\n$$ \\frac{1}{N}\\sum_{j=0}^{N}{e^{-i\\frac{2\\pi}{N}jk} e^{i\\frac{2\\pi}{N}jk^{\\prime}}} = \\delta^{kk^{\\prime}} $$\nThis gives us the discrete Fourier transform (DFT) and its inverse.\nThis is the same old discrete fourier transform and inverse discrete fourier transform but entirely from group theory along.\nOkay, but how about the idea that for a given function that compose of different vibration mode. Recall that we could break down that function into weighted component of fourier parts. What do group theoretic approach has to do with this?\nWe recognize this given function as an reducible representation that contains various irreducible representation \\(n_{r}\\) times.\nSo, given the reducible representation \\(\\chi(j)\\) applying the test of reducibility gives the weight of the fourier component \\( n_{k} \\)\n$$ \\frac{1}{N} \\sum_{k=0}^{N-1}{e^{-i\\frac{2\\pi}{N}jk} \\chi(j)}= n_{k} $$\nThen, the reducible representation can be written as the sum of each of irreducible representation with weight \\( n_{k} \\)\n$$ \\chi(j) = \\sum_{k=0}^{N-1}{n_{k}e^{i\\frac{2\\pi}{N}jk}} $$\nAlso known as the Fourier series.\nContinum limit: from \\(Z_{N}\\) to \\(U(1)\\) and from DFT to FT To extend the DFT to the continuous Fourier transform (FT), we take the limit \\(N \\rightarrow \\infty\\). The discrete group \\(Z_{N}\\) becomes \\(U(1)\\), and the summation becomes integration:\n$$\\sum_{k=0}^{N-1} \\rightarrow \\int_{0}^{\\infty}d\\mu(g)$$\nwhere \\(\\mu(g)\\) is the group measure.\nTo evaluate the orthogonality relation in the continuous case, one has to find the trace and the group measure as demonstrated as follow.\nfinding the trace \\(U(1)\\) group has the property that \\(U^{\\dagger}U = I\\). For \\(U(1)\\), it furnishes one dimension representation only and hence the trace of the representation is just the representation itself. We propose that \\(D(\\theta) = \\chi(\\theta, k) = e^{i\\theta k}\\). It clearly satisfies the unitary condition and preserve the algebraic structure of the group multiplication. The only difference is that now \\(\\theta\\) runs as continuous variable from \\(0 \u0026lt; \\theta \u0026lt;2\\pi\\).\nfinding the group measure The purpose of finding a group measure is to make the integral cover the group manifold. Hence, we could intepret the group measure \\(d\\mu(g)\\) runs over the group manifold. In this trivial case of \\(U(1)\\), the group manifold is merely a cycle and hence to cover the manifold we propose the following integral \\(\\int_{0}^{2\\pi}{d\\theta}\\).\nApplying orthogonality, we obtain:\n$$ \\int_{U(1)}{\\chi^{\\ast}(k,g)\\chi(k^{\\prime},g)d\\mu(g)} = \\int_{0}^{2\\pi}{e^{-i\\theta k^{\\prime}}e^{i\\theta k}d\\theta} = 2\\pi \\delta_{kk^{\\prime}}$$\nThus, we arrive at the Fourier transform.\nFinal Remark To summarize, from group theory, we recover the following results:\nFrom Discrete Group\nDFT:\\( \\frac{1}{N}\\sum_{j=0}^{N}{e^{-i\\frac{2\\pi}{N}jk} e^{i\\frac{2\\pi}{N}jk^{\\prime}}} = \\delta^{kk^{\\prime}} \\)\nInverse DFT:\\( \\sum_{k=0}^{N}{e^{-i\\frac{2\\pi}{N}jk} e^{i\\frac{2\\pi}{N}j^{\\prime}}} = \\delta^{jj^{\\prime}} \\)\nFourier Series: \\( \\chi(j) = \\sum_{k=0}^{N-1}{n_{k}e^{i\\frac{2\\pi}{N}jk}} \\)\nFrom Continuous Group\nFourier Transform: \\( \\int_{U(1)}{\\chi^{\\ast}(k,g)\\chi(k^{\\prime},g)d\\mu(g)} = \\int_{0}^{2\\pi}{e^{-i\\theta k^{\\prime}}e^{i\\theta k}d\\theta} = 2\\pi \\delta_{kk^{\\prime}} \\) One advantage of viewing the Fourier method through the lens of group theory is that it reveals Fourier methods as simply a consequence of translational symmetry. This perspective allows for potential generalizations to other orthogonality theorems, such as those based on symmetries like \\(SU(3)\\) in particle physics, which underpins the Standard Model.\nBibliography Dresselhaus\u0026#32;\u0026amp;\u0026#32;Dresselhaus (2002) Dresselhaus,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Dresselhaus,\u0026#32; G.\u0026#32; (2002). \u0026#32; Group theory: Application to the physics of condensed matter. \u0026#32; Springer. Zee (2016) Zee,\u0026#32; A.\u0026#32; (2016). \u0026#32; Group theory in a nutshell for physicists. \u0026#32; Princeton University Press.\u0026#32;Retrieved from\u0026#32; https://books.google.co.jp/books?id=FWkujgEACAAJ ","permalink":"https://htsod.github.io/posts/fourier/","summary":"Fourier Methods could be derived entirely from Group theory! As the title suggests, the entire concept of the Fourier transform can be derived if we understand some basic group theory. We start by introducing the group \\(Z_{N}\\) and its irreducible represenations. Using the orthogonality theorem, we will then derive the discrete Fourier transform (DFT) and the Fourier transform (FT).\nFourier analysis studies the periodicities of functions. Any continuous and differentiable function can be broken down into a linear combination of its frequency components, which is the foundation of Fourier series.","title":"Fourier Transforms: A Group Theoretic Perspective"},{"content":" The Wave and Particle Dispute The contradiction between classical physics and microscopic phenomena is one of the most fascinating episodes in the history of science, reshaping our fundamental understanding of waves and particles. To grasp the weirdness of wave-particle duality, let’s start with a simple analogy.\nImagine shooting bullets at a wall with two equally spaced gaps. As expected, the bullets passing through each gap will behave independently, forming two distinct patterns on a measurement panel behind the wall. Each pattern would look like a Gaussian distribution centered around the corresponding slit. The resulting measurement would be a straightforward combination of these two distributions:\n$$ \\rho_{12}(x) = \\rho_{1}(x) + \\rho_{2}(x) $$\nThis is because the events are independent, and the densities of bullet impacts simply add up.\nNow, let’s replace the bullets with waves—perhaps water waves or sound waves—propagating through the same two slits. Experimentally, something entirely different occurs. Instead of the waves passing independently through the slits, they interfere with each other, producing a distinctive interference pattern on the panel.\nIf we describe the waves mathematically, let \\( h_{1}(x)e^{i2\\pi \\nu t} \\) and \\( h_{2}(x)e^{i2\\pi \\nu t} \\) represent the waves passing through the first and second slits, respectively. When both slits are open, the total wave is: $$ (h_{1}(x) + h_{2}(x))e^{i2\\pi \\nu t} $$\nThe intensity of this combined wave is given by the square of the total wave function:\n$$ I_{12} = |h_{1}(x) + h_{2}(x)|^{2} = |h_{1}(x)|^{2} + |h_{2}(x)|^{2} + h_{1}(x)h_{2}^{\\ast} + h_{1}(x)^{\\ast}h_{2} $$\nThis reveals something new: the presence of interference terms. Unlike the case with bullets, where the results simply add, waves interact, creating regions of constructive and destructive interference:\n$$ I_{12}(x) = I_{1}(x) + I_{2}(x) + I_{interference}(x) \\neq I_{1}(x) + I_{2}(x) $$\nThe Wave-Particle Mystery This wave-particle duality extends beyond classical waves. Experiments with electrons—and more recently, with larger molecules like \\( C_{60} \\) (the largest known entity to show this duality)—reveal that quantum particles exhibit both wave-like and particle-like behaviors. As the famous physicist Richard Feynman put it:\n\u0026hellip; a phenomennon which is impossible, absolutely impossible, to explain in any classical way, and which has in the heart of quantum mechanics, \u0026hellip; We can not make the mystery go away by \u0026rsquo;explaining\u0026rsquo; how it works. We will just tell you how it works.\nThis duality isn’t the only strange aspect of quantum mechanics. Quantum effects are also evident in blackbody radiation and spectroscopy. In the case of blackbody radiation, it became necessary to assume that energy levels are discrete to match experimental data. In spectroscopy, quantum theory governs the probability of transitions between these discrete energy levels.\nTwo Paths to Quantum Mechanics: Schrödinger and Heisenberg Faced with these puzzling phenomena, physicists developed two different mathematical frameworks to describe quantum mechanics. The first was the matrix mechanics approach, formulated by Heisenberg, Born, and Jordan. The second was Schrödinger’s wave mechanics, which, despite starting from a different perspective, led to the same numerical results.\nFor the sake of clarity in our ongoing discussion of wave-particle duality, we will first explore Schrödinger’s wave equation, which builds directly on the wave-like nature of quantum systems. Heisenberg’s matrix mechanics, while equally valid, feels more like the \u0026ldquo;black magic\u0026rdquo; of theoretical physics—it’s powerful but requires more effort to follow. We’ll dive into that after setting the stage with Schrödinger\u0026rsquo;s more intuitive approach.\n( Citation: Zeng,\u0026#32;2008 Zeng,\u0026#32; J.\u0026#32; (2008). \u0026#32; 量子力学教程. \u0026#32; 科学出版社.\u0026#32;Retrieved from\u0026#32; https://books.google.co.jp/books?id=foQcPwAACAAJ ) The \u0026ldquo;Wave Equation\u0026rdquo; Approach to Quantum Mechanics: Abandoning Preconceived Notions Louis de Broglie proposed that every particle possesses a wavelength, which is related to its wave vector \\( |\\vec{k}| = 2\\pi \\lambda \\).\nFor a free particle, the energy is given by \\( E = p^{2}/2m \\). We can relate energy to angular frequency \\( \\omega \\) using \\( w=E/\\hbar \\) connect the wave vector \\( |\\vec{k}| \\) to momentum through \\( \\vec{k} = \\vec{p}/\\hbar \\). Schrödinger found inspiration in these ideas as he sought to explain wave-particle duality.\nHis journey toward the formulation of the Schrödinger equation began with a question from P. Debye:\nYou speak about waves, but where is the wave equation?\nHistorically, Schrödinger derived the wave function from the classical action principle. However, we will focus directly on the wave function ansatz to derive the Schrödinger equation.\nDeriving the Schrödinger Equation Consider a free particle with energy \\( E = \\frac{p^{2}}{2m} \\). According to de Broglie\u0026rsquo;s principle, this particle has an angular frequency \\( w = \\frac{E}{\\hbar} \\) and a wave vector \\( \\vec{k} = \\frac{\\vec{p}}{\\hbar} \\). We can suggest an ansatz for the wave function as a plane wave:\n$$ \\psi(r,t) \\sim e^{i(\\vec{k}\\cdot\\vec{r}-wt)} = e^{i(\\vec{p}\\cdot\\vec{r} -Et)/\\hbar} $$\nFrom this, we can derive:\n$$ i\\hbar \\frac{\\partial}{\\partial t}\\psi = E \\psi $$ $$ -i\\hbar \\nabla \\psi = \\vec{p}\\psi, -\\hbar^{2}\\nabla ^{2} \\psi = p^{2}\\psi $$\nFor a free particle, since \\(E = p^{2}/2m\\), we can equate these to obtain:\n$$ \\left( i\\hbar \\frac{\\partial}{\\partial t} + \\frac{\\hbar^{2}}{2m}\\nabla^{2} \\right)\\psi = \\left( E - \\frac{p^{2}}{2m} \\right)\\psi = 0$$\nThis leads to the Schrödinger equation for a free particle:\n$$ -i\\hbar \\frac{\\partial}{\\partial t} \\psi = \\frac{\\hbar^{2}}{2m}\\nabla^{2} \\psi $$\nSuperposition of Solutions The equation is linear, allowing for the superposition of plane wave solutions:\n$$ \\psi(\\vec{r}, t) = \\frac{1}{(2\\pi \\hbar)^{3/2}} \\int{\\varphi(\\vec{p}) e^{i(\\vec{p}\\cdot\\vec{r} - Et)/\\hbar} d^{3}p} $$\nSubstituting this expression confirms that it satisfies the Schrödinger equation:\n$$ i\\hbar \\frac{\\partial}{\\partial t}\\psi = \\frac{1}{(2\\pi \\hbar)^{3/2}}\\int{\\varphi(\\vec{p}) E e^{i(\\vec{p}\\cdot\\vec{r} - Et)/\\hbar} d^{3}p } $$\n$$ -\\frac{\\hbar^{2}}{2m}\\nabla^{2} \\psi = \\frac{1}{(2\\pi \\hbar)^{3/2}}\\int{\\varphi(\\vec{p}) p^{2} e^{i(\\vec{p}\\cdot\\vec{r} - Et)/\\hbar} d^{3}p } $$\nThus, we find:\n$$ \\left( i\\hbar \\frac{\\partial}{\\partial t} + \\frac{\\hbar^{2}}{2m}\\nabla^{2} \\right)\\psi = \\int{\\varphi(\\vec{p}) \\left( E - \\frac{p^{2}}{2m} \\right)e^{i(\\vec{p}\\cdot\\vec{r} - Et)/\\hbar} d^{3}p } = 0$$\nAny linear combination of wave packets will satisfy the Schrödinger equation for free particles.\nPromoting Observables to Operators In this derivation, we promote observables namely energy and momentum to operators acting on the wave function:\n$$ E \\rightarrow i\\hbar \\frac{\\partial}{\\partial t}, \\vec{p} \\rightarrow -i\\hbar \\nabla $$\nThe Case of a Particle in a Potential Field Now, let’s consider a particle in a potential field \\( V(\\vec{r}) \\). Based on the nonrelativistic relation of total energy:\n$$ E= \\frac{1}{2m}p^{2} + V(\\vec{r}) $$\nPromoting these quantities to operators yields Schrödinger\u0026rsquo;s equation:\n$$ i\\hbar \\frac{\\partial}{\\partial t}\\psi(\\vec{r}, t) = \\left[ -\\frac{\\hbar^{2}}{2m}\\nabla^{2} + V(\\vec{r})\\right] \\psi(\\vec{r}, t) $$\nImplications of the Schrödinger Equation wave-particle duality: The Schrödinger equation offers a fresh interpretation of matter and waves. In this framework, we retain properties such as mass and charge while moving away from classical trajectories—something we can never truly observe at the microscopic level. Instead, we adopt a probabilistic view of reality. Measurements on a dynamical system with observable \\(O(x)\\) are now defined probabilistically: $$ \\int_{-\\infty}^{\\infty}{\\psi^{\\ast}(x) O(x) \\psi(x)dx} $$\nHere, the statistical interpretation of the wave function leads to the normalization condition:\n$$ \\int_{-\\infty}^{\\infty}{\\psi^{\\ast}(x)\\psi(x)dx } = 1 $$\ndiscrete energy level The Schrödinger equation naturally leads to discrete energy levels due to \u0026ldquo;boundary conditions\u0026rdquo; imposed on \\( \\psi \\) when confined in an infinite potential well. The allowed energy states, or \u0026ldquo;characteristic values,\u0026rdquo; arise from the requirement that the wave function remains bounded. For example, in polar coordinates, the ordinary differential equation has singularities at \\(r=0\\) and \\(r = \\infty\\). The solutions are constrained to those that remain bounded, resulting in discrete energy levels.\nThe matrix calculus approach to quantum mechanics: only the measureables matter The gut of Heisenberg vec attempt to derive his quantum perspective is best summarized the following phrase\nDiscard all hope of observing hitherto unobservable quantities\nIn the case of the elecctron, we dispose unobservable quantities such as the position and period in the theory but leaving behind observables such as energy in stationary states together with the associated frequencies defined only upon two variables that characterized the transition\n$$ w(n, n - \\alpha) = \\frac{1}{\\hbar} { W(n) - W(n - \\alpha) } $$\nwhich shows the algebraic structure of the frequency \\(w\\) with the transition in energy level \\(W\\).\nFrom the perspective of Heisenberg, there is not thing wrong with the classical theory, it is the classical variables have to redefine to match the algebraic structure of a quantum variable\nCarrying on with this logic, Heisenberg promote kinetic variables with the observables he defined to derive the quantum equivalent of some classical theories. For example\nA simple one-dimensional model of an atom consisting of an electron undergoes periodic motion For a state characterized by the label \\( n \\), fundamental frequency \\( \\omega(n) \\) and coordinate \\( x(n, t) \\), ne can represent \\(x(n,t) \\) as a Fourier series\n$$ x(n,t) = \\sum_{\\alpha=-\\infty}^{\\infty}{X_{\\alpha}(n)exp[i\\omega(n)\\alpha t]} $$\nThen, Heisenberg asks the question: \u0026lsquo;how is the quantity \\(x(t)^{2}\\)\u0026rsquo; to be represented?\u0026rsquo;. In classical theory, it would be\n$$ [x(t)]^{2} = \\sum_{\\alpha}{\\sum_{\\gamma}{X_{\\alpha}(n)X_{\\gamma}(n)e^{i\\omega(n)(\\alpha + \\gamma)t}}} = \\sum_{\\beta}{Y_{\\beta}(n)e^{i\\omega(n)\\beta t}}$$\nIn classical theory, the frequencies simply add up. The resulting algebric structure is then\n$$ Y_{\\beta}(n) = \\sum_{\\alpha}{X_{\\alpha}(n)X_{\\beta - \\alpha}(n)} $$\n$$ \\omega(n)\\beta = \\omega(n)\\alpha + \\omega(n)(\\beta - \\alpha) $$\nNote that these quantities could not be combined in the same way in the case of quantum, one crucial different is how the frequencies would combine like\n$$ \\omega(n, n-\\alpha) + \\omega(n-\\alpha, n-\\beta) = \\omega(n, n-\\beta) $$\nHence, promoting these varibales to quantum compatibles, yield the following algebraic relation\n$$ Y(n, n-\\beta) = \\sum_{\\alpha}{X(n, n-\\alpha)X(n-\\alpha, n-\\beta)} $$\nAlso known as the Heisenberg\u0026rsquo;s law for multiplying transition amplitude together. One profound characterisitics is that these promoted quantities don\u0026rsquo;t commute. These non-commutativity define the Heisenberg algrebra that could essentially allow us to solve discrete energy eigenvalues.\nLet\u0026rsquo;s see how this commutation relation be applied define the momentum and position operator to be \\(p = i\\hbar \\nabla \\) and \\( q = x \\). Its communtation relation \\( [p, q] = pq - qp = i\\hbar \\) can be verified by acting on a wavefunction \\(\\psi(x)\\).\n$$ [p, q] = -i\\hbar \\frac{\\delta }{\\delta x}(x\\psi(x)) + i \\hbar x \\frac{\\delta }{\\delta x} {\\psi(x)} $$ $$ = -i\\hbar \\psi -i\\hbar x \\psi ^{\\prime} + i\\hbar x\\psi^{\\prime} = -i \\hbar \\psi = -i \\hbar $$\nIgonoring the planck constant \\(\\hbar\\) we yield the Heisenberg algebrba \\( [ p, q ] = i\\). We move a step forward defining an operator \\(a, a^{\\dagger}\\) from the Heisenberg algebra\n$$ a = \\frac{1}{\\sqrt{2}}(q+ip), a^{\\dagger} = \\frac{1}{\\sqrt{2}}(q-ip) $$\nThe communtation relation \\( [a, a^{\\dagger}]\\) known as Dirac algebra. Defining the Hermitian operator \\(N = aa^{\\dagger}\\). It could be diagonlized with eigenvalues of \\(n\\) and eigenvector \\(\\left| n \\right\\rangle \\).\nConsider the following communtation relation:\n$$ [a, N ] = aa^{\\dagger}a - a^{\\dagger}aa= (aa^{\\dagger} - a^{\\dagger}a)a = [a, a^{\\dagger}]a$$\nHence,\n$$ Na \\left | n \\right\\rangle = (aN - a) \\left | n \\right\\rangle = (n-1)a \\left |n \\right\\rangle$$\nThus \\(a \\left | n \\right\\rangle\\) is an eigenstate of \\( N \\) with eigenvalue equal to \\((n-1)\\).\nWrite the state \\( a\\left | n \\right\\rangle = C_{n} \\left | n-1 \\right\\rangle \\) with \\(C_{n}\\) some normalization factor. Hermitian conjuating both side yields \\( \\left\\langle n \\right| a^{\\dagger} = \\left\\langle n-1 \\right| C_{n}^{\\ast}\\). Squaring \\(a\\left| n \\right\\rangle\\)\n$$ \\left\\langle n \\right| a^{\\dagger}a \\left | n \\right\\rangle = \\left\\langle n\\right| N \\left| n \\right\\rangle = n = \\left\\langle n-1 \\right | |C_{n}^{2}| \\left | n-1 \\right\\rangle$$\nHence, the normalization factor \\(C_{n} = \\sqrt{n}\\) and the recursion relation\n$$ a \\left | n \\right\\rangle = \\sqrt{n-1} \\left| n -1 \\right\\rangle$$\nand for \\(a^{\\dagger}\\), simiarly\n$$ a^{\\dagger} \\left | n \\right\\rangle = \\sqrt{n+1} \\left| n + 1 \\right\\rangle $$\nFor \\(n\\) being a positive integer, it suggests there exists an eigenvector \\( a\\left| 1 \\right\\rangle = \\left| 0 \\right\\rangle \\) and unbounded upper eigenvector. There are two observation to be made from this creation and annihilation operators approach.\nA inifinite dimension matrix: Hilbert space The Dirac algebra can be relaized in terms of an infinite-dimensional matrix \\(A\\) with element \\(A_{n-1, n} = \\left\\langle n-1 \\right | a \\left| n \\right\\rangle \\) above the diagonal. So, does the operators \\(p\\) and \\(q\\) in the Heisenerg algebra. The operators in quantum mechanics live in the infinite dimensional space which we call it the Hilbert space.\nTop-down derivation of harmonics potential The eigenvalues of the harmonics potential is given by \\(\\frac{1}{2}(N + 1)\\). Starting from here and assuming we do not know the functional structure of the harmonics potential,\n$$ H = \\frac{1}{2}(N + 1) = \\frac{1}{2}a^{\\dagger}a + \\frac{1}{2} = \\frac{1}{2}\\left( (q-ip)(q+ip) + 1 \\right) $$ $$ H = \\frac{1}{2}(p^{2} + q^{2}) = -\\frac{1}{2} \\frac{d^{2}}{dx^{2}} + \\frac{1}{2}x^{2}$$\nPreciesely the Hamiltonian of the harmonic oscillator.\nNaturally, the next question we ask is if this method is general. Meaning that for any particular system with \\(k\\) degree of freedom, with matrix \\( q_{1},\u0026hellip;,q_{k},p_{1},\u0026hellip;,p_{k} \\) that satisfies the communitation rules. And for these matrices, we always find a matrix \\(H(q_{1},\u0026hellip;,q_{k},p_{1},\u0026hellip;,p_{k})\\) that could be diagonlized. In order to compare the Schrodinger wavefunction formalism and the matrix theory, this will be our starting point to transform these two distinct interpretation into one coherent quantum picture.\n( Citation: Aitchison,\u0026#32;MacManus \u0026amp; al.,\u0026#32;2004 Aitchison,\u0026#32; I.,\u0026#32; MacManus,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Snyder,\u0026#32; T. \u0026#32; (2004). \u0026#32;Understanding heisenberg’s ’magical’ paper of july 1925: A new look at the calculational details. ) Equivalency of these two methods Solving the eigenvalue problem\n\\(F_{z}\\) and \\(F_{\\Omega}\\) space and Hilbert space\nIn a nutshell, so far we have introduce the Schrodinger wavefunction interpretation of quantum system characterized by the Schrodinger equation\n$$ \\hat{H} \\psi(q_{1},\u0026hellip;,q_{k}, p_{1},\u0026hellip;,p_{k}) = E \\psi(q_{1},\u0026hellip;,q_{k}, p_{1},\u0026hellip;,p_{k})$$\nAnd we demonstrate the matrix method first by how Heisenberg initiate its thought on promoting the classical theory to matrix and found out that the commutation relation between observables. By an example of the communtation relation \\([p, q] = i\\) and a diagonizable matrix \\(H(p, q)\\), we were able to find out the inifinite set of eigenvalues and eigenvector that agree nicely with the wavefunction methods.\nTo show how these two methods compare to each other, we first transform the matrix method into a equivalent math problem of solving eigenvalues and eigenvectors. Subsequently we compare formalism and their math representation.\nMatrix method? eigenvalues problem in disguise! First, seek the matrices \\( q_{1},\u0026hellip;,q_{k},p_{1},\u0026hellip;,p_{k} \\) that satisfy the commutation rules. And combined to give a matrix\n$$ \\bar{H} = H(\\bar{q_{1}},\u0026hellip;,\\bar{q_{k}},\\bar{p_{1}},\u0026hellip;,\\bar{p_{k}}) $$\nwould not be a diagonal matrix. The diagonalized form could be obtained by similarity transformation\n$$ q_{i} = S^{-1}\\bar{q_{i}}S, \\quad p_{i} = S^{-1}\\bar{p_{i}}S $$\nThe communtation relation will carry over, to see this\n$$ [q, p] = S^{-1}\\bar{q}SS^{-1}\\bar{p}S - S^{-1}\\bar{p}SS^{-1}\\bar{q}S = S^{-1} [\\bar{q}, \\bar{p}] S$$\nHence, \\(\\bar{H}\\) goes over into \\( H \\) with \\(S^{-1}\\bar{H}S = H\\)\nThe only requirement from the above relation on \\(S\\) is that \\( S^{-1}\\bar{H}S \\) be a diagonal matrix where \\( \\bar{H} \\) is given.\nLet the matrix \\( \\bar{H} \\) have the elements \\(h_{\\mu \\nu}\\). the desired matrix \\(S \\) has element \\(S_{\\mu\\nu}\\), and the diagonal matrix \\(H\\) has element \\(w_{\\mu}\\), writing the matrix multiplication explicitly we got\n$$ \\sum_{\\nu}{h_{\\mu\\nu} s_{\\nu\\rho}} = w_{\\rho}\\cdot s_{\\nu\\rho} $$\nWe recognize that \\(s_{\\mu\\rho} \\) is the column vector \\( s_{1\\rho}, s_{2\\rho},\u0026hellip; \\). Hence, to specify the transformation \\(S\\) is equivalent in solving eigenvalues problem which runs as follows:\n$$ \\sum_{\\nu}{h_{\\mu\\nu}x_{\\nu}} = \\lambda \\cdot x_{\\mu} \\quad \\quad (\\mu = 1, 2, \u0026hellip;) $$\nThese set of eigenvalues and eigenvectors are essentially the only solutions. The knowledge of \\( S, H \\) determine all the solutions of the eigenvalue problem, but conversly, we can also determine \\( S, H \\) as soon as we have solved the eigenvalue problem completely.\nThe fundamental problem of the matrix theory is then the solution of the eigenvalue equation\n$$ \\sum_{nu}{h_{\\mu\\nu}x_{\\nu}} = E \\cdot x_{\\mu} \\quad \\quad (\\mu = 1, 2, \u0026hellip;) $$\nThe remaining task for us is to check that how this transformation to eigenvalues problems matches with the wavefunction formulism.\nWavefunction looking alike but not the same The defining charactersitics of the wave equation is the following\n$$ \\hat{H}\\psi(q_{1}, \u0026hellip;, q_{k}) = \\lambda \\psi(q_{1}, \u0026hellip;, q_{k}) $$\nIn which \\(H\\) is the Hamiltonian differential operator. We seek all solutions \\(\\psi(q_{1},\u0026hellip;,q_{k})\\) and \\( \\lambda \\). At first sight, this is very similar to what was required in the eigenvalue equation, which we could regard it as a function \\(x_{\\nu}\\) of the \u0026ldquo;discontinuous\u0026rdquo; variable \\(\\nu\\) which ranges over \\(1, 2,\u0026hellip;\\) corresponds to the function \\(\\psi(q_{1}, \u0026hellip;, q_{k})\\) with the \u0026ldquo;continuous\u0026rdquo; variables \\(q_{1}, \u0026hellip;, q_{k}\\), with \\(\\lambda\\) playing the same role each time.\nUpon further inspection on how these two objects transform, they do differ in a subtle way. The matrix method leads to a vector representation of the quantum state.\n$$ x_{\\mu} \\rightarrow \\sum_{\\nu}{h_{\\mu\\nu}x_{\\nu}} $$\nBut how does the wavefunction transform? We know probability is conserved in a closed quantum system and hence quantum state must undergo unitary transformation to conserve the probability.\nTo show, we first recall the norm is defined for the wavefunction\n$$ \\int{\\psi^{\\ast}(\\vec{r})\\psi(\\vec{r})d(\\vec{r})} = 1 $$\nWe expect that under transformation the norm is preserved or equivalently the total probability is conserved\n$$ \\int{\\bar{\\psi}^{\\ast}(\\vec{r})\\bar{\\psi}(\\vec{r})d\\vec{r}} = 1 $$\nLet\u0026rsquo;s define the one dimension representation of the \\(U(1)\\) be \\(e^{i\\vec{r}\\cdot\\vec{p}}\\). Then, under unitary transformation, the norm becomes\n$$ \\int{\\psi^{\\ast}(\\vec{r})e^{-i\\vec{r}\\cdot\\vec{p}}e^{i\\vec{r}\\cdot\\vec{p}}\\psi(\\vec{r})d\\vec{r}} = 1 $$\nAlso we recall that \\(\\psi(\\vec{r}) \\propto \\int{\\phi(\\vec{p})e^{i\\vec{r}\\cdot\\vec{p}}d\\vec{p}}\\). The wavefunction transform into different basis by fourier method.\nWe can indeed draw a parallel between these function. The first being vector space with well defined norm. The Schrodinger picture could also be understood as a vector with orthogonality vector and its weight. These properties are known as the Hilbert space.\n( Citation: Neumann,\u0026#32;1955 Neumann,\u0026#32; J.\u0026#32; (1955). \u0026#32; Mathematical foundations of quantum mechanics. \u0026#32; Princeton University Press.\u0026#32;Retrieved from\u0026#32; https://books.google.co.jp/books?id=JLyCo3RO4qUC ) Final Remark In the spirit of Feynnman, it will be a waste of time to put effort in making different interpretation of the quantum derivation. Nevertheless, it is still a great practice to be more thoroughly understand the historical context and mathematical motivation. Especially, when most quantum textbook often introduces these two concepts at its convience and yet never compare how these two methods differ. For it might be crucial in understanding what appraoch might be more appropriate in solving a particular quantum system.\nTo emphazie one more important aspect of such attempt to compare methods, it is that a new era of quamtum computation which heavily relies on the John Von Neumann density matrix representation of the quantum state. For this particular representation, operation of quantum mixed state and pure state is easily computed with mathematical convinence. And the ponding question of how the matrix thoery and wavefunction approach settle into one unify picture of microscopic phenomena is what drive Von Neumann to derive a representation that is irrevelent on how it\u0026rsquo;s presented.\nWe neglect some eariler motivation from Schrodinger and Heisenberg because in the development stage of a brand new theory it often took a lot of guess work to keep pushing.\nSome aspects are being abbrivated such as Schrodinger\u0026rsquo;s thought on the relevence of action principle to the formulation of quantum mechanics. But it was later developed by Dirac and Feynnman that the quantum version of the action principle could be devised starting from a notion of summing of all possible paths along the initial and final states but that\u0026rsquo;s another topic.\nRevision (2026/) Biblography Zeng (2008) Zeng,\u0026#32; J.\u0026#32; (2008). \u0026#32; 量子力学教程. \u0026#32; 科学出版社.\u0026#32;Retrieved from\u0026#32; https://books.google.co.jp/books?id=foQcPwAACAAJ Aitchison,\u0026#32; MacManus\u0026#32;\u0026amp;\u0026#32;Snyder (2004) Aitchison,\u0026#32; I.,\u0026#32; MacManus,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Snyder,\u0026#32; T. \u0026#32; (2004). \u0026#32;Understanding heisenberg’s ’magical’ paper of july 1925: A new look at the calculational details. Neumann (1955) Neumann,\u0026#32; J.\u0026#32; (1955). \u0026#32; Mathematical foundations of quantum mechanics. \u0026#32; Princeton University Press.\u0026#32;Retrieved from\u0026#32; https://books.google.co.jp/books?id=JLyCo3RO4qUC ","permalink":"https://htsod.github.io/posts/wave_particle/","summary":"The Wave and Particle Dispute The contradiction between classical physics and microscopic phenomena is one of the most fascinating episodes in the history of science, reshaping our fundamental understanding of waves and particles. To grasp the weirdness of wave-particle duality, let’s start with a simple analogy.\nImagine shooting bullets at a wall with two equally spaced gaps. As expected, the bullets passing through each gap will behave independently, forming two distinct patterns on a measurement panel behind the wall.","title":"Wave Equation Approach and Matrix Method Derivation of the Schrodinger Equation"},{"content":"Can the order of physics discovery be differernt? From Zee Einstein\u0026rsquo;s Gravity in a nutshell, a hypothetical situation is proposed: a far away civilization where life was evolved from modular planet happen to understand the constancy of light before understanding electromagnetism. A brilliant physicist then could derived E.M. and gravity from the correct conception of space and time.\nThe derivation was casted in a simplisitc manner: By placing the potential term either inside or outside the action square root, we obtain the familiar interactions of gravity or electromagnetism, with major concepts left unexplained. This blog will follow closely to the derivation of the book, with more elaborated reasoning to each steps and assumption made.\nWhile reading the book, two questions are worthy of pursuing\nwhy outside corresponding to EM and inside corresponding to gravity?\nIn derivation of Maxwell Equations, how does addition of a spatially dependent potential term to the action relates to the properties of equation of motion?\nHopfully by answering these two question in this blog, the bottom-up derivation of Electromagnetism and Gravitation would gain more clarity and fun.\nRelativistic Action The action principle states that the condition of extremizing the action, which is defined by a integrating a functional along the particle trajectories, the mechanical variables that statisfy such conditions follow the equation of motion.\n$$ S_{c} = \\int^{b}_{a}{L(q, \\dot{q}, t) dt} \\tag{1}$$\nwhere \\(L\\) is defined as the Lagrangian. And the Euler-Lagrange equation solves the extremizing condtions of the action:\n$$ \\frac{d}{dt} \\frac{\\partial L}{\\partial \\dot{q}} = \\frac{\\partial L}{\\partial q} \\tag{2}$$\nGalilean and Lorentz Transformation The equation of motion must not depend on the observers; observers from different inertial frame of reference must observe the same physical phenomena. This restricts the form of the Lagrangian.\nIt is well-known that classical mechanics is built upon Galilean transformation, that under Galilean transformation the equation of motion remains essentially the same. The Galilean transformation is defined as:\n$$ x = x^{\\prime} + Vt \\tag{3}$$ $$ t = t^{\\prime} \\tag{4}$$\nThe same goes with the relativistic action principle, except that the relativistic mechanics now permits Lorentz transformation to incorporate the contancy of speed of light. The Lorentz transformation is given as:\n$$ x = \\frac{x^{\\prime} + \\frac{V}{c}ct^{\\prime}}{\\sqrt{1- \\frac{V^{2}}{c^{2}}}} \\tag{5}$$ $$ ct = \\frac{\\frac{V}{c}x^{\\prime} + ct^{\\prime}}{\\sqrt{1- \\frac{V^{2}}{c^{2}}}} \\tag{6}$$\nUnder Lorentz trnsformation, the spacetime interval will stay the same. One could verify by substitution to the following:\n$$ ds^{2} = dx^{2} + dt^{2} = dx^{\\prime2} + dt^{\\prime2} = ds^{\\prime2} \\tag{7}$$\nLagrangian for a free particle Then, the minimizing of the action would be invariant under the transformation stated above. For example, in classical mechanics, the Lagrangian of the free particle is expressed as:\n$$ L = \\frac{1}{2}m v^{2} \\tag{8}$$\nWith Galilean transformation to another inertia frame:\n$$ L^{\\prime} = L((v+V)^{2}) = \\frac{1}{2}m(v^{2} + 2v\\cdot V +V^{2}) \\tag{9}$$ with \\(V\\) being a constant velocity relating the two inertia frames. The above transformed Lagrangian can written as the original Lagrangian with a total derivative term - a term that vanishes with extremizing the action: $$ L^{\\prime} = \\frac{1}{2}mv^{2} + \\frac{d(mr\\cdot V + \\frac{1}{2}mV^{2}t)}{dt} \\tag{10}$$\nHence the classical form of Lagranigian \\(L = \\frac{1}{2}mv^{2}\\) results in the same equation of motion with Galilean transformation. (cite landau mechanics) ( Citation: Landau\u0026#32;\u0026amp;\u0026#32;Lifshitz,\u0026#32;1976 Landau,\u0026#32; L.\u0026#32;\u0026amp;\u0026#32;Lifshitz,\u0026#32; E.\u0026#32; (1976). \u0026#32; Mechanics (3). \u0026#32; Butterworth–Heinemann. ) The relativistic Lagrangian must be leading to the invariant of equation of motion under Lorentz transformation. For example, the spacetime interval \\(ds\\): $$ S_{rel} = -\\alpha \\int^{b}_{a}{ds} \\tag{11}$$ where \\(\\alpha\\) is a constant to be determined.\nFor a given spacetime interval, we could always find an inertia frame that the spatial position between the two events remain the same. Then, we can write \\(ds^{2} = c^{2}d\\tau^{2}\\) where \\(\\tau\\) is defined as the proper time, the time interval in which two events happen in the same spatial position. And the action becomes:\n$$ S_{rel} = -\\alpha \\int^{b}_{a}c{d\\tau} \\tag{12}$$\nFrom the Lorentz transformation, \\(dt = \\frac{ d\\tau^{\\prime}}{\\sqrt{1- \\frac{V^{2}}{c^{2}}}}\\), which gives the action of the form:\n$$ S_{rel} = -\\alpha \\int^{b}_{a}c{dt\\sqrt{1- \\frac{v^{2}}{c^{2}}}} \\tag{13}$$\nIn the classical limit, the relativistic Lagrangian must recover the form of the classical Lagrangian \\(\\frac{1}{2}m v^{2}\\) in the coefficient of \\(v^{2}\\). By expanding the terms of the intergrand: $$ \\sqrt{1- \\frac{v^{2}}{c^{2}}} \\approx\\alpha c- \\alpha c \\frac{1}{2} \\frac{v^{2}}{c^{2}} \\tag{14}$$ Hence, \\(\\alpha = mc\\).\nUsing the four-vector representation to express the relativistic Lagrangian,\\(ds = \\sqrt{\\eta_{i j} dx^{i} dx^{j}}\\), where \\(\\eta_{i j}\\) is the flat spacetime metric \\((-1, 1, 1, 1)\\) with the Einstein summation convention, the relativistic action could now be written in an elagant form:\n$$ S_{rel} = -mc^{2} \\int_{a}^{b}{ds} = -mc^{2} \\int^{b}_{a}{ \\sqrt{\\eta _{ij} dx^{i}dx^{j}}} \\tag{15}$$\nInteraction with the Fields In Zee\u0026rsquo;s book of gravity, a potential term is added either inside or outside the square root of the relativistic Lagrangian to take interaction into consideration. Stating the argument from the book:\nOutside the square root $$ S = -\\int{[-m\\sqrt{-\\eta_{i j} dx^{i} dx^{j} } + V(x)dt]} \\tag{16}$$ By promoting the scalar potential to the vector potential and completing the symmetry, we get:\n$$ S = -\\int{ [-m \\sqrt{-\\eta_{i j} dx^{i} dx^{j} } + A_{i}(x)dx^{i}] } \\tag{17}$$\nInside the square root $$ S = -\\int{ -m \\sqrt{\\left(1+\\frac{2V}{m}\\right)dt^{2} - d\\vec{x}^{2}}} \\tag{18}$$ Here, symmetry and promotion to Lorentz invariance give us the curved spacetime version:\n$$ S = -m \\int{ \\sqrt{-g_{i j} dx^{i} dx^{j}}} \\tag{19}$$\nBut this is rather a bold guess. How do we know there are exactly two interaction in nature that corresponding to that two terms: one embodies through the metric \\(g_{ij}\\), one embodies as a four vector integration \\(\\int{A_{i}(x)dx^{i}}\\). And how can we tell which one is Electromagnetism and which one is Gravity?\nRelativistic view of particle and field Suppose a rotating rigid body, and we assigned reference frames along the radius vector from the center of the rotation. They would agree on the radial distance measurement because the velocity is tangential to the radial vector. However, they would evalue different circumference as compare to the rest frame and hence different circumference to radius ratio to \\(2 \\pi\\), which is contradicting to the assumption that difference reference frame must observe the same physics. In reality, the rigid body must compress and stretch in a complex way to account for the relativistic effect, hence not any more rigid. We arrive at the conclusion that particle in the relativistic sense must not have any spatial extension but only a point coordinate in the space. And the particle is affected by the present of other particles in the form of fields prevail through the space. In this sense, we are treating field and particles as separate identities.\nElectromagnetic field So, the field interaction as an separate term of the action, which also has to be invariant under Lorentz transform and be a scaler, with the infinitesimal element being a four-vector, the integrand has to be another four-vector to contract forming a scaler, which gives the interaction term outside of the square root: $$ S = -\\int{ [-m \\sqrt{-\\eta_{i j} dx^{i} dx^{j} } + A_{i}(x)dx^{i}] } \\tag{20}$$\nWe know this does not correspond to gravity. Because from classical mechanics, equation of motion under gravity is affected by both the mass that generate the field and the mass inside the field. But where else can we place the potential term? Aha! Inside the square root where there is coefficient of mass.\nHowever, without knowing the properties of gravity, that its strength is related to the mass of both entities, it will be a great mental leap from the classical action to the general relativistic action. To see this, we compare the Electromagnetism action with the classical action, \\( S = -\\int{( \\frac{1}{2}m v^{2} - V(x))dt}\\), they are strikingly similar in that they both have a term that corresponding to free particle which leaves the equation of motion unchanged when under the transforms of their respective kind. And they also contain a spatially dependent potential term in the action, but in the classical case, the effects of field never present in the term of free particle. Therefore, it will be a great leap from including a potential term in side the squre root.\nGravity as inertial frame From a historical point of view, it was Einstein who made the observation that gravity is indistinguishable for the observers under acceleration to observers being in a non-inertial frame, which suggests that in near approximity, the mathematics structure of force under gravity must be the same as observer in a non-inertial frame. This is known as the principle of equivalence.\nThen how the non-inertial frame corresponding to addition of potential term inside of the square root.\n$$ ds^{2} = c^{2}dt^{2} - dx^{2} -dy^{2} -dz^{2}$$\nSay for example, we are transforming to a uniformly rotating frame with angular velocity of \\(\\Omega\\), the length element in the Minikowski space might not be in the same form of that in the flat spacetime. Instead $$ x = x^{\\prime} \\cos{\\Omega t} - y^{\\prime} \\sin{\\Omega t}$$ $$ y = x^{\\prime} \\sin{\\Omega t} + y ^{\\prime} \\cos{\\Omega t}$$ So in general, a non-inertial frame could be encoded in the functional form of the metric. $$ ds^{2} = [c^{2} - \\Omega (x^{\\prime 2} + y^{\\prime 2})]dt^{2} - dx^{\\prime 2} - dy^{\\prime 2} - dz^{\\prime 2} + 2\\Omega y^{\\prime} dx^{\\prime}dt - 2\\Omega x^{\\prime }dy^{\\prime} dt $$\n$$ ds^{2} = g_{i j}dx^{i}dx^{j} $$\nJustifying the addition of potential term inside the square roots.\nEM force preserve the structure of the classical view of force hence as adding a potential term outside of the square root. Whereas gravity took in the form of a non-inertial frame hence inside the square root. That answers the first question: why outside corresponding to EM and inside corresponding to gravity?\nLorentz Force Equation in Vector Form From the action principle, we extremize the action. The only equation satisfies the extremizing condition when the variation is arbitary is the equation of motion with the given Lagrangian. In classical mechanics, the Euler-Lagrangian method leads to the Newton\u0026rsquo;s equation of motion. Now we working with the following relativistic action:\n$$ S = -\\int{ [-m c\\sqrt{\\eta_{i j} dx^{i} dx^{j} } + \\frac{e}{c}A_{i}(x)dx^{i}] } $$\nConsider case for flat time metric, we have the following simplification \\( \\eta_{i j} dx^{i} dx^{j} = dx_{j} dx^{j} \\)\nLorentz force equation in tensor form Extremizing the relativistic action:\n$$ \\delta S = -\\delta \\int{ [m c^{2}\\sqrt{ dx_{j} dx^{j} } + \\frac{e}{c}A_{i}(x)dx^{i}] } $$\nSince we have \\( d\\tau = dx_{j} dx^{j} \\), the first term becomes\n$$ \\delta \\int{ m c \\sqrt{ dx_{j} dx^{j} }} = \\int{ mc \\frac{dx_{j} d \\delta x^{j}}{d\\tau} } $$\nsince \\(\\frac{dx_{j}}{d\\tau} = u_{j}\\), where \\(u_{j}\\) is the compoenent of the four velocity, we have the following:\n$$ mc\\int{ u_{j} d \\delta x^{j}} = mc\\left(u_{j}x^{j}\\right)| + mc\\int{ du_{j} \\delta x^{j}} $$\nExtremizing the the second term:\n$$ \\delta \\int{ A_{i}(x) dx^{i}} = \\int{d\\tau \\left[A_{i}\\frac{d\\delta x^{i}}{d\\tau} + \\delta_{j}A_{i}\\delta x^{j} \\frac{dx^{i}}{d\\tau}\\right]} $$\nUsing by parts to the first term to find the common terms of the above intergrand:\n$$ \\int{d\\tau A_{i}(x)\\frac{d\\delta x^{i}}{d\\tau}} = -\\int{d\\tau \\frac{dA_{i}(x)}{d\\tau}\\delta x^{i}} = -\\int{d\\tau \\delta_{j} A_{i}(x) \\frac{dx^{j}}{d\\tau} \\delta x^{i}}$$\nSince the repeated indexes are being sum together, interchanging \\(i\\) and \\(j\\) leads to the following:\n$$ \\delta \\int{d\\tau A_{i}(x) \\frac{dx^{i}}{d\\tau}} = \\int{d\\tau \\left( \\delta_{i}A_{j} - \\delta_{j}A_{i} \\right) \\frac{dx^{j}}{d\\tau} \\delta x^{i}} $$\nDefining the antisymmetric tensor field\n$$ F_{i j}(x) \\equiv \\delta_{i}A_{j}(x) - \\delta_{j}A_{i}(x) $$\nThus, the variation of the action becomes:\n$$ \\delta S = \\int{d\\tau \\left( mc \\frac{du_{j}}{d\\tau} \\delta x^{j} + \\frac{e}{c}F_{i j}\\frac{dx^{j}}{d\\tau} \\delta x^{i} \\right)} $$\nThis results in the Lorentz force law in tensor form:\n$$ mc\\frac{du^{j}}{d\\tau} = \\frac{e}{c}F^{ji} u_{i} $$\nDefinition of the field from the equation of motion In an attempt to express the force law into the vector form, we express the four-vector potential \\(A_{i}\\) in terms of two parts, which include a time piece and a spatial piece: \\(A_{i} = [\\phi_{0}(\\vec{x}, t), \\vec{A}(\\vec{x}, t)]\\). With this notation, we can expand the Lorentz force law along the x axis:\n$$ \\frac{dp_{x}}{dt} = \\frac{e}{c} \\left[ (\\delta_{x} A_{y} - \\delta_{y}A_{x})v_{y}+ (\\delta_{x}A_{z} - \\delta_{z}A_{x} ) v_{z} + (\\delta_{x}\\phi_{0} - \\delta_{0}A_{x}) \\right] $$\n$$ \\frac{dp_{x}}{dt} = \\frac{e}{c} \\left[ (\\nabla \\times A)\\vert_{x} v_{y} -(\\nabla \\times A)\\vert_{y} v_{z} + (\\nabla \\phi_{0})\\vert_{x} - \\frac{dA_{x}}{dt} \\right] $$\nWriting out the Lorentz force law in vector form:\n$$ \\frac{d \\vec{p}}{dt} = - \\frac{e}{c} \\frac{\\partial \\vec{A}}{\\partial t} - e \\nabla{\\phi_{0}} + \\frac{e}{c} \\vec{v} \\times (\\nabla \\times \\vec{A}) $$\nThey split into terms with explicit dependent on \\(v\\) and terms without explicit denpendent on \\(v\\), which motivates the definition of the following vector field:\n$$ \\vec{E} = \\frac{1}{c} \\frac{\\partial \\vec{A}}{\\partial t} + \\nabla{\\phi_{0}} $$\n$$ \\vec{B} = \\frac{1}{c}(\\nabla \\times \\vec{A}) $$\nwhere \\(\\vec{E}\\) is defined to be the electric field and \\(\\vec{B}\\) is defined to be the magnetic field. Expresssing the Lorentz force in terms of the fields:\n$$ \\vec{F} = -e\\vec{E} + e\\vec{v}\\times\\vec{B}$$\nThe Lorentz force law tells us that the force a charge will experience under the present of electromagnetic field. The force will be directed in the direction of the electric field regardless of velocity of the charge. The magnetic field will act in the direction perpendicular to both the velocity and the magnetic field, and it is propotional to the magnitude of the velocity.\nBut this does not tell the whole story of electromagnetism. How do the fields arise in the first place?\nThe Complete Action of the Electromagnetic Field with Moving Charges Field is a physical quantity. If the Lorentz force law holds, then one can measure the fields from the motion of the charge and it will be uniquely determined. That suggests that the E.M. action we have earlier, which only includes the term of free particle and the field particle interaction, must also include the action of the field only.\n$$ S = S_{f} + S_{m} + S_{mf} $$\nwhere \\(S_{m}\\) and \\(S_{mf}\\) are the action of the free particle and action of interaction resepectly. The \\(S_{f}\\) is the action of the field and is the objective of this section to search for its form.\nTerms with Field $$ S = -\\sum{mc\\int{ds}} + -\\sum{\\frac{e}{c} \\int{A_{k} dx^{k}}} + S_{f}$$\nSome properties of the fields:\nUniquely determined Follows superposition From the first condition, the field Lagrangian must not involve explictly the potential terms since they are subject to the Gauge degree of freedom and is not unique. But at the same time, the field is determined from the potential hence must involve the potential terms implicitly. One suitable candidiate is the antisymmetric tensor field \\(F_{ij} = \\delta_{i}A_{j} - \\delta_{j}A_{i}\\). It is a \\(4 \\times 4\\) matrix, where each term is the E.M field pointing at \\(x, y, z\\) direction. Assume\n$$ F_{0j} = (E_{x}, E_{y}, E_{z})$$\n$$ F_{ij}\\vert_{ij\\neq 0} = (B_{x}, B_{y}, B_{z})$$\nFrom the experimental facts of the field, the field must follow from the superposition of the fields. That suggests the field produced by a collecton of charges are the summation of the fields from each individual along. This is equivalent to say that summation of fields also satisfies the equation of motion of a single field. We know that a linear differential equation satisfy this criteria hence the Lagrangian of the field equation must be quadratic in nature. Additionally, the field must be uniquely determine since it connects directly to the force, therefore it must contain the field tensor but not the potential. Lastly, the Lagrangian must be a scaler that is invariant under Lorentz transformation. That leaves us the choice:\n$$ S_{f} = \\frac{-1}{16\\pi c}\\int{F_{ik}F^{ik}dVdt} $$\nSo, the action of the elctromagnetic field can be expressed as:\n$$ S = -\\sum{mc\\int{ds}} + -\\sum{\\frac{e}{c} \\int{A_{k} dx^{k}}} + \\frac{-1}{16\\pi c}\\int{F_{ik}F^{ik}dVdt} $$\nContinuous charge distribution The motion of charge or motion of collective charges are more conviently express in density term rather than a summation.\nLet the charge density defined by \\(\\rho = \\sum_{a}{\\rho_{a} \\delta (r- r_{a}) }\\)\n$$ de dx^{i} = \\rho dV dx^{i} = \\rho dV dt \\frac{dx^{i}}{dt} = j^{i} dV dt$$\nwhere we define the four current quantity \\(j^{i} = (c\\rho, \\rho v)\\)\nRewriting the action:\n$$ S = -\\sum{mc\\int{ds}} + -\\frac{1}{c} \\int{\\rho A_{k} dVdt \\frac{dx^{k}}{dt}} + \\frac{-1}{16\\pi c}\\int{F_{ik}F^{ik}dVdt}$$\n$$ S = -\\sum{mc\\int{ds}} + -\\frac{1}{c} \\int{j^{j} A_{k} d\\Omega} + \\frac{-1}{16\\pi c}\\int{F_{ik}F^{ik}d\\Omega}$$\nMaxwell\u0026rsquo;s Equations in Hindsight With the complete form of the action in hands, we are only one step away from the four Maxwell equations.\nVariation of the action:\n$$ \\delta S = -\\int{\\frac{1}{c}\\left[ \\frac{1}{c} j^{i}\\delta A_{i} + \\frac{1}{8\\pi} F^{ik}\\delta F_{ik} \\right] d\\Omega} = 0 $$\nWrite \\(F_{ik} = \\frac{\\partial A_{k}}{\\partial x^{i}} - \\frac{\\partial A_{i}}{\\partial x^{k}}\\) and interchanging the index \\(F_{ik} = -F_{ki}\\)\n$$ \\delta S = - \\int{\\frac{1}{c} \\left[ \\frac{1}{c} j^{i}\\delta A_{i} - \\frac{1}{4\\pi} F^{ik} \\frac{\\partial}{\\partial x^{k}} \\delta A_{i} \\right] d\\Omega}$$\nSimplify using integration by parts. The surface term vanishes:\n$$ \\delta S = - \\int{\\frac{1}{c} \\left[ \\frac{1}{c} j^{i} - \\frac{1}{4\\pi} \\frac{\\partial}{\\partial x^{k}} F^{ik} \\right]\\delta A_{i} d\\Omega} - \\frac{1}{4\\pi c} \\int{F^{ik} \\delta A_{i} dS_{k}} = 0$$\nThe equation of motion of collective charges and current \\(j^{i} = (c\\rho , \\rho v)\\):\n$$ \\frac{1}{c}j^{i} - \\frac{1}{4\\pi}\\frac{\\partial F^{ik}}{\\partial x^{k}} = 0$$\nwhich leads to the equation of motion with the present of four current:\n$$ \\frac{\\partial F^{ik}}{\\partial x^{k}} = -\\frac{4\\pi}{c} j^{i}$$\nFirst pair of Maxwell equations The first pair of Maxwell\u0026rsquo;s equations can be from vector operation on the fields definition. With\n$$ E = \\frac{1}{c} \\frac{\\partial A}{\\partial t} + \\nabla \\phi_{0} $$\n$$ B = \\nabla \\times A$$\nDivergence of \\(B\\) vanishes because divergence of any curl vanishes.\n$$ \\nabla \\cdot B = \\nabla \\cdot (\\nabla \\times A) = 0$$\nCurl of \\(E\\) gives the following because curl of any gradient vanishes.\n$$ \\nabla \\times \\vec{E} = - \\frac{1}{c} \\frac{\\partial \\vec{B}}{\\partial t}$$\nIn integral forms:\n$$ \\int_{A}{(\\nabla \\times \\vec{E}) \\cdot d\\vec{a}} = \\oint_{S}{\\vec{E} \\cdot d\\vec{r}} = -\\frac{1}{c}\\frac{\\partial}{\\partial t} \\int_{A}{\\vec{B} \\cdot d\\vec{a}} $$\n$$ \\int_{V}{(\\nabla \\cdot \\vec{B}) d v} = \\oint_{A}{\\vec{B} \\cdot d\\vec{a}} = 0 $$\nSecond pair of Maxwell equations By Helmhotz\u0026rsquo;s theorem, well-behaved vector fields can be uniquely determined by its divergence and curl. But it is not the case when taking divergence of \\(E\\) and curl of \\(B\\) because the potential does not simply vanishes in this case. For examples:\n$$ \\nabla \\cdot \\vec{E} = \\nabla (\\nabla \\phi_{0}) = \\nabla^{2}\\phi_{0}$$\n$$ \\nabla \\times \\vec{B} = \\nabla \\times (\\nabla \\times \\vec{A})$$\nAddition information is needed to remove the potential dependent, which brings us to the equation of motion in the present of four current:\n$$ \\frac{\\partial F^{ik}}{\\partial x^{k}} = -\\frac{4\\pi}{c} j^{i}$$\nWe inumerate all possibilities:\n$$ \\frac{\\partial F^{0k}}{\\partial x^{k}} = \\nabla \\cdot \\vec{E} = -\\frac{4\\pi}{c} c\\rho = -4\\pi \\rho$$\n$$ \\frac{\\partial F^{i0}}{\\partial x^{0}} = \\frac{\\partial \\vec{E}}{\\partial t} = - \\frac{4\\pi}{c} \\vec{j} $$\n$$ \\left. \\frac{\\partial F^{ij}}{\\partial x^{j}} \\right\\vert_{ij\\neq 0} = \\nabla \\times \\vec{B} = -\\frac{4\\pi}{c} \\vec{j} $$\nCombining the last two equations:\n$$ \\nabla \\times \\vec{B} = \\frac{1}{c}\\vec{j} + \\frac{1}{c} \\frac{\\partial \\vec{E}}{\\partial t} $$\n$$ \\nabla \\cdot \\vec{E} = \\frac{\\rho}{\\epsilon_{0}}$$\nRevision (2026/01/04 ~ 2026/02/11) Topics added:\nThe effects of the transformation on the action principle - on how they lead to different equation of motion, both classically and relativistically\nJustify why force of gravity must have its potential in the square root.\nDefine electromagnetic field from the Lorentz law\nIntroduce action for field. Action for sum of charges and current. Defining four current.\nStill need:\nNon-uniqueness of potentials\nExplaination on the antisymmetry tensor \\(F_{ij}\\)\nUniquess of field. Helmotze theorem.\nDifferent between the first pair and second pair of Maxwell\u0026rsquo;s equations\nImplication of this approach\nBibliography Zee (2013) Zee,\u0026#32; A.\u0026#32; (2013). \u0026#32; Einstein gravity in a nutshell. \u0026#32; Princeton University Press. Landau\u0026#32;\u0026amp;\u0026#32;Lifshitz (1976) Landau,\u0026#32; L.\u0026#32;\u0026amp;\u0026#32;Lifshitz,\u0026#32; E.\u0026#32; (1976). \u0026#32; Mechanics (3). \u0026#32; Butterworth–Heinemann. Landau\u0026#32;\u0026amp;\u0026#32;Lifshitz (1975) Landau,\u0026#32; L.\u0026#32;\u0026amp;\u0026#32;Lifshitz,\u0026#32; E.\u0026#32; (1975). \u0026#32; The classical theory of fields (4). \u0026#32; Butterworth–Heinemann. Arfken,\u0026#32; Weber\u0026#32;\u0026amp;\u0026#32;Harris (2012) Arfken,\u0026#32; G.,\u0026#32; Weber,\u0026#32; H.\u0026#32;\u0026amp;\u0026#32;Harris,\u0026#32; F.\u0026#32; (2012). \u0026#32; Mathematical methods for physicists (7). \u0026#32; Academic Press. ","permalink":"https://htsod.github.io/posts/maxwell_eqns_derive/","summary":"Can the order of physics discovery be differernt? From Zee Einstein\u0026rsquo;s Gravity in a nutshell, a hypothetical situation is proposed: a far away civilization where life was evolved from modular planet happen to understand the constancy of light before understanding electromagnetism. A brilliant physicist then could derived E.M. and gravity from the correct conception of space and time.\nThe derivation was casted in a simplisitc manner: By placing the potential term either inside or outside the action square root, we obtain the familiar interactions of gravity or electromagnetism, with major concepts left unexplained.","title":"Bottom-up derivation of Maxwell's Equations"},{"content":"Passive components—resistors, capacitors, and inductors—look deceptively simple on a schematic. Yet every analog circuit is built on the way these three elements enforce conservation laws and store or dissipate energy. This post takes a physics-forward route into passives: starting from charge flow and electromagnetic work, we motivate Kirchhoff’s current and voltage laws, connect them to Ohm’s law, and build intuition for why “voltage drops” and “current continuity” are more than rules-of-thumb.\nFrom there we move into what makes circuits dynamic. Capacitors and inductors introduce energy stored in electric and magnetic fields—so even the simplest RC or LC connection becomes a differential equation. We’ll use that ODE viewpoint to explain transient behavior (charging, discharging, damping), then switch perspectives to the frequency domain where the same circuits become low-pass filters, high-pass filters, and resonators. Along the way we’ll interpret time constants, \\(3 dB \\) points, phase shifts, and Q factor as different faces of the same underlying physics.\nBy the end, you should be able to look at a passive network and predict—without memorizing formulas—what it will do to a signal in time and in frequency, and why.\n1. Resistor Circuit Current law In electronics, we are dealing with the flow of a fundmanetal particle known as electron. These electrons carry a physical quantity measured in \\(C\\) Coulomb. Charges flowing through a area per unit time is defined as current \\(I = \\frac{dQ}{dt} = \\int{J da} \\) measured in \\(A\\) Ampere, where \\(J\\) is the current density and \\(da\\) is the infinitesmal area.\nThe flow of current must follow the Equation of Continuity, which states that the net amount of charge flowing from a given volume must lead to the change of charge density in that volume:\n$$ \\oint{J da} = \\int{\\frac{\\partial \\rho}{\\partial t} dV} $$\nwhich is equation of continuity in integral form. Applying Stokes\u0026rsquo; theorem, we obtain the equation of continuity in differential form:\n$$ \\int{\\nabla \\cdot J dV} = \\int{\\frac{\\partial \\rho}{\\partial t} dV} $$\n$$ \\nabla \\cdot J = \\frac{\\partial \\rho}{\\partial t} $$\nWhen designing the system, we wish the state of the system to be time-independent \\(\\partial \\rho / \\partial t = 0\\). Hence, in the transient state, the net flow of current is zero:\n$$ \\nabla \\cdot I = \\int{\\nabla \\cdot J da } = 0 $$\nIn a circuitry, we working with network of components with inter-connecting nodes. Applying this to each node, then the net-flow of current into and out of the node must be zero.\n$$ \\sum_{i}{I} = 0$$\nwhich is known as the Kichroff\u0026rsquo;s current law.\nVoltage law The motion of charge is determined by the electric field and the magnetic field. And it is given from the Lorent\u0026rsquo;s law of motion:\n$$ F = q(E + v \\times B)$$\nThe electric fields \\(E\\) and magnetic field \\(B\\) in terms of scalar potential \\(V\\) and vector potential \\(A\\):\n$$ E = \\frac{1}{c}\\frac{\\partial A}{\\partial t} - \\nabla V $$\n$$ B = \\nabla \\times A $$\nThe work done by the field from point b to point a:\n$$ W_{ba}=\\int_{b}^{a}{F \\cdot dr} = q\\left(\\int_{b}^{a}{E\\cdot dr} + \\int_{b}^{a}{(v\\times B)\\cdot dr}\\right)$$\nSince the second term on the right hand side is perpendicular to the path traveled, the dot product vanishes. And substitute \\(E\\) into the equation:\n$$ W_{ba} = -qV_{ba} + \\frac{q}{c}\\int_{b}^{a}{\\frac{\\partial A}{\\partial t}} \\cdot dr $$\nwhich represents the work done on the charge by the electric field. The first term is straightforward: a charge gain energy from the potential difference \\(V_{ba}\\) from \\(a\\) to \\(b\\). The second term is related to the change of magnetic flux \\(\\Phi = B \\cdot A\\). To see this, first assume it is a closed-loop integral \\(a = b\\) which the potential difference \\(V\\) vanishes.\n$$ W_{closed} = \\frac{q}{c} \\frac{\\partial }{\\partial t}\\oint{ A \\cdot dr} = \\frac{q}{c} \\frac{\\partial }{\\partial t}\\int{ \\nabla \\times A \\cdot da}$$\n$$ W_{closed} = \\frac{q}{c} \\frac{\\partial \\Phi}{\\partial t} $$\nHence, a charge also gains energy moving in a closed loop from a change of magnetic flux. The contribution from the magnetic field is significantly smaller because it is scaled down by the speed of light \\(c = 3\\times 10^{8} ms^{-1}\\).\nIn static magnetic field, where there is no change of flux, the work done is now simplified to:\n$$ W_{ba} = -qV_{ba} = -q(V_{b} - V_{a}) $$\nWhen that applies to circuitry, we can identify two nodes in the circuit, then the work done on the charge will be characterized by the voltage difference across these components. Furthermore, if we studying a circuit of a closed loop:\n$$ \\sum_{i}{V_{i}} = 0$$\nwhich states that the net voltage-drop around a circuit loop must be zero.\nRelation between voltage and current To characterize how electronics modify the signal, we study how does the energy carried by the charge being modified across the compoenent. This can be done by taking time derivative on the work done by the field from point \\(a\\) to point \\(b\\).\n$$ \\frac{d W_{ab}}{dt} = \\frac{d}{dt}(qV_{ab}) $$\nThe potential difference \\(V_{ab}\\) is determined by the spatial distribution of charges within the region. In designing a stable system, we are generally interested in the transient behavior of the system: the system\u0026rsquo;s microscopic interaction happen in a much smaller time-scale that the system\u0026rsquo;s observables will be stablized immediately. In this case, the charges will equilibrate so fast that their configuration will become stable immediately and leading to a constant voltage difference across the component.\n$$ \\frac{dW_{ab}}{dt} = \\frac{dq}{dt}V_{ab} = I V_{ab}$$\nwhich says that the if a current is flowing through a potential difference of \\(V_{ba}\\), there is a dissipation of energy across the component. Or in other words, a electronics component modifies the signal by changing the energy carried by the charge. And this can be measured from the current flow, the voltage difference and the relation between the two.\nConductor A conductor, in a simple definition, is a type of material that allows for free movement of electrons. It contains a \u0026ldquo;sea\u0026rdquo; of electrons that constantly interacting and react instantly to the external field, such that it equilibrate to a stable configuration at a time-scale of \\(t\u0026lt; 10^{-12}\\). We can then make the assumption that there is no motion of charge within the conductor and hence no electric field within the conductor:\n$$ E_{inside} = 0 $$\nFrom the Maxwell\u0026rsquo;s equation we have:\n$$ \\int_{V}{(\\nabla \\cdot E_{inside})dV}= \\int_{V}{\\frac{\\rho}{ \\epsilon_{0}}dV} = \\frac{Q_{enclosed}}{\\epsilon_{0}} = 0 $$\nThe charge does not live inside the conductor but it resides on the surface of the conductor.\nIf we do a closed line-integral just right above the surface, so that the perpendicular component approach zero, we have:\n$$ \\oint_{P}{E\\cdot dr} = E_{||} d + E_{inside} d = 0$$\nwhere \\(E_{||}\\) represents the parallel Electric field just above the surface. Writing it as potential connecting one point to another point of the conductor:\n$$ \\int_{a}^{b}{ E_{||} \\cdot dr} = -(V_{b} - V_{a}) = 0 $$\nHere we arrived at a important result of conductor: the conductor surface is at a equal potential. From our previous discussion, the change of energy across a component is characterized by product of the voltage difference and current. Hence, for current flowing through a conductor, there is no change of energy which preserves the integrity of the signal.\nOhm\u0026rsquo;s law In non-conductive materials, they usually do not have the sea of electron to provide the charge mobility. The flowing electrons might bump into the material\u0026rsquo;s atoms and create friction that dissipates energy while transporting. Ohm\u0026rsquo;s states that eventually it will reach a transient state where a potential difference create a constant flow of electron, meaning the current is proportional to the potential differences:\n$$ V \\propto I $$\nThe proportionality constant is called resistance \\(R\\) measued in ohms \\(\\Omega \\). This gives Ohm\u0026rsquo;s law:\n$$ V = IR $$\nThe power dissipation across a components for a Ohmic material is then:\n$$ P = VI = I^{2}R = \\frac{V^{2}}{R}$$\nThevenin\u0026rsquo;s theorem Suppose we have two system blocks, each with its input and output. When we connect the output of the first system to the input of the second system, both systems interact and the signal and power transfer changes accordingly.\nThevenin\u0026rsquo;s theorem states that any two system blocks of resistors could be simplified as a source voltage, source resistance and load resistance. This generalizes to any single input and single output resistor network.\nTo find the Thevenin equivalent voltage and Thevenin equivalent resistance can be calculated as such:\nIf nothing is connected to the output, then \\(V_{out} = V_{th}\\)\n(a) If the output is short-circuited, then \\(V_{out} = 0\\) and \\(V_{th} = R_{th} I_{short}\\)\nAlternatively, Thevenin equivalent resistance can also be computed as such:\n(b) Replace all EMF by short, and computes the equivalent resistance at the output. Potential Divider: Voltage signal and Power distribution Considerations A simpliest example would be a potential divider. If two resistors are treated as two separate systems that connect in series, the potential difference distribute to each of the resistor by the ratio of the resistance.\nA potential divider circuit\nTo illustrate, we know the current running through them will be the same. And the voltage drop across \\(R_{1}\\) and \\(R_{2}\\) will be the current times each resistance value. The output voltage across \\(R_{2}\\) will then be:\n$$ V_{out} = V_{in} \\frac{R_{2}}{R_{1} + R_{2}} $$\nProvided if \\(R_{2} \\gg R_{1}\\), \\(V_{out} = V_{in}\\); there is no loss in signal but realistically signal will experience reduce in magnitude when passing from stages to stages. This is known as the loading effect.\nIn some other scenario, in communication system or power distribution system, it is of much greater interest to deliver the maximum power than to preserve the voltage signal. Using a potential divider circuit as an example:\nThe power distribution across \\(R_{1}\\) and \\(R_{2}\\) will then be:\n$$ P_{1} = V_{in}^{2} \\frac{R_{1}}{(R_{1}+ R_{2})^{2}} $$\n$$ P_{2} = I^{2} \\frac{R_{2}}{(R_{1}+ R_{2})^{2}} $$\nWe want the resistance ratio such that the power distributed to the second resistance be maximum.\n$$ \\frac{d P_{2}}{d R_{2}} = I^{2} \\frac{d}{dR_{2}} \\left[\\frac{R_{2}}{(R_{1}+ R_{2})^{2}}\\right] = I^{2} \\frac{(R_{1} + R_{2})(R_{1} - R_{2})}{(R_{1} + R_{2})^{4}} = 0$$\nThe maximizing condition gives \\(R_{1} = R_{2}\\) for the source to deliver the maximum power to the load.\nThevenin\u0026rsquo;s equivalent circuit of potential divider When working with more than one stages, Thevenin\u0026rsquo;s theorem comes in handy because it reduces stages into \\(V_{Th}\\), \\(R_{Th}\\) and \\(R_{load}\\). Suppose now we are using the potential divider as a driver, to drive a resistive load \\(R_{load}\\):\nLoading a potential divider circuit. Simulation shows tht the voltage drop across \\(R_{load} = 3.6 V\\)\nThe Thevenin\u0026rsquo;s voltage will be the output voltage as if the node is not connected to any load. $$ V_{Th} = V_{out} = V_{in} \\frac{R_{2}}{R_{1} + R_{2}} $$\nIf \\(V_{out}\\) is to be shorted, \\(V_{out} = 0\\) and the short current will be \\(V_{in}/ R_{1} = I_{short} = V_{Th} /R_{Th}\\). Hence, the Thevenin resistance can be calculated by substituting \\(V_{Th}\\):\n$$ R_{Th} = \\frac{V_{Th}}{I_{short}} = \\left. \\left(V_{in}\\frac{R_{2}}{R_{1} + R_{2}}\\right) \\right/ \\left(\\frac{V_{in}}{R_{1}}\\right) = \\frac{R_{1}R_{2}}{R_{1}+ R_{2}} = R_{1}||R_{2}$$\nThe Thevenin equivalent circuit for the potential divider is then can be drawn as:\nThevenin equivalent circuit. The simulation result shows 3.6V votlage drop.\nThe voltage felt at the load can be easily calculated:\n$$ V_{out} = V_{Th} \\frac{R_{load}}{R_{Th} + R_{load}} = 3.6V $$\nwhich agrees with the simulation value of the potential divider circuit with a reisistive load.\n2. RCL circuit Beside resistive loads, we have two different components that behaves in quite a different manner, the capacitance and inductance; they store energy in the form of electric field and magnetic field. Resistor network allows us to split the power distribution across different loads. Whereas capacitive and inductive loads react to the input signal of different frequencies.\nCapacitance A capacitor is a electronic devices that consists of conducting materials. It store charges in a particular configuration such that the voltage and charge are proportional. The constant of proportionality is called the capacitance.\nLet\u0026rsquo;s solve a simple EM question to illustrate this:\nFlat plate capacitor The charge resides on the condcutor can be denoted by the charge density multiply by its area. Maxwell\u0026rsquo;s equation in integral form\n$$ \\oint_{A}{E\\cdot da} = \\frac{\\sigma A }{\\epsilon_{0}} = E \\cdot 2A$$ $$ E = \\frac{\\sigma}{2\\epsilon_{0}} $$\nFor a charge moves from one plate to another, it gain a energy per charge of:\n$$ V = \\int{E\\cdot dr} = E \\cdot d = \\frac{\\sigma d}{2 \\epsilon_{0}}$$\nDefine the capacitance for a flat plate capcitor:\n$$ C = \\frac{Q}{V} = Q\\frac{2\\epsilon_{0}}{\\sigma d} = QA \\left(\\frac{2\\epsilon_{0}}{A\\sigma d}\\right) $$\nSince the charge density times the area of the plate equals to the total charge stored in the capacitor \\(Q = \\sigma A\\), the capacitance for a particular configuration of conductor is a constant that depends on the geometric of the configuration:\n$$ C = 2\\frac{\\epsilon_{0}A}{d} $$\nCapacitance is understood as the ability to store charge in a given potential \\(V\\). So, it is of no suprise that the \\(\\epsilon_{0}\\), which is material dependent quantity could increase the ability to store charge. As well as increasing the area of the plate, and decreasing the distance of separation between the conducting plates, which reduces the potential difference.\nInductance Solenoid with N coils As for energy stored in the form of magnetic field, consider the Maxwell\u0026rsquo;s equation:\n$$ \\nabla \\times B = \\mu_{0} J + \\mu_{0} \\epsilon_{0} \\frac{\\partial E}{\\partial t}$$\nCasting it in integral form:\n$$ \\int_{A}{(\\nabla \\times B ) \\cdot da} = \\oint_{R}{B \\cdot dr } = \\mu_{0} I + \\mu_{0}\\epsilon_{0} \\int_{A}{\\frac{\\partial E}{\\partial t} \\cdot da}$$\nFor a single loop of current and static electric, we have the following relationship:\n$$ B \\cdot 2 \\pi r = \\mu_{0} I$$\n$$ B = \\frac{\\mu_{0}I}{2\\pi r}$$\nWe are trying to look for how this particular configuration of conductor could have lead to a specific relationship between the voltage and the current. To this end, we need the other Maxwell\u0026rsquo;s equation as well:\n$$ \\nabla \\times E = -\\frac{\\partial B}{\\partial t}$$\nTaking a surface integral of the equation above:\n$$ \\int_{A}{(\\nabla \\times E)\\cdot da} = \\int_{A}{\\frac{\\partial B}{\\partial t}\\cdot da}$$\nSubstitute \\(B\\) as current:\n$$ \\int_{A}{\\frac{\\partial B}{\\partial t}\\cdot da} = \\frac{\\mu_{0}}{2\\pi} \\frac{d}{d t}I \\int_{A}{\\frac{1}{r}rdrd\\theta} = \\frac{\\mu_{0} r}{2}\\frac{d I}{d t}$$\nFor the term in the left-hand side, we can apply integral theorem to rewrite the surface integral into a closed loop integral:\n$$ \\int_{A}{(\\nabla \\times E)\\cdot da} = \\oint_{R}{E \\cdot dr} = V$$\nThen, for a single loop of current, we have the following relationship between the voltage and the current:\n$$ V = L \\frac{dI}{dt} $$\nwhere \\(L\\), the inductance of a single loop of current, is defined as:\n$$L = \\frac{\\mu_{0}r}{2} $$\nFor a solenoid with N loops around, the inductant can be generalized to the following:\n$$L = \\frac{\\mu_{0}rN}{2} $$\nPhysically, the inductance is the ability to resist the change of current by generating a potential difference. The inductance is also a constant that depends on how the current loop is configurated.\nCapacitors, inductors and resistor can be connected in series or parallel following the voltage law and current law forming a system block that processes the input signal. Together, they form a linear operator on the input signal. The solution of the output signal accounts to solve a linear ODE.\nLinear ODE form by R, C and L For resistor, capacitor and inductor, the voltage and current relation are listed below:\n$$ V = IR $$ $$ V = \\frac{Q}{C} = \\frac{1}{C}\\int{I dt} $$ $$ V = L \\frac{dI}{dt}$$\nThey all relate current and voltage with a time derivative or a integral. To make them more convinent in use we take derivative of every equations.\n$$ R\\frac{dI}{dt} = \\frac{dV}{dt}$$ $$ C \\frac{dV}{dt} = I $$ $$ \\frac{dV}{dt} = L \\frac{d^{2}I}{dt^{2}}$$\nIn general, we can express the above equation in a general form:\n$$ \\frac{d^{m}}{dt^{m}}V = k \\frac{d^{n}}{dt^{n}}Q$$\nWe can characterize each components by the number \\(d = m-n\\). So, resistor, capacitor and inductor has \\(d = 0, 1, -1\\) respectively.\nWe want to know what kind of voltage and current dynamics it will cause when we connect them in parallel or series which we will make use of the current and voltage law to formulate the dynamics.\nIf we connect resistor and capacitor in series, the current running through both components will be the same from the current law.\nA simple RC circuit connected in series\nHence, we must have\n$$ C \\frac{dV_{C}}{dt} = I = \\frac{V_{R}}{R}$$\n$$ \\frac{dV_{C}}{dt} = \\frac{V_{R}}{RC} $$\nwhere \\(V_{C}\\) and \\(V_{R}\\) each denotes the voltage drop across capacitor and resistor respectively. Depending on which node we try to probe the circuit, we can rename the voltage as input voltage and output voltage and hence the RC circuit form a first-order linear differential equation on the voltage.\nWhat if we connect the RC in a parallel fashion?\nRC circuit connected in parallel\nIn this case, the voltage across each component will be the same but current will be divided.\n$$ I_{R}R = \\frac{1}{C} \\int{I_{C} dt}$$\nTaking derivative\n$$ \\frac{dI_{R}}{dt} = \\frac{I_{C}}{RC} $$\nConnection in series leads to first-order linear ODE in their voltage, whereas connection in parallel leads to first-order linear ODE in their current. This makes sense, because the voltage law implies they have equal current and the current law implies they have equal voltage. RC circuit create a firt order ODE because there is a different in the order of time derivative in their respective physical law. eg. resistor with \\(d_{R} = 0\\) and capacitor with \\(d_{C} = 1\\) makeing a difference of \\(O_{RC} = d_{R} - d_{C} = 1\\) which invitably leads to a first-order ODE.\nA more interesting case when we connect inductor to capacitor. \\(O_{CL} = 1- (-1) = 2\\), which tells us that they will form a second-order ODE.\nIf connecting them in series,\nA CL circuit connected in series\n$$ C\\frac{d^{2}V_{C}}{dt^{2}} = \\frac{dI}{dt} = \\frac{V_{L}}{L}$$\n$$ \\frac{d^{2}V_{C}}{dt^{2}} = \\frac{dI}{dt} = \\frac{1}{LC}V_{L} $$\nIf they were connected in parallel:\n$$ L \\frac{d^{2}I_{L}}{dt^{2}} = \\frac{I_{C}}{C}$$\n$$ \\frac{d^{2}I_{L}}{dt^{2}} = \\frac{1}{LC} I_{C}$$\nAs expected, they lead to a second-order differential equation in either voltage or current. It is also worth noting that the current law will restrict \\(I_{L}\\) and \\(I_{C}\\) be opposite sign. This leads to the equation of motion of the form\n$$ \\frac{d^{2}x}{dt^{2}} = -kx$$\nwhich assembles the equation of motion for a oscillating pendulum with the spring constant replace by \\(\\frac{1}{CL}\\).\nTime series analysis Though we can configure the input and output differently, it will lead to different equation of motion. But for RC connected in series, they will be related by the following equation:\n$$ \\frac{dV_{C}}{dt} = \\frac{V_{R}}{RC}$$\nBy configuring the input and output voltage accordingly, we will have different dependent of \\(V_{in}\\) and \\(V_{out}\\) in place of \\(V_{C}\\) and \\(V_{R}\\).\nCase 1: Output voltage across capacitor A RC circuit conneccted in series with output measured across the capacitor\nIt must follow that:\n$$ V_{C} = V_{out} $$\n$$ V_{R} = V_{in} - V_{out}$$\nThe voltage across the resistor depends on the direction of the current flow.\nwhich leads to the first-order linear ODE:\n$$ \\frac{dV_{out}}{dt} = \\frac{1}{RC} (V_{in} - V_{out}) $$\n$$ \\frac{d V_{out}(t)}{d t} + \\frac{1}{RC} V_{out}(t) = \\frac{1}{RC} V_{in}(t) $$\nWith the source term \\(\\frac{1}{RC} V_{in}(t)\\). Let\u0026rsquo;s assume for the time being a constant voltage input \\(V_{in} = V_{const}\\). This equation has a general solution of the form:\n$$ V_{out} = V_{homogeneous} + V_{inhomogenous} $$\nTreating \\(V_{const} = 0\\), and solving the ODE with separation method with the initial condition \\(V_{out} (t=0) = V_{const}\\), we yield the following homogeneous solution:\n$$ V_{out}(t) = V_{const} e^{-\\frac{t}{RC}}$$\nThe inhomogeneous case can be solved by integrating factor. For constant source term \\(V_{in} = V_{const}\\), we have the following solution:\n$$ V_{inhomogenous} = V_{const}$$\nIntroducing the initial condition \\(V_{out} (t= 0) = 0\\)\n$$ V_{out} = V_{const}(1- e^{-\\frac{t}{RC}})$$\nAs we can see in the first case is a exponential decay from \\(V_{const}\\) and assmytotic to the \\(V = 0\\). This characterizes the capacitor decay from its charge state to its discharged state. In the second case, the capacitor is initially discharged, with \\(V = 0\\) and approaching the charge state \\(V= V_{const}\\).\nThis can be illustrated by simulation with an input voltage of a square wave of \\(5V\\) with period of \\(20ms\\).\nRC circuit simulation with square wave input. It shows both the charging process and the discharging process.\nWe can observe the rising \\(V_{out}\\) signal in the region of high \\(V_{in}\\) region that indicates the capacitor charging. On the low-state of the square wave, the capacitor signal \\(V_{out}\\) is discharging. To verifies the simulation agrees with the calculation, we take a single point when the output voltage falls to half of its value. This happens at its half-life:\n$$ t = RC\\ln{2} = 0.69ms $$\nFrom the annotated point on graph, we have \\(20.678 - 20.094 = 0.58ms \\), with a fractional error of \\( \\frac{0.69- 0.58}{0.58}= 0.18 \\). Not too bad I guess.\nThe rate of decaying depends on the product of capacitance and resistance, which is commonly defined as the time constant \\(\\tau = RC\\) for the RC circuit. The effect of a larger capacitance meaning slower change of voltage with the same current flow. Thus, leading to a slower charging and discharing time. The present of a resistor is to acts against the flow of current, which in turn has the same effect of slowing down charging rate.\nA different perspective on the result is that, the RC circuit with output measures across the capacitor is considered as integrator on the input voltage. This can be eaily shown in the equation as well as on the time series plot. This might come in handy when working with experiment that has a meaningful integral output.\nCase 2: Output Voltage across the Resistor RC circuit connected in series with output measured across the resistor.\n$$ V_{R} = V_{out} $$\n$$ V_{C} = V_{in} - V_{out}$$\nFollowing the same analysis, this leads to the differential equation\n$$ \\frac{d}{dt} V_{out} + \\frac{1}{RC} V_{out} = \\frac{d V_{in}}{dt} $$\nIt is tempting to treat this question the same way as we did in the previous question. But with the input signal being differentiatied, there are some subtlties to consider.\nSupose \\(V_{in}\\) is a step function that has a jump at \\(t_{0}\\),\n$$ V_{in}(t) = \\begin{cases} V_{const} \u0026amp;\\text{if } t \u0026lt; t_{0} \\\\ 0 \u0026amp;\\text{if } t \u0026gt; t_{0} \\end{cases} $$\nThere is a discontinuity at \\(t_{0}\\). Derivative at \\(t_{0}\\) will be infinite and zero elsewhere. We know one function (or distribution) that does exactly that, the Dirac Delta function:\n$$ V_{const}\\delta (t - t_{0}) = \\left. \\frac{dV_{in}}{dt}\\right\\vert_{t=t_{0}} = \\begin{cases} \\infty \u0026amp;\\text{if } t = t_{0}\\\\ 0 \u0026amp;\\text{if } t \\ne t_{0}\\end{cases} $$\nThe equation then becomes:\n$$ \\frac{d}{dt} V_{out} + \\frac{1}{RC} V_{out} = V_{const}\\delta (t-t_{0}) $$\nThe homogeneous solution with integrating factor \\(\\alpha = e^{t/RC}\\)\n$$ V_{homogeneous}(t) = V_{0} e^{-\\frac{t}{RC}}$$\nFrom the initial condition, \\(V_{out}(t= 0) = 0\\), so \\(V_{0} = 0\\); the homogeneous term does not contribute to the solution.\nThe inhomogeneous solution:\n$$ V_{inhomogeneous}(t) = V_{const}e^{-\\frac{t}{RC}} \\left[\\int^{t_{0} - \\epsilon}{e^{\\frac{t}{RC}}\\delta (t-t_{0})dt} + \\int_{t_{0}}^{t}{e^{\\frac{t}{RC}}\\delta (t-t_{0})dt} \\right] $$\nThe non-homogeneous solution separete into integral of two regions. Because of the property of the Dirac Delta function, the only contribution comes from the time region \\(t\u0026gt; t_{0}\\).\nHence, in the region \\(t \u0026lt; t_{0}\\), the contribution from both the homogeneous term and inhomogeneous term are zero.\n$$ V_{out}(t)\\vert_{t\u0026lt; t_{0}} = 0 $$\nFor the region \\(t \u0026gt; t_{0}\\),\n$$ V_{out}(t)\\vert_{t\u0026gt; t_{0}} = V_{const}e^{-\\frac{t}{RC}} \\int_{t_{0}}^{t}{e^{\\frac{t}{RC}}\\delta (t-t_{0})dt} = V_{const}e^{-\\frac{t-t_{0}}{RC}} $$\nAt the \\(t = t_{0}\\), \\(V_{out}(t)\\) jumps from \\(0V\\) to \\(V_{const}\\), which reaches its maximum. After \\(t\u0026gt; t_{0}\\), \\(V_{out}(t) = V_{const} e^{-(t-t_{0})/RC}\\), it decays exponentially with the time constant \\(RC\\).\nRC circuit with output measured across the resistor. It behaves as a differentiator to the input signal.\nThe qualitative behavior of the simulation agrees with our analytic predicton. However, at the falling edge, the output voltage becomes negative because the derivative becomes negative at the falling edge.\nThe sharply peaked behavior of the capacitor is known as a differentiator because it is a derivative on the input voltage. The discussion of differentiator and integrator is important because they process the incoming signal in different manner. eg. For a fast varing signal, the integration of the input signal will not reflect the information as much as a differentiation of the fast varying signal. Whereas a slow varying signal, the differentiator won\u0026rsquo;t reflect this character as much as an integrator on its output.\nCase 3: RCL parallel resonant circuit Resistor and capacitor connected parallel to the inductor forming a resonance circuit.\nA extension to the CL circuit, we add a resistor parallel to the inductor. This leads to an additional term with a first-order time derivative to the equation.\n$$ I_{R} = I_{C} + I_{L}$$\n$$ \\frac{V_{R}}{R} = C\\frac{dV_{C}}{dt} + \\frac{1}{L} \\int{V_{L}dt} $$\nTaking a time derivative on both side:\n$$ \\frac{1}{R} \\frac{dV_{R}}{dt} = C\\frac{d^{2}V_{C}}{dt^{2}} + \\frac{1}{L}V_{L} $$\nFrom the schematic, we can conclude that:\n$$ V_{R} = V_{in} - V_{out} $$\n$$ V_{L} = V_{C} = V_{out} $$\nwhich leads to equation:\n$$ C\\frac{d^{2}}{dt^{2}} V_{out} + \\frac{1}{R}\\frac{d}{dt} V_{out} + \\frac{1}{L} V_{out} = \\frac{d}{dt} V_{in}$$\nwhich assembles a second-order linear ODE with restorative force as well as resistive terms.\n$$ m\\frac{d^{2}x}{dt^{2}} + b \\frac{dx}{dt} + kx = F(t)$$\nIts dynamics, in addition to that of a oscillating pendulum as in the CL circuit, it also has a damping term. The coefficient of friction that resists the motion, is the coefficient of the first-order derivative. The damping coefficient is defined by \\(1/CR\\) for this particular circuit configuration and its effect is to dissipates the energy stores in the system. In the absence of the forcing term and suppose the system comes with an initial swing, it will eventually leads to a static state. But in the present of the forcing term, it will stablize in an oscillatory state.\nFor the above RCL resonant circuit, the resonant frequency is \\(f_{0} = 1/(2\\pi \\sqrt{CL}) = 160Hz\\). Running a time series simulation reveal the fact that the static state of the system will be oscillating.\nTime series plot of both input and output signal. The output signal shifted in phase and being attenuated.\nIf we isolate the system itself by assuming zero input voltage and giving it a initial swing, how will the system behave? This is equivalent to solve for the homogeneous equation. The properties of the homogeneous solution of the damped oscillation is instrinsic to the system. In order words, its properties depend on the resistance, capacitant and inductant. Define the terms:\n$$ \\gamma = \\frac{1}{2RC}$$\n$$ w_{0} = \\frac{1}{CL}$$\nThe first term is known as the damping coefficient and the second term is known as the natural angular frequency. Rewriting the equation in the homogeneous case:\n$$ \\frac{d^{2}V}{dt^{2}} + 2\\gamma \\frac{dV}{dt} + w_{0}^{2} V = 0$$\nAssume \\(V(t) = V_{0}e^{\\lambda t + \\phi}\\), leading to quadratic equation of the unknown parameter \\(\\lambda\\):\n$$ \\lambda^{2} + 2\\gamma \\lambda + w_{0}^{2} \\lambda = 0$$\nHence, the roots for the quadratic equation has the solution of\n$$ \\lambda = \\frac{-2\\gamma \\pm \\sqrt{4\\gamma^{2} - 4w_{0}^{2}}}{2} = -\\gamma \\pm \\sqrt{\\gamma^{2} - w_{0}^{2}} $$\nThe solution:\n$$ V_{out} (t) = V_{0} e^{-\\gamma t} e^{\\pm t \\sqrt{(\\gamma^{2} - w_{0}^{2})} }$$\nDepending on the relative magnitude of the damping coefficient and the natural angular frequency, the solution can be of these three types of qualitative behaviors:\nUnderdamped. \\(w_{0} \u0026gt; \\gamma \\) The exponent will be complex number which leads to a oscillator behavior with an exponential dampping.\n$$ V_{out} = V_{0} e^{-\\gamma t} e^{-iw_{d}t + \\phi }$$\nwhere we define \\(w_{d} = \\sqrt{(w_{0}^{2} - \\gamma^{2})}\\)\nOverdamped. \\( \\gamma \u0026gt; w_{0} \\) $$ V_{out} = V_{0} e^{-(\\gamma+w_{d}) t + \\phi} $$\nCritical Damped. \\( \\gamma = w_{0} \\) $$ V_{out} = V_{0} e^{-\\gamma t + \\phi}$$\nPlotting these different solutions and compare their behavior overtime:\nThree types of solution for a damped oscillation.\nThe three solutions all reach a state of zero amplitude; they differed in how fast they approach this static state. Thus, the effect of the damping coefficient is to resist the motion of the system and to dissipate energy until it is stationary.\nIn electronic parameter, \\(\\gamma = 1/RC \\), the larger the resistor the more rapidly it dissipates the energy, which makes sense because this is the only component that dissipates energy. The question arises from the fact that capacitance also affect the rate of dissipation. The smaller the capacitance also leads to an increase in rate of dissipation. But physically why?\nPower To have a quantitative measure of the power dissipation, we need to calculate the average power dissapated. In static case, power is computed as:\n$$ P = VI $$\nBut now we are working with oscillator voltage and current, so the current and voltage in general can be written as:\n$$ V(t) = V_{0} \\cos{(wt)}$$ $$ I(t) = I_{0} \\cos{(wt + \\phi)}$$\nwhere \\(w\\) is the angular frequency and \\(\\phi\\) is the phase difference between the voltage and the current. As we shall see in the transfer function analysis, the difference in phase arises from how we assembles the electronics components, but for the time being, this is reckoned as a fact.\nThe average power is then calculated by summing the product of voltage and current over its period and divided by the period:\n$$ \\left\u0026lt; P \\right\u0026gt; = \\frac{1}{T} V_{0} I_{0} \\int^{T}_{0}{\\cos{(wt) \\cos{(wt+\\phi)}}dt}$$\nBy triogometric identities, the integrand can be simplified to:\n$$ \\cos(wt)\\cos(wt+ \\phi) = \\cos(wt)[\\cos(wt)\\cos(\\phi) - \\sin(wt)\\sin(\\phi)]$$\n$$= \\cos^{2}(wt)\\cos(\\phi) - \\frac{1}{2}\\sin(2wt)\\sin(\\phi) $$\nIntergrating over its period, the second term vanishes:\n$$ \\left\u0026lt; P \\right\u0026gt; = \\frac{1}{T} V_{0}I_{0} \\cos(\\phi) \\int_{0}^{T}{\\cos^{2}(wt)dt}$$\n$$ \\left\u0026lt; P \\right\u0026gt; = \\frac{1}{2} V_{0}I_{0} \\cos(\\phi)$$\nThe average power is also scaled by the phase difference between the voltage and current. This is the reason why the capacitance has the dissipative effect. The capacitor stores energy in terms of electric field by the processing of charging. This in turns lags the voltage from the current, creating the phase difference that actually reduces the dissipation of energy from the resistive load. We will come back to the phase difference dependent on the electronic parameter \\(CL\\) and the configuration of the circuitry in the next section.\nTransfer Function Assuming input voltage and input current to be complex quantity with a monochromatic angular frequency of \\(w\\)\n$$ \\widetilde{V}(t) = V_{0} e^{-iwt} $$\nwhere \\(V_{0}\\) is the magnitude of the voltage signal.\nIt is convient to express it in terms of complex number because it will reduce the linear ODE that governs the dynamics of the voltage into an algebraic equations in the complex plane. For example,\n$$ \\frac{d}{dt} \\widetilde{V}(t) = -iw\\widetilde{V}(t)$$\nHigh-pass filter Hence, for RC circuit:\nA differentiator also behaves as a high-pass filter.\n$$ \\frac{d}{dt} \\widetilde{V_{o}} + \\frac{1}{RC}\\widetilde{V_{o}} = \\frac{d \\widetilde{V_{i}}}{dt} $$\n$$ \\left(-iw + \\frac{1}{RC}\\right) \\widetilde{V_{0}} = -iw \\widetilde{V_{i}}$$\nDefine the transfer function \\(T(w)\\). It mesures the absolute ratio of the output volage to the input voltage.\n$$T(w) = \\left| \\frac{\\widetilde{V_{o}}}{\\widetilde{V_{i}}} \\right|$$\nNotice the transfer function is scalar quantity that measures the relative magnitude between the output signal and the input signal. And more importantly, it has no time dependent but only depends on the frequency. For the RC circuit,\n$$ T(w) = \\left| \\frac{-iw}{-iw + \\frac{1}{RC}} \\right| = \\frac{1}{\\sqrt{\\left(\\frac{1}{wRC}\\right)^{2} + 1}} $$\nThe transfer function for RC circuit with output voltage defined on the resistor has two region of behavior. For low angular frequency, \\(w \\ll RC\\), \\(T(w) \\ll 1\\), meaning it produces low output when the input frequency is lower than the time constant \\(\\tau = RC\\). Whereas in the case of high angular frequency, \\(w\\gg RC\\), the transfer function \\(T(w) = 1\\), which allows the high frequency signal to go through. Hence, this RC setup is known as the high-pass filter.\nReactant and Impedence One simplistic view of the method is redefined capacitance and inductance in the complex plane, which generalizes resistance to impedance. Recognizing:\n$$ \\frac{d}{dt} \\equiv -iw$$\n$$ C\\frac{d}{dt}\\widetilde{V} = -iwC\\widetilde{V} = \\widetilde{I}$$\nDefine the reactant \\( \\widetilde{X} = \\widetilde{V}/\\widetilde{I} \\), we have purely imaginary reactant for capacitor and inductor:\n$$ X_{C} = \\frac{i}{wC} $$\nAnd similarly for inductance:\n$$ X_{L} = -iwL $$\nWe can further generalize the quantity impedance, which is the sum of resistance and reactant, which composes of both real and imaginary part each representing the resistive part and reactive part of a circuit configuration.\nThis perspective of loads have the advantage to treat passive circuit with either AC or DC signal in the same way as resistor network. For example, Ohm\u0026rsquo;s law can be re-expressed as such:\n$$ \\widetilde{V} = \\widetilde{Z}\\widetilde{I} $$\nLow-pass filter For example, consider an integrator where the output voltage is measured across the capacitor.\nAn integrator behaves as a low-pass filter.\nIt forms a potential divider with \\(R_{1} = R, R_{2} = X_{C}\\)\n$$ \\widetilde{V_{o}} = \\widetilde{V_{i}} \\frac{X_{C}}{R + X_{C}} = \\widetilde{V_{i}} \\frac{\\frac{i}{wC}}{R + \\frac{i}{wC}}$$\n$$ T(w) = \\left| \\frac{i}{wCR +i} \\right| = \\frac{1}{\\sqrt{(wCR)^{2} + 1} }$$\nAs \\(w \\gg CR\\), \\(T(w) \\approx 0\\) and \\(w\\ll CR\\), \\(T(w)\\approx 1\\). Opposite to the potential measure across the resistor (differentiator), an integrator is a low-pass filter.\nDecibel per Octave One way to quantify the relative magnitude dependence on the frequency is to define the quantity \u0026ldquo;decibel per octave\u0026rdquo;. One octave means a double of frequency. Decible is the measure of the signal power relative to a reference level in logarithmic scale. eg. Each decibel \\(dB\\) corresponding to \\(10 \\log{\\frac{P}{P_{0}}}\\). Because power is proportional to the square of the voltage \\(P \\propto V^{2}\\), we have decibel measure in transfer funti: \\(20 \\log{\\frac{V}{V_{0}}} = 20 \\log{T(w)} \\).\nPlotting the transfer function in dB with the frequency on a octave scale, we can clearly observe the transfer function is at maximum at a small frequency and decreases when frequency drops, which matches our prediction that it is a low-pass filter.\nFrequency response of a low-pass filter with its \\(3dB\\) point labelled.\nDiving a little deeper into the high frequency region. At high frequency, the transfer function scales roughly as \\(w^{-1}\\). This behavior is known as the power-law scaling with coefficient of \\(-1\\) in phase transition literature. Doubling the frequency in the high frequency region will reduce the amplitude by half. This is shows as a straight line on the plot. In terms of decibel, we can characterize the striaght line by the large frequency behavior of \\(w^{-1}\\):\n$$ 20 \\log_{10}{\\frac{1}{2}} = -6 dB / octave$$\nWhereas in the low frequency region, \\(T(w) \\approx 1\\), which corresponds to a horizontal line on the curve. Separating the two regions of behavior is called the 3dB point. The point is annotated on the graph for the low-pass filter with measured value of \\(159 Hz\\).\n3dB point In the electrical engineering literature, the \\(3dB\\) point is often being defined by the frequency in which the transfer function becomes \\(T(w) = 1/\\sqrt{2}\\).\n$$ \\frac{1}{\\sqrt{2}} = \\frac{1}{\\sqrt{(wCR)^{2} + 1}} $$\n$$ f_{3dB} = \\frac{1}{2\\pi CR} = 160Hz$$\nAs an alternative, we can refer to the phase transition literature and define the \\(3dB\\) point from the singularity of \\(\\widetilde{T(w)}\\).\n$$ \\lim_{w\\to -i/CR} \\widetilde{T(w)} = \\lim_{w\\to -i/CR} \\frac{i}{wCR + i} = \\infty$$\nThe transfer function has a singular point at the \\(3dB\\) point, which leads to a power-law scaling and a phase transition.\nPhase Shift As shown on the plot above, it also includes the dotted plot that indiciates the phase difference, showing that the input and output voltage is out of phase. To see this, refer back to the complex transfer function:\n$$ \\widetilde{T(w)} = \\frac{i}{wCR + i} = \\frac{1+ iwCR}{1+(wCR)^{2}}$$\nFor large \\(w \\gg CR\\), the transfer function can be approximated to \\(\\widetilde{T(w)} \\approx \\frac{i}{wCR} \\), a pure imaginary number. This implies that the output voltage leads the input voltage by \\(90^{\\circ}\\).\nIn general, for a complex number \\(Z = Re(Z)+ iImg(Z) = Ae^{\\phi}= A\\cos{\\phi} + iA\\sin{\\phi} \\). The phase and magnitude can be calculated by matching the real and imaginary part with the Euler representation of complex number.\n$$ A = \\sqrt{Re(Z)^{2} + Img(Z)^{2}}$$\n$$ \\phi = \\arctan{\\frac{Img(Z)}{Re(Z)}}$$\nFor pure imaginary output, \\(\\phi = \\arctan{\\infty} = \\pi /2\\), which verifies that claim earlier.\nLCR resonant circuit Considering LCR resonant circuit configured in different ways:\nA serially connected RCL circuit with output measured across the resistor.\nBecause the capacitor and inductance are connected in series, we can calculate their reactant combined.\n$$ X_{tot} = X_{C} + X_{L} = \\frac{-1}{iwC} -iwL$$\nTogether with the resistor, they form a potential divided with output voltage measure across the resistor. This leads to the transfer function of the following form:\n$$ \\widetilde{T(w)} = \\frac{R}{X_{tot} + R} = \\frac{R}{\\frac{-1}{iwC} - iwL + R} = \\frac{iwCR}{w^{2} CL + iwCR - 1}$$\nDivide nominator and denominator so that the highest power of \\(w\\) in the denominator is 1. And define the damping coefficient \\(\\gamma_{0} = \\frac{R}{L}\\) and the resonant frequency \\(w_{o} = \\frac{1}{\\sqrt{CL}}\\).\n$$ \\widetilde{T(w)} = \\frac{iw\\frac{R}{L}}{w^{2} - \\frac{1}{CL} +iw \\frac{R}{L}} = \\frac{-iw\\gamma_{0}}{w^{2} - w_{0}^{2} +iw \\gamma_{0}}$$\n$$ |\\widetilde{T(w)}| = \\frac{w\\gamma_{0}}{\\sqrt{(w^{2} - w_{o}^{2})^{2} + (w\\gamma_{0})^{2}}}$$\nWhen the input frequency reaches the resonant frequency of \\(w = w_{0}\\) or \\(f_{0} = w_{0}/2\\pi = 1/(2\\pi \\sqrt{LC}) = 159Hz\\), the transfer function reaches its maximum \\(T(w) = 1\\) or \\(20\\log_{10}(1) = 0dB\\) in decibel. Moving away from the resonant \\(w_{0}\\) will increase the factor of the denominator and results in a smaller transfer function. In a sense, the resonant circuit picks up the frequency that is close to the resonant frequency. As can be seen in the figure below:\nFrequency response of a resonance circuit. It peaks at the resonance frequency.\nDamping coefficient In the RCL series, the damping coefficient is different from the RCL parallel connection in the time series analysis. Back then, it was define to be \\(\\gamma = 1/ RC\\), comparing with RCL series \\(\\gamma = R/L \\). A series connection form a voltage divider between components. Therefore, the voltage law is applied to connect the voltage between each every components. Writing the voltage in terms of their common current formulates the second-order differential equation in the function of the current. The coefficient of the second-order derivative must be of inductance due to its nature \\(L \\frac{dI}{dt} = V\\). It is the \u0026ldquo;mass\u0026rdquo; term of the current second-order ODE. The inductor is a component that slows down the change of current.\nSimilar conclusion can be made for RCL parallel connection, where it forms a current divider and so on and so forth. The capacitor is a component that slows down the change of voltage due to its nature \\(C\\frac{dV}{dt} = I\\).\nTo see this from another perspective, we bring back a topic that being postponed in the discussion of phase difference between voltage and current. Recall the average power depends not only on average current and average voltage, but also their relative phase:\n$$ \\left\u0026lt; P \\right\u0026gt; = I_{rms} V_{rms} \\cos{\\phi}$$\nThe present of \\(C\\) and \\(L\\) in the dissipative term leads us to consider the relation between the relative phase and \\(C\\) and \\(L\\). If we generalize Ohm\u0026rsquo;s law to impedence, which is a complex number in general, we will have the following relationship:\n$$ \\widetilde{V} = \\widetilde{I} \\widetilde{Z} $$\nExpressing them in Euler form:\n$$ V_{0}e^{i\\theta} = I_{0}Z_{0} e^{i(\\theta + \\phi)} $$\nHence, impedance will shift the relative phase between the voltage and current if it contains imaginary part, or for \\(\\theta_{Z}\\) multiple of \\(\\pi\\). To verify that the relative phase \\(\\phi\\) dependence on \\(C\\) and \\(L\\), we can calculate their Thevenin\u0026rsquo;s impedance in the parallel and series case. And evaluate the phase of the imepednace.\nFor series connection, \\(V_{Th} = V_{out}\\), \\(R_{Th} = V_{Th}/I_{short}\\)\n$$ V_{Th} = V_{in} \\frac{R}{ \\frac{i}{wC} - iwL + R}$$\n$$ I_{short} = V_{in} \\frac{1}{\\frac{i}{wC} - iwL}$$\nHence, the equivalent resitance:\n$$ Z_{series} = \\frac{-R(w_{0}^{2} -w^{2})^{2} - iRw\\gamma (w_{0}^{2} - w^{2})}{(w\\gamma)^{2} + (w_{0}^{2} - w^{2})^{2}}$$\n$$ \\phi_{series} = \\arctan{\\left( \\frac{R}{L} \\frac{w}{(w_{0}^{2}-w^{2})}\\right)}$$\nSimilarly for parallel connection\n$$ \\phi_{parallel} = \\arctan{\\left(\\frac{RC}{w}(w_{0}^{2} - w^{2})\\right)} $$\nwhich proves the relative phase dependence on \\(C, L\\).\nQ Factor From the transfer function,\n$$ |\\widetilde{T(w)}| = \\frac{w\\gamma_{0}}{\\sqrt{(w^{2} - w_{o}^{2})^{2} + (w\\gamma_{0})^{2}}}$$\nthe damping coefficient \\(\\gamma_{0}\\) presents in both the nominator and the denominator. With a larger damping coefficient, deviation of \\(w\\) from \\(w_{0}\\) will become less sensitive hence leads to a larger bandwidth.\nOne way to quantify the resonance width is to define the full width at half maximum. From the above circuitry setup, we can first calculate the \\(w_{3dB}\\) points on the rising and the falling edge. The difference in these two points measure the width of the resonance width.\n$$ \\frac{1}{\\sqrt{2}} = \\frac{w\\gamma_{0}}{\\sqrt{(w^{2} - w_{o}^{2})^{2} + (w\\gamma_{0})^{2}}}$$\n$$ \\pm w \\gamma_{0} = w^{2} - w_{o}^{2} $$\nwhich leads to two set of quadratic equation with a total of four solutions:\n$$ w_{\\pm} = \\pm \\sqrt{(\\gamma/2)^{2} + w_{0}^{2}} \\pm \\frac{\\gamma}{2}$$\nSince only positive frequency is physical, which left us with two solution indicating the two 3dB points.\n$$ w_{3dB} = \\pm \\frac{\\gamma}{2} + \\sqrt{(\\gamma/2)^{2} + w_{0}^{2}}$$\nThe width of the resonant peak is then the difference between these points.\n$$ \\delta w_{3dB} = \\gamma = \\frac{R}{L} $$\nThis is known as the full width at half maximum (FWHM). The Q factor, is then can be defined as:\n$$ Q = \\frac{w_{0}}{\\delta w_{3dB}} = \\frac{w_{0}}{\\gamma} $$\nThis gives a measure to the qualitative of the resonator. For example, for a large Q factor, it corresponds to a much larger damping coefficient compare to the resonance frequency, and \\(|T(w)|\\) will have a wider base near the resonant frequency. Whereas for a small Q factor, that means the transfer function \\(T(w)\\) will be sharply peaked near the resonance frequency.\nHow about for the RCL paralle circuit? Will the Q factor be the same in both cases? They should be the same because they describe the same resonant behavior. To simplify the calculation, the formal definition of Q factor is used:\n$$ Q = 2\\pi \\frac{\\text{maximum energy stored}}{\\text{energy dissipated in one oscillation}}$$\nThe energy is stored in the capacitor and the inductor.\n$$ \\left\u0026lt; E_{C} \\right\u0026gt; = \\frac{1}{2}CV_{rms}^{2}$$\n$$ \\left\u0026lt; E_{L} \\right\u0026gt; = \\frac{1}{2}LI_{rms}^{2}$$\nThey are actually related. If we write \\(I = C\\frac{dV}{dt} \\), then \\(I^{2}_{rms} = Cw^{2}V^{2} \\)\nThe energy dissipated is calculated when current running through a purely resistive load, so the phase difference \\(\\phi\\) between the voltage and current is zero.\n$$ \\left\u0026lt; P \\right\u0026gt; = I_{rms}V_{rms}$$\nSo the energy dissipated per cycle is:\n$$ T\\left\u0026lt; P \\right\u0026gt; = TI_{rms}V_{rms}$$\nWriting Q factor in terms of \\(E_{C}\\) (the total energy will \\(2E_{C}\\) or \\(2E_{L}\\)):\n$$ Q = 2\\pi \\frac{2E_{C}}{TI_{rms}V_{rms}} = w_{0} \\frac{CV_{rms}^{2}}{V_{rms} I_{rms}} = w_{0}CR = \\frac{w_{0}}{\\gamma}$$\nIn agreement with the Q factor for the series RCL.\nOpen questions: phase transitions, fluctuations, and dissipation Passive components may be “simple,” but together they form the smallest laboratory where physical law becomes signal processing. By enforcing charge conservation (current law) and energy accounting around loops (voltage law), resistors, capacitors, and inductors turn circuit connectivity into dynamics. In the time domain that shows up as exponential charging/discharging, damping, and resonating; in the frequency domain it becomes impedance, transfer functions, phase shifts, 3 dB points, and resonance. The same three elements let you build integrators, differentiators, and selective resonators—linear operators whose behavior can be read either from differential equations or from poles/zeros in the complex plane.\nIs a filter edge a “phase transition,” or just an analogy? In several places the response sharply changes between regimes (e.g., low-frequency plateau vs high-frequency power-law roll-off). On Bode plots this looks like a crossover with scaling laws (like \\(T(\\omega)\\sim \\omega^{-1}\\)). If we treat the transfer function magnitude as an “order parameter,” what is the precise sense in which the crossover near the 3 dB point resembles a critical point—and what breaks the analogy (finite bandwidth, linearity, lack of diverging correlation length)?\nBibliography Steck (2019) Steck,\u0026#32; D.\u0026#32; (2019). \u0026#32; Analog and digital electronics. \u0026#32; https://steck.us/teaching/. Horowitz\u0026#32;\u0026amp;\u0026#32;Hill (2015) Horowitz,\u0026#32; P.\u0026#32;\u0026amp;\u0026#32;Hill,\u0026#32; W.\u0026#32; (2015). \u0026#32; The art of electronics (3). \u0026#32; Cambridge University Press. Floyd (2015) Floyd,\u0026#32; T.\u0026#32; (2015). \u0026#32; Digital fundamentals (11). \u0026#32; Pearson Education. ","permalink":"https://htsod.github.io/ee/passivecomponent/","summary":"Passive components—resistors, capacitors, and inductors—look deceptively simple on a schematic. Yet every analog circuit is built on the way these three elements enforce conservation laws and store or dissipate energy. This post takes a physics-forward route into passives: starting from charge flow and electromagnetic work, we motivate Kirchhoff’s current and voltage laws, connect them to Ohm’s law, and build intuition for why “voltage drops” and “current continuity” are more than rules-of-thumb.","title":"Passive Components: Physical Laws, Transient Dynamics, and Resonance"},{"content":"Renormalization Group Approach in Dynamical System The renormalization group (RG) method is an approximation technique initially developed for solving strongly interacting many-body problems in quantum field theory, where perturbative solutions deviate from the actual solutions. The fundamental concept of the renormalization group approach is to eliminate irrelevant degrees of freedom in a physical system while preserving its essential characteristics ( Citation: P. Kopietz,\u0026#32;2010 P. Kopietz,\u0026#32; F.\u0026#32; (2010). \u0026#32; Introduction to the Functional Renormalization Group (1). \u0026#32; Springer, Berlin Heidelberg 2010. https://doi.org/10.1007/978-3-642-05094-7 ) . This method has been extended to the field of statistical mechanics, providing a quantitative description of universality and scale invariance phenomena.\nUniversality and Scale Invariance Universality refers to the observation that distinct physical systems can exhibit similar behavior near their critical points. Scale invariance occurs when the qualitative features of a system remain unchanged as we vary the system\u0026rsquo;s size ( Citation: Sethna,\u0026#32;2020 Sethna,\u0026#32; J.\u0026#32; (2020). \u0026#32; Entropy, Order Parameters, and Complexity (2). \u0026#32; Clarendon Press. ) . These concepts offer a framework for categorizing phenomena that exhibit self-replicating patterns in space (fractal structures) and time (period doubling). In this review, we will explore the relationship between changing the scale of a system and the emergence of chaos at certain parameter limits. By applying the renormalization group method, we can establish the close connection between these concepts and provide a unifying language for their analysis.\nRenormalization Procedure: Decimation and Rescaling The renormalization group method involves two key steps: decimation (mode reduction) and rescaling. Decimation eliminates irrelevant degrees of freedom in the system, while rescaling ensures that the remaining parameters are adjusted in such a way that the essential properties of the system are preserved.\nWe can describe the action of the renormalization group on a system using the operator \\(R(b;g)\\), where $g$ represents the relevant parameters or coupling terms that describe the system, and \\(b\\) defines the scaling operation. By applying this operator recursively, we can track the evolution of the parameters \\(g\\) as we scale the system by a factor of \\(b\\).\n$$ \\vec{g}^{\\prime} = \\vec{R}(b;\\vec{g}) $$ $$ \\vec{g}^{\\prime\\prime} = \\vec{R}(b^{\\prime};\\vec{g}^{\\prime}) = \\vec{R}(b^{\\prime};\\vec{R}(b;\\vec{g})) = \\vec{R}(b^{\\prime}b;\\vec{g}^{\\prime}) $$\nBy repeatedly applying this operation $n$ times, we obtain the relationship:\n$$ \\vec{g}^{(n)} = \\vec{R}(b;\\vec{g})^{n-1}=\\vec{R}(b^{(n)};\\vec{g}) $$\nThis equation signifies that by removing irrelevant degrees of freedom, we modify the system\u0026rsquo;s coupling parameters \\(\\vec{g}\\). To ensure that \\(\\vec{g}\\) remains fixed at each iteration, the scaling operation $b$ must be recursively applied.\nThe conceptual understanding of the renormalization group method is straightforward, but the challenge lies in determining the form of the operator \\(\\vec{R}(b;\\vec{g})\\), as we will see in the case of the logistic map.\nExample I: One-dimensional Ising Model In the case of one dimension Ising model, every resursive process scales the total number of lattice by \\( \\frac{1}{2} \\) while keeping the form of partition function fixed. As a result, the coupling constant \\( g \\equiv \\frac{J}{T} \\) is mapped to \\( g^{\\prime} \\) accordingly to match the partition function.\nBy scaling the lattice point by half, the transfer matrix is rescaled as such\n$$ T^{\\prime}=e^{f^{\\prime}}[[e^{g^{\\prime}+h^{\\prime}}, e^{-g^{\\prime}}], [e^{-g^{\\prime}}, E^{g^{\\prime} - h^{\\prime}}]] = e^{2f}[e^{2g+2h} + e^{-2g}] $$\nWhich gives us three equations to solve for the three new coupling constant in terms of the old coupling constant. Writting out explicitly.\nThe renormalized external magnetic field $$ h^{\\prime} = h + \\frac{1}{2}\\ln{\\left[ \\frac{\\cosh{(2g+h)}}{\\cosh{2g-h}} \\right]} $$ In the absent of external magnetic field\nThe renormalized free energy $$ f^{\\prime} = 2f + + \\ln{\\left( 2 \\sqrt{2\\cosh{2g}} \\right)} $$\nThe renormalized nearest-neighbor interaction $$ \\tanh{g^{\\prime}} = \\tanh^{2}g $$\nThe RG flow and fixed points Defining \\( x_{0} \\) to be\n$$ x_{0} = \\tanh{\\left( \\frac{J}{T} \\right)} = \\tanh{g} $$\nwhich follows that\n$$ \\tanh{g_{1}} = x_{1} = x_{0}^{2} $$\nAnd we obtain the recursion relation of the fraction \\( g \\)\n$$ x_{n+1} = x_{n}^{2} $$ $$ x_{n} = \\tanh{\\left( \\frac{J}{T_{n}} \\right)} $$\nSince hyperbolic tangent is asmptotic to -1 and +1, hence the recursion will map the hyperbolic tangent from \\( |1| \\) to \\( 0 \\), or zero temperature to infinite temperature as we shrinks the system.\nAlternatively, the RG flow could be captured by calculating the correlation function \\( G(r_{i} - r_{j}) \\) and the correlation length \\( \\xi \\). Recall that \\( \\xi \\) is defined in terms of the asymptotic behavor of the correlation function \\( G(r_{i} - r_{j}) \\) for \\( |r_{i} - r_{j}| \\rightarrow \\infty \\), or, equivalently, in terms of the behavior of its Fourier transfrom \\( G(k) \\) for small wave vectors.\n$$ \\xi^{\\prime} \\equiv \\xi (x^{\\prime}) = \\frac{\\xi (x)}{b} $$\n$$ \\xi (x^{2}) = \\frac{\\xi (x)}{2} $$\n$$ \\xi (x) = - \\frac{a_{0}}{\\ln{x}} $$\nHence\n$$ \\xi \\sim \\frac{a}{2} e^{2J/T} $$\nExample II: Logistic Map and Period Doubling The logistic map is an example of a discrete-time dynamical system defined by the recursion law:\n$$ x_{n+1} = f(x_{n})=\\mu x (1-x) $$\nAlthough the equation appears simple, it exhibits a captivating phenomenon known as period doubling as the bifurcation parameter $\\mu$ increases, ultimately leading to chaos at the limit $\\mu_{\\infty}$, as illustrated in Figure below.\nInterestingly, a constant $\\delta$ known as the Feigenbaum constant emerges and remains fixed across certain families of functions. It can be stated as in the limit of $n \\rightarrow \\infty$\n$$ \\delta = \\lim_{n\\rightarrow \\infty} \\frac{\\mu_{n}-\\mu_{n-1}}{\\mu_{n+1}-\\mu_{n}} $$\nThrough numerical analysis, the Feigenbaum constant has been calculated to be a value close to $4.66914$.\nThe RG method provides a quantitative approach to calculating the Feigenbaum constant and generalizing the behavior of period doubling phenomena to a function space. However, the mathematical derivations involved can be complex, and interested readers are encouraged to consult relevant papers ( Citation: Sfondrini,\u0026#32;2012 Sfondrini,\u0026#32; A. \u0026#32; (2012). \u0026#32;Introduction to universality and renormalization group techniques.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1210.2262 ) for more detailed information.\nTo keep it simple, we will only be focusing on providing a descriptive account of the steps involved in calculating the Feigenbaum constant, with the emphasis on the interpretation of the results and their significance in the dynamical analysis of chaos.\nTo summarize the steps, we first make observations about the logistic map, recognizing it as a unimodal function. We also note that other unimodal functions exhibit period doubling behavior. We define a functional subspace of unimodal functions and seek to apply the RG method in a way that preserves this characteristic of being unimodel. Once we have the form of \\(R(b;g)\\) at hand, we evaluate the behavior of the fixed point \\(R(\\phi^{ast})=\\phi^{ast}\\) where $\\phi$ denotes the flow. Intuitively, we expect to find one unstable manifold with eigenvalues of modulus greater than one that correspond to bifurcation variable \\(\\mu_{\\infty}\\) that destabilizes the orbit. And an infinite stable manifold with eigenvalues of modulus less than one corresponds to \\(mu_{n}\\) that corresponds to stable oribit with period \\(2^{n}\\). By visualizing the mathematical spaces (refer to the figure below), we can observe that they share similar features with period doubling, albeit in different domains. The spacing of the stable and unstable manifolds follows a geometric series, defining the Feigenbaum constant. By making suitable ansatz for the sequence and fitting it with \\(R(\\phi)\\), we are able to calculate \\(\\sigma=4.66914\\).\nThis derivation clarifies some confusions and provides a unique perspective on the period doubling phenomenon. It demonstrates how the logistic map, initially perceived as a one-dimensional system that cannot exhibit chaotic behavior, actually resides in an infinite-dimensional space that converges to chaos. Moreover, it reveals that unimodal mapping is the underlying universality group characterized by the Feigenbaum constant, with the logistic map representing just one special case. In simpler derivations, numerical mappings of one set of \\(\\phi_{n}$ to $\\phi_{n-1}\\)inform us of the functional form of the renormalization group operator \\(R(\\phi)=-ag(g(-z/a))\\). Without making further assumptions, this form also enables the calculation of the Feigenbaum constant. In this sense, the renormalization group operator captures the essential structure of the onset of chaos behavior.\nBibliography Sfondrini (2012) Sfondrini,\u0026#32; A. \u0026#32; (2012). \u0026#32;Introduction to universality and renormalization group techniques.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1210.2262 Sethna (2020) Sethna,\u0026#32; J.\u0026#32; (2020). \u0026#32; Entropy, Order Parameters, and Complexity (2). \u0026#32; Clarendon Press. P. Kopietz (2010) P. Kopietz,\u0026#32; F.\u0026#32; (2010). \u0026#32; Introduction to the Functional Renormalization Group (1). \u0026#32; Springer, Berlin Heidelberg 2010. https://doi.org/10.1007/978-3-642-05094-7 ","permalink":"https://htsod.github.io/posts/rg_method/","summary":"Renormalization Group Approach in Dynamical System The renormalization group (RG) method is an approximation technique initially developed for solving strongly interacting many-body problems in quantum field theory, where perturbative solutions deviate from the actual solutions. The fundamental concept of the renormalization group approach is to eliminate irrelevant degrees of freedom in a physical system while preserving its essential characteristics ( Citation: P. Kopietz,\u0026#32;2010 P. Kopietz,\u0026#32; F.\u0026#32; (2010). \u0026#32; Introduction to the Functional Renormalization Group (1).","title":"Renormalization Group: A Descriptive Overview"}]