<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Study Notes on Statistical Mechanics | ButterCat</title>
<meta name=keywords content="Statistical Mechanics,Review"><meta name=description content="Entropy, Order Parameters and Complexity What is statistical mechanics? Ensemble How to connect microscopic law with macroscopic phenomenon? The concept of ensemble provides a method that connects the two. Treating a large system(macroscopic) as collection of similarly prepared systems(microscopic).
Entropy Entropy to be defined as a function of probability distribution \(p_{i}\) must satisfy the following:
Maximum at \(\frac{1}{p_{i}}\) Minimum at \(p_{i} = 0 , S = 0\) Minimum at \(p_{i} = 1, S = 0\) The unique function that satisfies the above requirement is \(S=-p_{i}\log{p_{i}}\), giving rise to the information interpretation of entropy."><meta name=author content="Max"><link rel=canonical href=https://canonical.url/to/page><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://htsod.github.io/images/cutter_cat.jpg><link rel=icon type=image/png sizes=16x16 href=https://htsod.github.io/images/cutter_cat.jpg><link rel=icon type=image/png sizes=32x32 href=https://htsod.github.io/images/cutter_cat.jpg><link rel=apple-touch-icon href=https://htsod.github.io/images/cutter_cat.jpg><link rel=mask-icon href=https://htsod.github.io/images/cutter_cat.jpg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://htsod.github.io/review/statistical_mechanics/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Study Notes on Statistical Mechanics"><meta property="og:description" content="Entropy, Order Parameters and Complexity What is statistical mechanics? Ensemble How to connect microscopic law with macroscopic phenomenon? The concept of ensemble provides a method that connects the two. Treating a large system(macroscopic) as collection of similarly prepared systems(microscopic).
Entropy Entropy to be defined as a function of probability distribution \(p_{i}\) must satisfy the following:
Maximum at \(\frac{1}{p_{i}}\) Minimum at \(p_{i} = 0 , S = 0\) Minimum at \(p_{i} = 1, S = 0\) The unique function that satisfies the above requirement is \(S=-p_{i}\log{p_{i}}\), giving rise to the information interpretation of entropy."><meta property="og:type" content="article"><meta property="og:url" content="https://htsod.github.io/review/statistical_mechanics/"><meta property="og:image" content="https://htsod.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="review"><meta property="article:published_time" content="2023-06-11T00:00:00+00:00"><meta property="article:modified_time" content="2024-09-11T00:00:00+00:00"><meta property="og:site_name" content="ButterCat"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://htsod.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Study Notes on Statistical Mechanics"><meta name=twitter:description content="Entropy, Order Parameters and Complexity What is statistical mechanics? Ensemble How to connect microscopic law with macroscopic phenomenon? The concept of ensemble provides a method that connects the two. Treating a large system(macroscopic) as collection of similarly prepared systems(microscopic).
Entropy Entropy to be defined as a function of probability distribution \(p_{i}\) must satisfy the following:
Maximum at \(\frac{1}{p_{i}}\) Minimum at \(p_{i} = 0 , S = 0\) Minimum at \(p_{i} = 1, S = 0\) The unique function that satisfies the above requirement is \(S=-p_{i}\log{p_{i}}\), giving rise to the information interpretation of entropy."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Reviews","item":"https://htsod.github.io/review/"},{"@type":"ListItem","position":2,"name":"Study Notes on Statistical Mechanics","item":"https://htsod.github.io/review/statistical_mechanics/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Study Notes on Statistical Mechanics","name":"Study Notes on Statistical Mechanics","description":"Entropy, Order Parameters and Complexity What is statistical mechanics? Ensemble How to connect microscopic law with macroscopic phenomenon? The concept of ensemble provides a method that connects the two. Treating a large system(macroscopic) as collection of similarly prepared systems(microscopic).\nEntropy Entropy to be defined as a function of probability distribution \\(p_{i}\\) must satisfy the following:\nMaximum at \\(\\frac{1}{p_{i}}\\) Minimum at \\(p_{i} = 0 , S = 0\\) Minimum at \\(p_{i} = 1, S = 0\\) The unique function that satisfies the above requirement is \\(S=-p_{i}\\log{p_{i}}\\), giving rise to the information interpretation of entropy.","keywords":["Statistical Mechanics","Review"],"articleBody":"Entropy, Order Parameters and Complexity What is statistical mechanics? Ensemble How to connect microscopic law with macroscopic phenomenon? The concept of ensemble provides a method that connects the two. Treating a large system(macroscopic) as collection of similarly prepared systems(microscopic).\nEntropy Entropy to be defined as a function of probability distribution \\(p_{i}\\) must satisfy the following:\nMaximum at \\(\\frac{1}{p_{i}}\\) Minimum at \\(p_{i} = 0 , S = 0\\) Minimum at \\(p_{i} = 1, S = 0\\) The unique function that satisfies the above requirement is \\(S=-p_{i}\\log{p_{i}}\\), giving rise to the information interpretation of entropy. From the first requirement, we could see that entropy is maximized when the distribution is more even, or in other words, more mixed, leading to the disorder interpretation of entropy. If entropy is zero, the probability distribution is either \\(0\\) or \\(1\\). Then, if the value of entropy is the only thing we know about the system, we could easily reproduce the system distribution with certainty. As entropy increases, it will be less likely that we could reproduce the system from scratch, making the system irreversible. And this gives rise to the reversibility interpretation of entropy.\nQuantum Statistical Mechanics Quantum mechanics is the law that governs microscopic evolution and particles type. Whereas in everyday life where temperature is sufficiently high, we do not have to worry about quantum mechanical effect. The underlying reason is that the asymetrical states that are created by quantum mechanics effect is thermalized to equally occupied states. So, in sufficiently high temperature, we could treat particles as ideal gas that have no internal structure.\nThe reverse is true. As we cool object, the states might settle into one of the asymetrical states and shows bewildering behavior.\nMonte Carlo It allow the computer to find ensemble averages in systems far too complicated to allow analytical evaluation.\nPhases Different phases are categorized by different symmetries. Matters with different symmetries cannot cross by perturbation theory. So far, there are two recognized way for matter to transit from one state to another. Namely the abrupt phase transition and continous phase transition.\nFluctuations and correlations How a system reponse is related to the correlation function of the system. THe correlation function measure the alignment of state within the system and it condensed the information more compactly.\nAbrupt phase transition By the name suggests, it happens when there is a discontinouity at the first derivative of the free energies because the phase boundary must have equal free energy but above and below it is a different free energy function.\nCriticality The second type of phase changes is by continuous phase transition. This happens when the symmetries of matter change. At critical temperature where the transition occurs, the system fluction is singular at the zero frequency vibration mode. At this point of transition, the system is self-similar. This phenomena is universal across different physical system.\nRandom walks and emergent properties Random walks is defined as taking a random steps in a given manifold randomly for every time step. This simple microscopic evolution rule\nTemperature and equilibrium Why do systems approach equilibrium?. When one of my classmates asked this question in a highschool chemistry class, I was astonished by how naturally we took in the concept that system eventually reaches equilibrium but never asked why. This inquiry of equilibrium had stucked in my head for quite a while until reading upon a statistical mechanics textbook which offers a elegant explanation to equilibrium. The short answer is that the states of equilibrium is much more probable than some other non-equlibrium states for microcanonical ensembles. One could imagine a system dynamics as flow in a network diagram with each nodes representing a possible state.\nAs time proceeds, the flow on each node has probability to evolve into some other states and will eventually reach a time-independent state which we called it equilibrium. Formally speaking, when we say a system has reached a state of equilibrium, we mean that the observable of the time average and the ensemble averages equal. Alternatively, no matter how the flow initially biased towards a particular set of states, as time proceeds, it eventually becomes uniform across all the possible states (a microcannonical ensemble). Noting that the above diagram has a equal number of nodes at the start and at the end. This is because the system is isolated and the total possible states remain unchanged.\nTo illustrate how the above model could be used to explain why a microcannonical system will reach equilibrium eventually,imagining mixing two types of particles with a total particle number of N. Initially, \\(\\frac{N}{2}\\) of particle A and \\(\\frac{N}{2}\\) of particle B each occupies half of the box with a partition in between.\nIgnoring the momentum degree of freedom, the possible states are the volume configuration \\( \\Omega_{unmixed} = \\frac{((V/2)^{N/2})^{2}}{(N/2)!(N/2)!} = \\frac{1}{2^{N}} \\frac{(V)^{N}}{(N/2)!(N/2)!} \\). The nominator states that there is \\( (V/2)^{N/2} \\) ways of configurations allowed for \\(N/2\\) particle in a volume \\(V/2\\). The denominator states that in those indistinguishable \\(N/2\\) particle A and \\( N/2 \\) particle B, they can freely exchange their states without changing the configuration of the system. Now by removing the partition, a total of N particle could fill the entire volume. The mixed state becomes \\( \\Omega_{mixed} = \\frac{(V)^{N}}{(N/2)!(N/2)!} = 2^{N} \\Omega_{unmixed} \\). Provided N is a large number of order \\( \\sim 10^{23} \\). The mixed state is much more likely than the unmixed state by simply out numbering the possibles state of an un mixed state. Hence, as the system evolves (by how we do not care), it is much more likely to fill the mixed states than the unmixed state reaching equilibrium.\nFraming this mixing process with the network model we defined above. Before mixing, we have \\( \\Omega_{unmixed} \\) states to start with. As we remove the partition, the system is no longer isolated and as a result the total number of states have increased to \\( \\Omega_{mixed} = 2^{N} \\Omega_{unmixed} \\). Because the flow is distributed uniformly across all possible states in equilibrium, the mixed states are far more likely than the unmixed states.\nThen, how likely the system will remain in the non-equlibrium unmixed state after mixing? With a crazy small probability of \\(P_{unmixed} = \\frac{1}{2^{N}} \\). Take \\( N = 10 \\), \\( P_{unmixed} = 0.001 \\). Not to mention that N usually of order \\( \\sim 10^{23} \\)\nPhase-space dynamics and ergodicity One might consider further and ponder what kind of system will eventually reach equilibrium where macroscopic observable remain statistically fixed except for rare thermal fluctuation. Or equivalently, for what kind of system does its time average behavior and ensemble behavior equal. Let’s start with a most common system. A classical \\(N\\) particle system in which its evolution is descirbed by its Hamiltonian. \\( H(P,Q) = \\sum_{\\alpha}{p_{\\alpha}^{2}/2m_{\\alpha} + U(q_{1}, …, q_{3N})} \\). We want to study how Hamiltonian evolution will modify the \\(\\rho(q_{1},…,q_{3N},p_{1},…,p_{3N})\\) \\(6N\\) dimensional phase space density distribution.\nConsider the total time derivative on \\(\\rho\\)\n$$ \\frac{d \\rho}{dt} = \\frac{\\partial \\rho}{\\partial t} + \\vec{\\nabla} \\cdot \\vec{J} $$\nwhere \\(\\vec{J} = (\\rho \\dot{P}, \\rho \\dot{Q}\\)) is the phase space probability current. For probability to conserve, total derivative of \\(\\rho\\) must vanishes giving the continuity equation\n$$ \\frac{\\partial \\rho}{\\partial t} = -\\sum_{\\alpha=1}^{3N}{\\frac{\\partial \\rho}{\\partial q_{\\alpha}}q_{\\alpha} + \\frac{\\partial q_{\\alpha}}{\\partial q_{\\alpha}}\\rho_{\\alpha} + \\frac{\\partial \\rho}{\\partial p_{\\alpha}}p_{\\alpha} + \\frac{\\partial p}{\\partial p_{\\alpha}}\\rho_{\\alpha}} $$\nSo far we did the discussion is completely general to the \\(6N\\)-dimensional system. Observing the terms above we see two of them are rather strange looking \\( \\frac{\\partial q_{\\alpha}}{\\partial q_{\\alpha}}\\rho_{\\alpha} \\) and \\( \\frac{\\partial p}{\\partial p_{\\alpha}}\\rho_{\\alpha} \\). Indeed, for Hamiltonian system these term cancel out leading to the Liouville’s theorem\n$$ \\frac{\\partial \\rho}{\\partial t} = -\\sum_{\\alpha=1}^{3N}{\\frac{\\partial \\rho}{\\partial q_{\\alpha}}q_{\\alpha} + \\frac{\\partial \\rho}{\\partial p_{\\alpha}}p_{\\alpha} = 0} $$\nFor folks that familiar with fluid dynamics this is like saying fluid only flow around but cannot be compressed.(changing its density by going somewhere in space) In this case, the \\(6N\\)-dimensional phase space behaves as the incompressible fluid; some states become less likely only by “flowing” its likeliness to its neighbor states.\nEntropy Free energies Ever being involved in drawing a color ball from a mysterious box and winning a price only when a certain color or combination are drawn? Statistical mechanics from another perspective is fairly similar to this concept of drawing color ball except that some functional constraints have been applied to the color ball ensuring that this model is compatible with some physical process in interest.\nIn microcanonical ensemble, the states of the system are labelled by its position and momentum, which together gives the position and momentum dependent energy \\(E_{s} = \\frac{p^{2}}{2m} + U(r)\\) of that state.\nQuantum statistical mechanics Ongoing\nBibliography Bibliography called, but no references ","wordCount":"1444","inLanguage":"en","image":"https://htsod.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2023-06-11T00:00:00Z","dateModified":"2024-09-11T00:00:00Z","author":{"@type":"Person","name":"Max"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://htsod.github.io/review/statistical_mechanics/"},"publisher":{"@type":"Organization","name":"ButterCat","logo":{"@type":"ImageObject","url":"https://htsod.github.io/images/cutter_cat.jpg"}}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"]]}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://htsod.github.io/ accesskey=h title="ButterCat (Alt + H)"><img src=https://htsod.github.io/images/cutter_cat_hu56f3c7c30ca74edcfa530cf2c35cd698_16367_0x35_resize_q75_box.jpg alt aria-label=logo height=35>ButterCat</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://htsod.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://htsod.github.io/review/ title=Reviews><span>Reviews</span></a></li><li><a href=https://htsod.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://htsod.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://htsod.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://htsod.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://htsod.github.io/review/>Reviews</a></div><h1 class="post-title entry-hint-parent">Study Notes on Statistical Mechanics
<span class=entry-hint title=Draft><svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentcolor"><path d="M160-410v-60h3e2v60H160zm0-165v-60h470v60H160zm0-165v-60h470v60H160zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22-4.5 22.5T862.09-380L643-160H520zm3e2-263-37-37 37 37zM580-220h38l121-122-18-19-19-18-122 121v38zm141-141-19-18 37 37-18-19z"/></svg></span></h1><div class=post-meta><span title='2023-06-11 00:00:00 +0000 UTC'>June 11, 2023</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1444 words&nbsp;·&nbsp;Max&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/review/statistical_mechanics.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#entropy--order-parameters-and-complexity>Entropy, Order Parameters and Complexity</a><ul><li><a href=#what-is-statistical-mechanics>What is statistical mechanics?</a></li><li><a href=#random-walks-and-emergent-properties>Random walks and emergent properties</a></li><li><a href=#temperature-and-equilibrium>Temperature and equilibrium</a></li></ul></li><li><a href=#phase-space-dynamics-and-ergodicity>Phase-space dynamics and ergodicity</a></li><li><a href=#entropy>Entropy</a></li><li><a href=#free-energies>Free energies</a></li><li><a href=#quantum-statistical-mechanics>Quantum statistical mechanics</a></li><li><a href=#bibliography>Bibliography</a></li></ul></nav></div></details></div><div class=post-content><h2 id=entropy--order-parameters-and-complexity>Entropy, Order Parameters and Complexity<a hidden class=anchor aria-hidden=true href=#entropy--order-parameters-and-complexity>#</a></h2><h3 id=what-is-statistical-mechanics>What is statistical mechanics?<a hidden class=anchor aria-hidden=true href=#what-is-statistical-mechanics>#</a></h3><p><em>Ensemble</em>
How to connect microscopic law with macroscopic phenomenon? The concept of ensemble provides a method that connects the two. Treating a large system(macroscopic) as collection of similarly prepared systems(microscopic).</p><p><em>Entropy</em>
Entropy to be defined as a function of probability distribution \(p_{i}\) must satisfy the following:</p><ol><li>Maximum at \(\frac{1}{p_{i}}\)</li><li>Minimum at \(p_{i} = 0 , S = 0\)</li><li>Minimum at \(p_{i} = 1, S = 0\)</li></ol><p>The unique function that satisfies the above requirement is \(S=-p_{i}\log{p_{i}}\), giving rise to the information interpretation of entropy. From the first requirement, we could see that entropy is maximized when the distribution is more even, or in other words, more mixed, leading to the disorder interpretation of entropy. If entropy is zero, the probability distribution is either \(0\) or \(1\). Then, if the value of entropy is the only thing we know about the system, we could easily reproduce the system distribution with certainty. As entropy increases, it will be less likely that we could reproduce the system from scratch, making the system irreversible. And this gives rise to the reversibility interpretation of entropy.</p><p><em>Quantum Statistical Mechanics</em>
Quantum mechanics is the law that governs microscopic evolution and particles type. Whereas in everyday life where temperature is sufficiently high, we do not have to worry about quantum mechanical effect. The underlying reason is that the asymetrical states that are created by quantum mechanics effect is thermalized to equally occupied states. So, in sufficiently high temperature, we could treat particles as ideal gas that have no internal structure.</p><p>The reverse is true. As we cool object, the states might settle into one of the asymetrical states and shows bewildering behavior.</p><p><em>Monte Carlo</em>
It allow the computer to find ensemble averages in systems far too complicated to allow analytical evaluation.</p><p><em>Phases</em>
Different phases are categorized by different symmetries. Matters with different symmetries cannot cross by perturbation theory. So far, there are two recognized way for matter to transit from one state to another. Namely the abrupt phase transition and continous phase transition.</p><p><em>Fluctuations and correlations</em>
How a system reponse is related to the correlation function of the system. THe correlation function measure the alignment of state within the system and it condensed the information more compactly.</p><p><em>Abrupt phase transition</em>
By the name suggests, it happens when there is a discontinouity at the first derivative of the free energies because the phase boundary must have equal free energy but above and below it is a different free energy function.</p><p><em>Criticality</em>
The second type of phase changes is by continuous phase transition. This happens when the symmetries of matter change. At critical temperature where the transition occurs, the system fluction is singular at the zero frequency vibration mode. At this point of transition, the system is self-similar. This phenomena is universal across different physical system.</p><h3 id=random-walks-and-emergent-properties>Random walks and emergent properties<a hidden class=anchor aria-hidden=true href=#random-walks-and-emergent-properties>#</a></h3><p>Random walks is defined as taking a random steps in a given manifold randomly for every time step. This simple microscopic evolution rule</p><h3 id=temperature-and-equilibrium>Temperature and equilibrium<a hidden class=anchor aria-hidden=true href=#temperature-and-equilibrium>#</a></h3><p><strong>Why do systems approach equilibrium?</strong>. When one of my classmates asked this question in a highschool chemistry class, I was astonished by how naturally we took in the concept that system eventually reaches equilibrium but never asked why. This inquiry of equilibrium had stucked in my head for quite a while until reading upon a statistical mechanics textbook which offers a elegant explanation to equilibrium. The short answer is that <strong>the states of equilibrium is much more probable than some other non-equlibrium states for microcanonical ensembles</strong>. One could imagine a system dynamics as flow in a network diagram with each nodes representing a possible state.</p><p>As time proceeds, the flow on each node has probability to evolve into some other states and will eventually reach a time-independent state which we called it equilibrium. Formally speaking, when we say a system has reached a state of equilibrium, we mean that <strong>the observable of the time average and the ensemble averages equal</strong>. Alternatively, no matter how the flow initially biased towards a particular set of states, as time proceeds, it eventually becomes uniform across all the possible states (a microcannonical ensemble). Noting that the above diagram has a equal number of nodes at the start and at the end. This is because the system is isolated and the total possible states remain unchanged.</p><p>To illustrate how the above model could be used to explain why a microcannonical system will reach equilibrium eventually,imagining mixing two types of particles with a total particle number of N. Initially, \(\frac{N}{2}\) of particle A and \(\frac{N}{2}\) of particle B each occupies half of the box with a partition in between.</p><p>Ignoring the momentum degree of freedom, the possible states are the volume configuration \( \Omega_{unmixed} = \frac{((V/2)^{N/2})^{2}}{(N/2)!(N/2)!} = \frac{1}{2^{N}} \frac{(V)^{N}}{(N/2)!(N/2)!} \). The nominator states that there is \( (V/2)^{N/2} \) ways of configurations allowed for \(N/2\) particle in a volume \(V/2\). The denominator states that in those indistinguishable \(N/2\) particle A and \( N/2 \) particle B, they can freely exchange their states without changing the configuration of the system. Now by removing the partition, a total of N particle could fill the entire volume. The mixed state becomes \( \Omega_{mixed} = \frac{(V)^{N}}{(N/2)!(N/2)!} = 2^{N} \Omega_{unmixed} \). Provided N is a large number of order \( \sim 10^{23} \). The mixed state is much more likely than the unmixed state by simply out numbering the possibles state of an un mixed state. Hence, as the system evolves (by how we do not care), it is much more likely to fill the mixed states than the unmixed state reaching equilibrium.</p><p>Framing this mixing process with the network model we defined above. Before mixing, we have \( \Omega_{unmixed} \) states to start with. As we remove the partition, the system is no longer isolated and as a result the total number of states have increased to \( \Omega_{mixed} = 2^{N} \Omega_{unmixed} \). Because the flow is distributed uniformly across all possible states in equilibrium, the mixed states are far more likely than the unmixed states.</p><p>Then, how likely the system will remain in the non-equlibrium unmixed state after mixing? With a crazy small probability of \(P_{unmixed} = \frac{1}{2^{N}} \). Take \( N = 10 \), \( P_{unmixed} = 0.001 \). Not to mention that N usually of order \( \sim 10^{23} \)</p><h2 id=phase-space-dynamics-and-ergodicity>Phase-space dynamics and ergodicity<a hidden class=anchor aria-hidden=true href=#phase-space-dynamics-and-ergodicity>#</a></h2><p>One might consider further and ponder what kind of system will eventually reach equilibrium where macroscopic observable remain statistically fixed except for rare thermal fluctuation. Or equivalently, for what kind of system does its time average behavior and ensemble behavior equal. Let&rsquo;s start with a most common system. A classical \(N\) particle system in which its evolution is descirbed by its Hamiltonian. \( H(P,Q) = \sum_{\alpha}{p_{\alpha}^{2}/2m_{\alpha} + U(q_{1}, &mldr;, q_{3N})} \). We want to study how Hamiltonian evolution will modify the \(\rho(q_{1},&mldr;,q_{3N},p_{1},&mldr;,p_{3N})\) \(6N\) dimensional phase space density distribution.</p><p>Consider the total time derivative on \(\rho\)</p><p>$$ \frac{d \rho}{dt} = \frac{\partial \rho}{\partial t} + \vec{\nabla} \cdot \vec{J} $$</p><p>where \(\vec{J} = (\rho \dot{P}, \rho \dot{Q}\)) is the phase space probability current. For probability to conserve, total derivative of \(\rho\) must vanishes giving the continuity equation</p><p>$$ \frac{\partial \rho}{\partial t} = -\sum_{\alpha=1}^{3N}{\frac{\partial \rho}{\partial q_{\alpha}}q_{\alpha} + \frac{\partial q_{\alpha}}{\partial q_{\alpha}}\rho_{\alpha} + \frac{\partial \rho}{\partial p_{\alpha}}p_{\alpha} + \frac{\partial p}{\partial p_{\alpha}}\rho_{\alpha}} $$</p><p>So far we did the discussion is completely general to the \(6N\)-dimensional system. Observing the terms above we see two of them are rather strange looking \( \frac{\partial q_{\alpha}}{\partial q_{\alpha}}\rho_{\alpha} \) and \( \frac{\partial p}{\partial p_{\alpha}}\rho_{\alpha} \). Indeed, for Hamiltonian system these term cancel out leading to the Liouville&rsquo;s theorem</p><p>$$ \frac{\partial \rho}{\partial t} = -\sum_{\alpha=1}^{3N}{\frac{\partial \rho}{\partial q_{\alpha}}q_{\alpha} + \frac{\partial \rho}{\partial p_{\alpha}}p_{\alpha} = 0} $$</p><p>For folks that familiar with fluid dynamics this is like saying fluid only flow around but cannot be compressed.(changing its density by going somewhere in space) In this case, the \(6N\)-dimensional phase space behaves as the incompressible fluid; some states become less likely only by &ldquo;flowing&rdquo; its likeliness to its neighbor states.</p><h2 id=entropy>Entropy<a hidden class=anchor aria-hidden=true href=#entropy>#</a></h2><h2 id=free-energies>Free energies<a hidden class=anchor aria-hidden=true href=#free-energies>#</a></h2><p>Ever being involved in drawing a color ball from a mysterious box and winning a price only when a certain color or combination are drawn? Statistical mechanics from another perspective is fairly similar to this concept of drawing color ball except that some functional constraints have been applied to the color ball ensuring that this model is compatible with some physical process in interest.</p><p>In microcanonical ensemble, the states of the system are labelled by its position and momentum, which together gives the position and momentum dependent energy \(E_{s} = \frac{p^{2}}{2m} + U(r)\) of that state.</p><h2 id=quantum-statistical-mechanics>Quantum statistical mechanics<a hidden class=anchor aria-hidden=true href=#quantum-statistical-mechanics>#</a></h2><p>Ongoing</p><h2 id=bibliography>Bibliography<a hidden class=anchor aria-hidden=true href=#bibliography>#</a></h2>Bibliography called, but no references</div><footer class=post-footer><ul class=post-tags><li><a href=https://htsod.github.io/tags/statistical-mechanics/>Statistical Mechanics</a></li><li><a href=https://htsod.github.io/tags/review/>Review</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://htsod.github.io/>ButterCat</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>